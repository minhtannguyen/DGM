{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import mxnet as mx\n",
    "from mxnet import image\n",
    "from mxnet import nd, gluon, autograd, init\n",
    "from mxnet.gluon.data.vision import ImageFolderDataset\n",
    "from mxnet.gluon.data import DataLoader\n",
    "from mxnet.gluon import nn\n",
    "from tensorboardX import SummaryWriter\n",
    "import numpy as np\n",
    "import shutil\n",
    "import _pickle as cPickle\n",
    "from sklearn import preprocessing\n",
    "from mxnet.gluon.parameter import Parameter, ParameterDict\n",
    "from common.util import download_file\n",
    "import subprocess\n",
    "\n",
    "from IPython.core.debugger import Tracer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Options:\n",
    "    def __init__(self):\n",
    "        self.seed_val = 0\n",
    "        self.num_train_sup = 73257\n",
    "        self.batch_size = 75\n",
    "        self.data_dir = '/tanData/datasets/svhn'\n",
    "        self.log_dir = '/tanData/logs'\n",
    "        self.model_dir ='/tanData/models'\n",
    "        self.exp_name = 'svhn_nlabels_%i_allconv13_lddrm_mm_pathnorm_min_max_meanteacher_seed_%i'%(self.num_train_sup, self.seed_val)\n",
    "        self.ctx = mx.gpu(4)\n",
    "        self.alpha_drm = 0.5\n",
    "        self.alpha_pn = 1.0\n",
    "        self.alpha_kl = 0.5\n",
    "        self.alpha_nn = 0.5\n",
    "        self.alpha_min = 0.5\n",
    "        self.alpha_max = 1.0 - self.alpha_min\n",
    "        self.alpha_consistent = 33.0\n",
    "        \n",
    "        self.use_bias = True\n",
    "        self.use_bn = True\n",
    "        self.do_topdown = True\n",
    "        self.do_countpath = False\n",
    "        self.do_pn = True\n",
    "        self.relu_td = False\n",
    "        self.do_nn = True\n",
    "        self.min_max = True\n",
    "\n",
    "opt = Options()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gpu_device(ctx=mx.gpu(0)):\n",
    "    try:\n",
    "        _ = mx.nd.array([1, 2, 3], ctx=ctx)\n",
    "    except mx.MXNetError:\n",
    "        return None\n",
    "    return ctx\n",
    "\n",
    "assert gpu_device(opt.ctx), 'No GPU device found!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = os.path.join(opt.log_dir, opt.exp_name)\n",
    "if not os.path.exists(log_dir):\n",
    "    os.makedirs(log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_shape = (3, 32, 32)\n",
    "train_data_sup = mx.io.ImageRecordIter(\n",
    "    path_imgrec = os.path.join(opt.data_dir,'svhn_train.rec'),\n",
    "    data_shape  = data_shape,\n",
    "    batch_size  = opt.batch_size,\n",
    "    mean_r             = 125.3,\n",
    "    mean_g             = 123.0,\n",
    "    mean_b             = 113.9,\n",
    "    std_r              = 63.0,\n",
    "    std_g              = 62.1,\n",
    "    std_b              = 66.7,\n",
    "    shuffle = True,\n",
    "    ## Data augmentation\n",
    "    rand_crop   = True,\n",
    "    max_crop_size = 32,\n",
    "    min_crop_size = 32,\n",
    "    pad = 4,\n",
    "    fill_value = 0,\n",
    "    rand_mirror = True)\n",
    "valid_data = mx.io.ImageRecordIter(\n",
    "    path_imgrec = os.path.join(opt.data_dir,'svhn_val.rec'),\n",
    "    data_shape  = data_shape,\n",
    "    batch_size  = opt.batch_size,\n",
    "    mean_r             = 125.3,\n",
    "    mean_g             = 123.0,\n",
    "    mean_b             = 113.9,\n",
    "    std_r              = 63.0,\n",
    "    std_g              = 62.1,\n",
    "    std_b              = 66.7,\n",
    "    ## No data augmentation\n",
    "    rand_crop   = False,\n",
    "    rand_mirror = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "L2_loss = gluon.loss.L2Loss()\n",
    "L1_loss = gluon.loss.L1Loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Normal(mx.init.Initializer):\n",
    "    \"\"\"Initializes weights with random values sampled from a normal distribution\n",
    "    with a mean and standard deviation of `sigma`.\n",
    "    \"\"\"\n",
    "    def __init__(self, mean=0, sigma=0.01):\n",
    "        super(Normal, self).__init__(sigma=sigma)\n",
    "        self.sigma = sigma\n",
    "        self.mean = mean\n",
    "\n",
    "    def _init_weight(self, _, arr):\n",
    "        mx.random.normal(self.mean, self.sigma, out=arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from resnet import ResNet164_v2\n",
    "from mxnet.gluon.model_zoo import vision as models\n",
    "from vgg_ld_opt_min_max import VGG_DRM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "writer = SummaryWriter(os.path.join(opt.log_dir, opt.exp_name))\n",
    "\n",
    "def get_acc(output, label):\n",
    "    pred = output.argmax(1, keepdims=False)\n",
    "    correct = (pred == label).sum()\n",
    "    return correct.asscalar()\n",
    "\n",
    "def extract_acts(net, x, layer_indx):\n",
    "    start_layer = 0\n",
    "    out = []\n",
    "    for i in layer_indx:\n",
    "        for block in net.features._children[start_layer:i]:\n",
    "            x = block(x)\n",
    "        out.append(x)\n",
    "        start_layer = i\n",
    "    return out\n",
    "\n",
    "def update_ema_variables(model, ema_model, alpha, global_step):\n",
    "    # Use the true average until the exponential average is more correct\n",
    "    alpha = min(1 - 1 / (global_step + 1), alpha)\n",
    "    for ema_param, param in zip(ema_model.collect_params().values(), model.collect_params().values()):\n",
    "        ema_param.set_data(alpha * ema_param.data() + (1 - alpha) * param.data())\n",
    "\n",
    "def test(net, valid_data, ctx):\n",
    "    valid_data.reset()\n",
    "    valid_loss = 0; valid_loss_xentropy = 0; valid_loss_drm = 0; valid_loss_pn = 0; valid_loss_kl = 0; valid_loss_nn = 0\n",
    "    valid_correct = 0; valid_total = 0; valid_correct2 = 0; valid_correctmax = 0; valid_correctmin = 0\n",
    "    num_batch_valid = 0\n",
    "    for batch in valid_data:\n",
    "        bs = batch.data[0].shape[0]\n",
    "        data = batch.data[0].as_in_context(ctx)\n",
    "        label = batch.label[0].as_in_context(ctx)\n",
    "        [output, output_min, xhat, xhat_min, _, loss_pn, loss_nn] = net(data, label)\n",
    "        loss_xentropy = opt.alpha_max * criterion(output, label) + opt.alpha_min * criterion(output_min, label)\n",
    "        loss_drm = opt.alpha_max * L2_loss(xhat, data) + opt.alpha_min * L2_loss(xhat_min, data)\n",
    "        softmax_val = opt.alpha_max * nd.softmax(output) + opt.alpha_min * nd.softmax(output_min)\n",
    "        loss_kl = -nd.sum(nd.log(10.0*softmax_val + 1e-8) * softmax_val, axis=1)\n",
    "        loss = loss_xentropy + opt.alpha_drm * loss_drm + opt.alpha_kl * loss_kl + opt.alpha_nn * loss_nn + opt.alpha_pn * loss_pn\n",
    "\n",
    "        valid_loss_xentropy += nd.mean(loss_xentropy).asscalar()\n",
    "        valid_loss_drm += nd.mean(loss_drm).asscalar()\n",
    "        valid_loss_pn += nd.mean(loss_pn).asscalar()\n",
    "        valid_loss_kl += nd.mean(loss_kl).asscalar()\n",
    "        valid_loss_nn += nd.mean(loss_nn).asscalar()\n",
    "        valid_loss += nd.mean(loss).asscalar()\n",
    "        valid_correct += get_acc(opt.alpha_max * output + opt.alpha_min * output_min, label)\n",
    "        valid_correct2 += opt.alpha_max * get_acc(output, label) + opt.alpha_min * get_acc(output_min, label)\n",
    "        valid_correctmax += get_acc(output, label)\n",
    "        valid_correctmin += get_acc(output_min, label)\n",
    "\n",
    "        valid_total += bs\n",
    "        num_batch_valid += 1\n",
    "    valid_acc = valid_correct / valid_total\n",
    "    valid_acc2 = valid_correct2 / valid_total\n",
    "    valid_accmax = valid_correctmax / valid_total\n",
    "    valid_accmin = valid_correctmin / valid_total\n",
    "    return valid_acc, valid_acc2, valid_accmax, valid_accmin, valid_loss_xentropy, valid_loss_drm, valid_loss_pn, valid_loss_kl, valid_loss_nn, valid_loss, num_batch_valid\n",
    "\n",
    "def write_results(writer, name, valid_acc, valid_acc2, valid_accmax, valid_accmin, valid_loss_xentropy, valid_loss_drm, valid_loss_pn, valid_loss_kl, valid_loss_nn, valid_loss, num_batch_valid, epoch):\n",
    "    writer.add_scalars('loss', {'%s'%name: valid_loss / num_batch_valid}, epoch)\n",
    "    writer.add_scalars('loss_xentropy', {'%s'%name: valid_loss_xentropy / num_batch_valid}, epoch)\n",
    "    writer.add_scalars('loss_drm', {'%s'%name: valid_loss_drm / num_batch_valid}, epoch)\n",
    "    writer.add_scalars('loss_pn', {'%s'%name: valid_loss_pn / num_batch_valid}, epoch)\n",
    "    writer.add_scalars('loss_kl', {'%s'%name: valid_loss_kl / num_batch_valid}, epoch)\n",
    "    writer.add_scalars('loss_nn', {'%s'%name: valid_loss_nn / num_batch_valid}, epoch)\n",
    "    writer.add_scalars('acc', {'%s'%name: valid_acc}, epoch)\n",
    "    writer.add_scalars('acc', {'%s_2'%name: valid_acc2}, epoch)\n",
    "    writer.add_scalars('acc', {'%s_max'%name: valid_accmax}, epoch)\n",
    "    writer.add_scalars('acc', {'%s_min'%name: valid_accmin}, epoch)\n",
    "    \n",
    "def save_best(net, name, valid_acc, valid_acc2, valid_accmax, valid_accmin, best_valid_acc, best_valid_acc2, best_valid_accmax, best_valid_accmin):\n",
    "    if valid_acc > best_valid_acc:\n",
    "        best_valid_acc = valid_acc\n",
    "        net.collect_params().save('%s/%s_%s.params'%(opt.model_dir, name, opt.exp_name))\n",
    "    if valid_acc2 > best_valid_acc2:\n",
    "        best_valid_acc2 = valid_acc2\n",
    "        net.collect_params().save('%s/%s_%s_2.params'%(opt.model_dir, name, opt.exp_name))\n",
    "    if valid_accmax > best_valid_accmax:\n",
    "        best_valid_accmax = valid_accmax\n",
    "        net.collect_params().save('%s/%s_%s_max.params'%(opt.model_dir, name, opt.exp_name))\n",
    "    if valid_accmin > best_valid_accmin:\n",
    "        best_valid_accmin = valid_accmin\n",
    "        net.collect_params().save('%s/%s_%s_min.params'%(opt.model_dir, name, opt.exp_name))\n",
    "    return best_valid_acc, best_valid_acc2, best_valid_accmax, best_valid_accmin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, ema_net, train_data_sup, valid_data, num_epochs, lr, wd, ctx, lr_decay, relu_indx):\n",
    "    trainer = gluon.Trainer(\n",
    "        net.collect_params(), 'adam', {'learning_rate': 0.001, 'wd': wd})\n",
    "    \n",
    "    prev_time = datetime.datetime.now()\n",
    "    best_valid_acc = 0; best_valid_acc2 = 0; best_valid_accmax = 0; best_valid_accmin = 0\n",
    "    best_valid_acc_ema = 0; best_valid_acc_ema2 = 0; best_valid_acc_emamax = 0; best_valid_acc_emamin = 0\n",
    "    iter_indx = 0\n",
    "    global_step = 0\n",
    "    \n",
    "    for epoch in range(num_epochs-100):\n",
    "        train_data_sup.reset()\n",
    "        train_loss = 0; train_loss_xentropy = 0; train_loss_drm = 0; train_loss_pn = 0; train_loss_kl = 0; train_loss_nn = 0\n",
    "        correct = 0; total = 0; correct2=0; correctmax=0; correctmin=0\n",
    "        num_batch_train = 0\n",
    "        \n",
    "        if epoch == 20:\n",
    "            sgd_lr = 0.15\n",
    "            decay_val = np.exp(np.log(sgd_lr / 0.0001) / (num_epochs - 2))\n",
    "            sgd_lr = sgd_lr * decay_val\n",
    "            trainer = gluon.Trainer(net.collect_params(), 'SGD', {'learning_rate': sgd_lr, 'wd': wd})\n",
    "            \n",
    "        if epoch >= 20:\n",
    "            trainer.set_learning_rate(trainer.learning_rate / decay_val)\n",
    "        \n",
    "        for batch_sup in train_data_sup:\n",
    "            bs = batch_sup.data[0].shape[0]\n",
    "            data_sup = batch_sup.data[0].as_in_context(ctx)\n",
    "            label_sup = batch_sup.label[0].as_in_context(ctx)\n",
    "            with autograd.record():\n",
    "                [output_sup, output_min_sup, xhat_sup, xhat_min_sup, _, loss_pn_sup, loss_nn_sup] = net(data_sup, label_sup)\n",
    "                if global_step == 0:\n",
    "                    for ema_param, param in zip(ema_net.collect_params().values(),net.collect_params().values()):\n",
    "                        ema_param.initialize(init=mx.initializer.Constant(param.data()), ctx=ctx)\n",
    "                [output_sup_ema, output_min_sup_ema, _, _, _, _, _] = ema_net(data_sup, label_sup)\n",
    "                loss_xentropy_sup = opt.alpha_max * criterion(output_sup, label_sup) + opt.alpha_min * criterion(output_min_sup, label_sup)\n",
    "                loss_drm_sup = opt.alpha_max * L2_loss(xhat_sup, data_sup) + opt.alpha_min * L2_loss(xhat_min_sup, data_sup)\n",
    "                softmax_sup = opt.alpha_max * nd.softmax(output_sup) + opt.alpha_min * nd.softmax(output_min_sup)\n",
    "                softmax_sup_ema = opt.alpha_max * nd.softmax(output_sup_ema) + opt.alpha_min * nd.softmax(output_min_sup_ema)\n",
    "                loss_kl_sup = -nd.sum(nd.log(10.0*softmax_sup + 1e-8) * softmax_sup, axis=1)\n",
    "                loss_consistent_sup = L2_loss(softmax_sup, softmax_sup_ema)\n",
    "                loss = loss_xentropy_sup + opt.alpha_drm * loss_drm_sup + opt.alpha_kl * loss_kl_sup + opt.alpha_nn * loss_nn_sup + opt.alpha_pn * loss_pn_sup + opt.alpha_consistent * loss_consistent_sup\n",
    "                \n",
    "            loss.backward()\n",
    "            trainer.step(bs)\n",
    "            global_step += 1\n",
    "            if global_step < 40000:\n",
    "                update_ema_variables(net, ema_net, ema_decay[0], global_step)\n",
    "            else:\n",
    "                update_ema_variables(net, ema_net, ema_decay[1], global_step)\n",
    "            \n",
    "            loss_drm = loss_drm_sup\n",
    "            loss_pn = loss_pn_sup\n",
    "            loss_xentropy = loss_xentropy_sup\n",
    "            loss_kl = loss_kl_sup\n",
    "            loss_nn = loss_nn_sup\n",
    "            \n",
    "            train_loss_xentropy += nd.mean(loss_xentropy).asscalar()\n",
    "            train_loss_drm += nd.mean(loss_drm).asscalar()\n",
    "            train_loss_pn += nd.mean(loss_pn).asscalar()\n",
    "            train_loss_kl += nd.mean(loss_kl).asscalar()\n",
    "            train_loss_nn += nd.mean(loss_nn).asscalar()\n",
    "            train_loss += nd.mean(loss).asscalar()\n",
    "            correct += get_acc(opt.alpha_max * output_sup + opt.alpha_min * output_min_sup, label_sup)\n",
    "            correct2 += opt.alpha_max * get_acc(output_sup, label_sup) + opt.alpha_min * get_acc(output_min_sup, label_sup)\n",
    "            correctmax += get_acc(output_sup, label_sup)\n",
    "            correctmin += get_acc(output_min_sup, label_sup)\n",
    "            \n",
    "            total += bs\n",
    "            num_batch_train += 1\n",
    "            iter_indx += 1\n",
    "        \n",
    "        writer.add_scalars('loss', {'train': train_loss / num_batch_train}, epoch)\n",
    "        writer.add_scalars('loss_xentropy', {'train': train_loss_xentropy / num_batch_train}, epoch)\n",
    "        writer.add_scalars('loss_drm', {'train': train_loss_drm / num_batch_train}, epoch)\n",
    "        writer.add_scalars('loss_pn', {'train': train_loss_pn / num_batch_train}, epoch)\n",
    "        writer.add_scalars('loss_kl', {'train': train_loss_kl / num_batch_train}, epoch)\n",
    "        writer.add_scalars('loss_nn', {'train': train_loss_nn / num_batch_train}, epoch)\n",
    "        writer.add_scalars('acc', {'train': correct / total}, epoch)\n",
    "        writer.add_scalars('acc', {'train2': correct2 / total}, epoch)\n",
    "        writer.add_scalars('acc', {'trainmax': correctmax / total}, epoch)\n",
    "        writer.add_scalars('acc', {'trainmin': correctmin / total}, epoch)\n",
    "        \n",
    "        cur_time = datetime.datetime.now()\n",
    "        h, remainder = divmod((cur_time - prev_time).seconds, 3600)\n",
    "        m, s = divmod(remainder, 60)\n",
    "        time_str = \"Time %02d:%02d:%02d\" % (h, m, s)\n",
    "        if valid_data is not None:\n",
    "            valid_acc, valid_acc2, valid_accmax, valid_accmin, valid_loss_xentropy, valid_loss_drm, valid_loss_pn, valid_loss_kl, valid_loss_nn, valid_loss, num_batch_valid = test(net, valid_data, ctx)\n",
    "            best_valid_acc, best_valid_acc2, best_valid_accmax, best_valid_accmin = save_best(net, 'best', valid_acc, valid_acc2, valid_accmax, valid_accmin, best_valid_acc, best_valid_acc2, best_valid_accmax, best_valid_accmin)\n",
    "            write_results(writer, 'valid', valid_acc, valid_acc2, valid_accmax, valid_accmin, valid_loss_xentropy, valid_loss_drm, valid_loss_pn, valid_loss_kl, valid_loss_nn, valid_loss, num_batch_valid, epoch)\n",
    "            epoch_str = (\"Epoch %d. Train Loss: %f, Train Xent: %f, Train Reconst: %f, Train Pn: %f, Train acc %f, Valid Loss: %f, Valid acc %f, Valid acc2 %f, Valid accmax %f, Valid accmin %f, Best valid acc %f, Best valid acc2 %f, Best valid accmax %f, Best valid accmin %f, \"\n",
    "                         % (epoch, train_loss / num_batch_train, train_loss_xentropy / num_batch_train, train_loss_drm / num_batch_train, train_loss_pn / num_batch_train,\n",
    "                            correct / total, valid_loss / num_batch_valid, valid_acc, valid_acc2, valid_accmax, valid_accmin, best_valid_acc, best_valid_acc2, best_valid_accmax, best_valid_accmin))\n",
    "            \n",
    "            valid_acc, valid_acc2, valid_accmax, valid_accmin, valid_loss_xentropy, valid_loss_drm, valid_loss_pn, valid_loss_kl, valid_loss_nn, valid_loss, num_batch_valid = test(ema_net, valid_data, ctx)\n",
    "            best_valid_acc_ema, best_valid_acc_ema2, best_valid_acc_emamax, best_valid_acc_emamin = save_best(ema_net, 'best_ema', valid_acc, valid_acc2, valid_accmax, valid_accmin, best_valid_acc_ema, best_valid_acc_ema2, best_valid_acc_emamax, best_valid_acc_emamin)\n",
    "            write_results(writer, 'valid_ema', valid_acc, valid_acc2, valid_accmax, valid_accmin, valid_loss_xentropy, valid_loss_drm, valid_loss_pn, valid_loss_kl, valid_loss_nn, valid_loss, num_batch_valid, epoch)\n",
    "            epoch_str_ema = (\"Epoch %d. Valid Loss EMA: %f, Valid acc EMA %f, Valid acc2 EMA %f, Valid accmax EMA %f, Valid accmin EMA %f, Best valid acc EMA %f, Best valid acc2 EMA %f, Best valid accmax EMA %f, Best valid accmin EMA %f, \"\n",
    "                         % (epoch, valid_loss / num_batch_valid, valid_acc, valid_acc2, valid_accmax, valid_accmin, best_valid_acc_ema, best_valid_acc_ema2, best_valid_acc_emamax, best_valid_acc_emamin))\n",
    "            # net.collect_params().save('%s/%s_latest.params'%(opt.model_dir, opt.exp_name))\n",
    "            if not epoch % 20:\n",
    "                net.collect_params().save('%s/%s_epoch_%i.params'%(opt.model_dir, opt.exp_name, epoch))\n",
    "                ema_net.collect_params().save('%s/%s_ema_epoch_%i.params'%(opt.model_dir, opt.exp_name, epoch))\n",
    "        else:\n",
    "            epoch_str = (\"Epoch %d. Loss: %f, Train acc %f, \"\n",
    "                         % (epoch, train_loss / num_batch_train,\n",
    "                            correct / total))\n",
    "        prev_time = cur_time\n",
    "        print(epoch_str + time_str + ', lr ' + str(trainer.learning_rate))\n",
    "        print(epoch_str_ema + time_str)\n",
    "        \n",
    "    return best_valid_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 500\n",
    "learning_rate = 0.15\n",
    "weight_decay = 5e-4\n",
    "lr_decay = 0.1\n",
    "ema_decay = [0.99, 0.999]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_train(num_exp, ctx):\n",
    "    valid_acc = 0\n",
    "    for i in range(num_exp):\n",
    "        ### CIFAR VGG_DRM\n",
    "        model = VGG_DRM('AllConv13', batch_size=opt.batch_size, num_class=10, use_bias=opt.use_bias, use_bn=opt.use_bn, do_topdown=opt.do_topdown, do_countpath=opt.do_countpath, do_pn=opt.do_pn, relu_td=opt.relu_td, do_nn=opt.do_nn, min_max=opt.min_max)\n",
    "        for param in model.collect_params().values():\n",
    "            if param.name.find('conv') != -1 or param.name.find('dense') != -1:\n",
    "                if param.name.find('weight') != -1:\n",
    "                    param.initialize(init=mx.initializer.Xavier(), ctx=ctx)\n",
    "                else:\n",
    "                    param.initialize(init=mx.init.Zero(), ctx=ctx)\n",
    "            elif param.name.find('batchnorm') != -1 or param.name.find('instancenorm') != -1:\n",
    "                if param.name.find('gamma') != -1:\n",
    "                    param.initialize(init=Normal(mean=1, sigma=0.02), ctx=ctx)\n",
    "                else:\n",
    "                    param.initialize(init=mx.init.Zero(), ctx=ctx)\n",
    "            elif param.name.find('biasadder') != -1:\n",
    "                param.initialize(init=mx.init.Zero(), ctx=ctx)\n",
    "        \n",
    "        # model.collect_params().load('%s/%s_epoch_%i.params'%(opt.model_dir, opt.exp_name, 380), ctx=ctx)\n",
    "        ema_model = VGG_DRM('AllConv13', batch_size=opt.batch_size, num_class=10, use_bias=opt.use_bias, use_bn=opt.use_bn, do_topdown=opt.do_topdown, do_countpath=opt.do_countpath, do_pn=opt.do_pn, relu_td=opt.relu_td, do_nn=opt.do_nn, min_max=opt.min_max)\n",
    "        # model.hybridize()\n",
    "        relu_indx = []\n",
    "        \n",
    "        for i in range(len(model.features._children)):\n",
    "            if model.features._children[i].name.find('relu') != -1:\n",
    "                relu_indx.append(i)\n",
    "                \n",
    "        acc = train(model, ema_model, train_data_sup, valid_data, num_epochs, learning_rate, weight_decay, ctx, lr_decay, relu_indx)\n",
    "        print('Validation Accuracy - Run %i = %f'%(i, acc))\n",
    "        valid_acc += acc\n",
    "\n",
    "    print('Validation Accuracy = %f'%(valid_acc/num_exp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0. Train Loss: 1.884872, Train Xent: 1.326683, Train Reconst: 0.479990, Train Pn: 0.007352, Train acc 0.580266, Valid Loss: 2.252857, Valid acc 0.833103, Valid acc2 0.817395, Valid accmax 0.815134, Valid accmin 0.819655, Best valid acc 0.833103, Best valid acc2 0.817395, Best valid accmax 0.815134, Best valid accmin 0.819655, Time 00:10:05, lr 0.001\n",
      "Epoch 0. Valid Loss EMA: 1.835661, Valid acc EMA 0.847646, Valid acc2 EMA 0.844976, Valid accmax EMA 0.842997, Valid accmin EMA 0.846955, Best valid acc EMA 0.847646, Best valid acc2 EMA 0.844976, Best valid accmax EMA 0.842997, Best valid accmin EMA 0.846955, Time 00:10:05\n",
      "Epoch 1. Train Loss: -0.103948, Train Xent: 0.523279, Train Reconst: 0.110850, Train Pn: 0.004944, Train acc 0.863187, Valid Loss: 3.026830, Valid acc 0.894601, Valid acc2 0.882133, Valid accmax 0.882920, Valid accmin 0.881345, Best valid acc 0.894601, Best valid acc2 0.882133, Best valid accmax 0.882920, Best valid accmin 0.881345, Time 00:11:35, lr 0.001\n",
      "Epoch 1. Valid Loss EMA: 3.780309, Valid acc EMA 0.920884, Valid acc2 EMA 0.918482, Valid accmax EMA 0.918847, Valid accmin EMA 0.918117, Best valid acc EMA 0.920884, Best valid acc2 EMA 0.918482, Best valid accmax EMA 0.918847, Best valid accmin EMA 0.918117, Time 00:11:35\n",
      "Epoch 2. Train Loss: -0.381819, Train Xent: 0.417138, Train Reconst: 0.047196, Train Pn: 0.004182, Train acc 0.892651, Valid Loss: 3.227665, Valid acc 0.916811, Valid acc2 0.907743, Valid accmax 0.904669, Valid accmin 0.910817, Best valid acc 0.916811, Best valid acc2 0.907743, Best valid accmax 0.904669, Best valid accmin 0.910817, Time 00:11:34, lr 0.001\n",
      "Epoch 2. Valid Loss EMA: 5.613696, Valid acc EMA 0.936292, Valid acc2 EMA 0.933641, Valid accmax EMA 0.934294, Valid accmin EMA 0.932988, Best valid acc EMA 0.936292, Best valid acc2 EMA 0.933641, Best valid accmax EMA 0.934294, Best valid accmin EMA 0.932988, Time 00:11:34\n",
      "Epoch 3. Train Loss: -0.496787, Train Xent: 0.374571, Train Reconst: 0.033220, Train Pn: 0.003820, Train acc 0.904838, Valid Loss: 11.251001, Valid acc 0.930375, Valid acc2 0.921921, Valid accmax 0.918501, Valid accmin 0.925341, Best valid acc 0.930375, Best valid acc2 0.921921, Best valid accmax 0.918501, Best valid accmin 0.925341, Time 00:11:36, lr 0.001\n",
      "Epoch 3. Valid Loss EMA: 12.054298, Valid acc EMA 0.941441, Valid acc2 EMA 0.940134, Valid accmax EMA 0.940519, Valid accmin EMA 0.939750, Best valid acc EMA 0.941441, Best valid acc2 EMA 0.940134, Best valid accmax EMA 0.940519, Best valid accmin EMA 0.939750, Time 00:11:36\n",
      "Epoch 4. Train Loss: -0.551616, Train Xent: 0.349974, Train Reconst: 0.026080, Train Pn: 0.003685, Train acc 0.910970, Valid Loss: 7.234946, Valid acc 0.940173, Valid acc2 0.934159, Valid accmax 0.932296, Valid accmin 0.936023, Best valid acc 0.940173, Best valid acc2 0.934159, Best valid accmax 0.932296, Best valid accmin 0.936023, Time 00:11:35, lr 0.001\n",
      "Epoch 4. Valid Loss EMA: 8.869704, Valid acc EMA 0.944092, Valid acc2 EMA 0.942747, Valid accmax EMA 0.942363, Valid accmin EMA 0.943132, Best valid acc EMA 0.944092, Best valid acc2 EMA 0.942747, Best valid accmax EMA 0.942363, Best valid accmin EMA 0.943132, Time 00:11:35\n",
      "Epoch 5. Train Loss: -0.580415, Train Xent: 0.337522, Train Reconst: 0.023350, Train Pn: 0.003664, Train acc 0.914214, Valid Loss: 1.197705, Valid acc 0.935019, Valid acc2 0.927031, Valid accmax 0.929579, Valid accmin 0.924483, Best valid acc 0.940173, Best valid acc2 0.934159, Best valid accmax 0.932296, Best valid accmin 0.936023, Time 00:11:34, lr 0.001\n",
      "Epoch 5. Valid Loss EMA: 10.053551, Valid acc EMA 0.948050, Valid acc2 EMA 0.946455, Valid accmax EMA 0.947781, Valid accmin EMA 0.945130, Best valid acc EMA 0.948050, Best valid acc2 EMA 0.946455, Best valid accmax EMA 0.947781, Best valid accmin EMA 0.945130, Time 00:11:34\n",
      "Epoch 6. Train Loss: -0.602440, Train Xent: 0.325826, Train Reconst: 0.023272, Train Pn: 0.003776, Train acc 0.917557, Valid Loss: 11.794236, Valid acc 0.938521, Valid acc2 0.931508, Valid accmax 0.934332, Valid accmin 0.928684, Best valid acc 0.940173, Best valid acc2 0.934159, Best valid accmax 0.934332, Best valid accmin 0.936023, Time 00:11:34, lr 0.001\n",
      "Epoch 6. Valid Loss EMA: 13.194460, Valid acc EMA 0.949856, Valid acc2 EMA 0.948127, Valid accmax EMA 0.948088, Valid accmin EMA 0.948165, Best valid acc EMA 0.949856, Best valid acc2 EMA 0.948127, Best valid accmax EMA 0.948088, Best valid accmin EMA 0.948165, Time 00:11:34\n",
      "Epoch 7. Train Loss: -0.620448, Train Xent: 0.315791, Train Reconst: 0.022783, Train Pn: 0.003848, Train acc 0.919850, Valid Loss: 13.146635, Valid acc 0.935985, Valid acc2 0.931028, Valid accmax 0.931873, Valid accmin 0.930183, Best valid acc 0.940173, Best valid acc2 0.934159, Best valid accmax 0.934332, Best valid accmin 0.936023, Time 00:11:34, lr 0.001\n",
      "Epoch 7. Valid Loss EMA: 12.902220, Valid acc EMA 0.949395, Valid acc2 EMA 0.947858, Valid accmax EMA 0.947896, Valid accmin EMA 0.947819, Best valid acc EMA 0.949856, Best valid acc2 EMA 0.948127, Best valid accmax EMA 0.948088, Best valid accmin EMA 0.948165, Time 00:11:34\n",
      "Epoch 8. Train Loss: -0.633994, Train Xent: 0.309184, Train Reconst: 0.022346, Train Pn: 0.003867, Train acc 0.921626, Valid Loss: 6.873544, Valid acc 0.935600, Valid acc2 0.928319, Valid accmax 0.929260, Valid accmin 0.927378, Best valid acc 0.940173, Best valid acc2 0.934159, Best valid accmax 0.934332, Best valid accmin 0.936023, Time 00:11:34, lr 0.001\n",
      "Epoch 8. Valid Loss EMA: 10.150614, Valid acc EMA 0.950048, Valid acc2 EMA 0.948915, Valid accmax EMA 0.949049, Valid accmin EMA 0.948780, Best valid acc EMA 0.950048, Best valid acc2 EMA 0.948915, Best valid accmax EMA 0.949049, Best valid accmin EMA 0.948780, Time 00:11:34\n",
      "Epoch 9. Train Loss: -0.643040, Train Xent: 0.302853, Train Reconst: 0.026464, Train Pn: 0.003948, Train acc 0.923275, Valid Loss: 6.292631, Valid acc 0.937906, Valid acc2 0.930778, Valid accmax 0.931374, Valid accmin 0.930183, Best valid acc 0.940173, Best valid acc2 0.934159, Best valid accmax 0.934332, Best valid accmin 0.936023, Time 00:11:35, lr 0.001\n",
      "Epoch 9. Valid Loss EMA: 8.381504, Valid acc EMA 0.953122, Valid acc2 EMA 0.951470, Valid accmax EMA 0.951854, Valid accmin EMA 0.951085, Best valid acc EMA 0.953122, Best valid acc2 EMA 0.951470, Best valid accmax EMA 0.951854, Best valid accmin EMA 0.951085, Time 00:11:35\n",
      "Epoch 10. Train Loss: -0.655013, Train Xent: 0.297655, Train Reconst: 0.023843, Train Pn: 0.003976, Train acc 0.925063, Valid Loss: 6.956906, Valid acc 0.937675, Valid acc2 0.930759, Valid accmax 0.933910, Valid accmin 0.927608, Best valid acc 0.940173, Best valid acc2 0.934159, Best valid accmax 0.934332, Best valid accmin 0.936023, Time 00:11:40, lr 0.001\n",
      "Epoch 10. Valid Loss EMA: 8.316549, Valid acc EMA 0.954636, Valid acc2 EMA 0.952050, Valid accmax EMA 0.952835, Valid accmin EMA 0.951264, Best valid acc EMA 0.954636, Best valid acc2 EMA 0.952050, Best valid accmax EMA 0.952835, Best valid accmin EMA 0.951264, Time 00:11:40\n",
      "Epoch 11. Train Loss: -0.661516, Train Xent: 0.295544, Train Reconst: 0.022194, Train Pn: 0.003889, Train acc 0.924736, Valid Loss: 5.573440, Valid acc 0.945898, Valid acc2 0.941864, Valid accmax 0.941479, Valid accmin 0.942248, Best valid acc 0.945898, Best valid acc2 0.941864, Best valid accmax 0.941479, Best valid accmin 0.942248, Time 00:11:42, lr 0.001\n",
      "Epoch 11. Valid Loss EMA: 4.608930, Valid acc EMA 0.954544, Valid acc2 EMA 0.952354, Valid accmax EMA 0.952738, Valid accmin EMA 0.951969, Best valid acc EMA 0.954636, Best valid acc2 EMA 0.952354, Best valid accmax EMA 0.952835, Best valid accmin EMA 0.951969, Time 00:11:42\n",
      "Epoch 12. Train Loss: -0.671467, Train Xent: 0.289563, Train Reconst: 0.022885, Train Pn: 0.003896, Train acc 0.927131, Valid Loss: 2.349626, Valid acc 0.946282, Valid acc2 0.939769, Valid accmax 0.940211, Valid accmin 0.939328, Best valid acc 0.946282, Best valid acc2 0.941864, Best valid accmax 0.941479, Best valid accmin 0.942248, Time 00:11:44, lr 0.001\n",
      "Epoch 12. Valid Loss EMA: 3.676639, Valid acc EMA 0.953814, Valid acc2 EMA 0.952142, Valid accmax EMA 0.951278, Valid accmin EMA 0.953007, Best valid acc EMA 0.954636, Best valid acc2 EMA 0.952354, Best valid accmax EMA 0.952835, Best valid accmin EMA 0.953007, Time 00:11:44\n",
      "Epoch 13. Train Loss: -0.675697, Train Xent: 0.287390, Train Reconst: 0.024819, Train Pn: 0.003906, Train acc 0.927369, Valid Loss: 2.797121, Valid acc 0.941018, Valid acc2 0.935139, Valid accmax 0.934640, Valid accmin 0.935639, Best valid acc 0.946282, Best valid acc2 0.941864, Best valid accmax 0.941479, Best valid accmin 0.942248, Time 00:11:40, lr 0.001\n",
      "Epoch 13. Valid Loss EMA: 2.551515, Valid acc EMA 0.954390, Valid acc2 EMA 0.952815, Valid accmax EMA 0.952853, Valid accmin EMA 0.952776, Best valid acc EMA 0.954636, Best valid acc2 EMA 0.952815, Best valid accmax EMA 0.952853, Best valid accmin EMA 0.953007, Time 00:11:40\n",
      "Epoch 14. Train Loss: -0.677685, Train Xent: 0.283584, Train Reconst: 0.028311, Train Pn: 0.003986, Train acc 0.928461, Valid Loss: 1.499026, Valid acc 0.942901, Valid acc2 0.937560, Valid accmax 0.941710, Valid accmin 0.933410, Best valid acc 0.946282, Best valid acc2 0.941864, Best valid accmax 0.941710, Best valid accmin 0.942248, Time 00:11:41, lr 0.001\n",
      "Epoch 14. Valid Loss EMA: 2.137502, Valid acc EMA 0.954928, Valid acc2 EMA 0.953429, Valid accmax EMA 0.954006, Valid accmin EMA 0.952853, Best valid acc EMA 0.954928, Best valid acc2 EMA 0.953429, Best valid accmax EMA 0.954006, Best valid accmin EMA 0.953007, Time 00:11:41\n",
      "Epoch 15. Train Loss: -0.676414, Train Xent: 0.284658, Train Reconst: 0.032815, Train Pn: 0.003956, Train acc 0.928093, Valid Loss: 3.841716, Valid acc 0.947166, Valid acc2 0.941306, Valid accmax 0.940903, Valid accmin 0.941710, Best valid acc 0.947166, Best valid acc2 0.941864, Best valid accmax 0.941710, Best valid accmin 0.942248, Time 00:11:46, lr 0.001\n",
      "Epoch 15. Valid Loss EMA: 2.755249, Valid acc EMA 0.954544, Valid acc2 EMA 0.952853, Valid accmax EMA 0.954121, Valid accmin EMA 0.951585, Best valid acc EMA 0.954928, Best valid acc2 EMA 0.953429, Best valid accmax EMA 0.954121, Best valid accmin EMA 0.953007, Time 00:11:46\n",
      "Epoch 16. Train Loss: -0.678624, Train Xent: 0.282000, Train Reconst: 0.036234, Train Pn: 0.003939, Train acc 0.928238, Valid Loss: 2.754672, Valid acc 0.947471, Valid acc2 0.941628, Valid accmax 0.942452, Valid accmin 0.940805, Best valid acc 0.947471, Best valid acc2 0.941864, Best valid accmax 0.942452, Best valid accmin 0.942248, Time 00:11:39, lr 0.001\n",
      "Epoch 16. Valid Loss EMA: 2.022815, Valid acc EMA 0.956619, Valid acc2 EMA 0.954563, Valid accmax EMA 0.955005, Valid accmin EMA 0.954121, Best valid acc EMA 0.956619, Best valid acc2 EMA 0.954563, Best valid accmax EMA 0.955005, Best valid accmin EMA 0.954121, Time 00:11:39\n",
      "Epoch 17. Train Loss: -0.680604, Train Xent: 0.280562, Train Reconst: 0.033149, Train Pn: 0.004093, Train acc 0.929171, Valid Loss: 2.100042, Valid acc 0.944976, Valid acc2 0.939443, Valid accmax 0.940672, Valid accmin 0.938213, Best valid acc 0.947471, Best valid acc2 0.941864, Best valid accmax 0.942452, Best valid accmin 0.942248, Time 00:11:45, lr 0.001\n",
      "Epoch 17. Valid Loss EMA: 1.488101, Valid acc EMA 0.954697, Valid acc2 EMA 0.952642, Valid accmax EMA 0.951431, Valid accmin EMA 0.953852, Best valid acc EMA 0.956619, Best valid acc2 EMA 0.954563, Best valid accmax EMA 0.955005, Best valid accmin EMA 0.954121, Time 00:11:45\n"
     ]
    }
   ],
   "source": [
    "run_train(1, ctx=opt.ctx)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
