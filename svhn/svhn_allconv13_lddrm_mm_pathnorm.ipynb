{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import mxnet as mx\n",
    "from mxnet import image\n",
    "from mxnet import nd, gluon, autograd, init\n",
    "from mxnet.gluon.data.vision import ImageFolderDataset\n",
    "from mxnet.gluon.data import DataLoader\n",
    "from mxnet.gluon import nn\n",
    "from tensorboardX import SummaryWriter\n",
    "import numpy as np\n",
    "import shutil\n",
    "import _pickle as cPickle\n",
    "from sklearn import preprocessing\n",
    "from mxnet.gluon.parameter import Parameter, ParameterDict\n",
    "from common.util import download_file\n",
    "import subprocess\n",
    "\n",
    "from IPython.core.debugger import Tracer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Options:\n",
    "    def __init__(self):\n",
    "        self.seed_val = 0\n",
    "        self.num_train_sup = 1000\n",
    "        self.batch_size = 128\n",
    "        self.data_dir = '/tanData/datasets/svhn'\n",
    "        self.log_dir = '/tanData/logs'\n",
    "        self.model_dir ='/tanData/models'\n",
    "        self.exp_name = 'svhn_nlabels_%i_allconv13_lddrm_mm_pathnorm_seed_%i'%(self.num_train_sup, self.seed_val)\n",
    "        self.ctx = mx.gpu(5)\n",
    "        self.alpha_drm = 0.5\n",
    "        self.alpha_pn = 1.0\n",
    "        self.alpha_kl = 0.5\n",
    "        self.alpha_nn = 0.5\n",
    "        \n",
    "        self.use_bias = True\n",
    "        self.use_bn = True\n",
    "        self.do_topdown = True\n",
    "        self.do_countpath = False\n",
    "        self.do_pn = True\n",
    "        self.relu_td = False\n",
    "        self.do_nn = True\n",
    "\n",
    "opt = Options()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(opt.data_dir, 'svhn_train.lst'), 'r') as f:\n",
    "    lines = f.readlines()\n",
    "    tokens = [i.rstrip().split('\\t') for i in lines]\n",
    "    idx_label = dict((int(idx), label) for idx, label, _ in tokens)\n",
    "    \n",
    "labels = set(idx_label.values())\n",
    "\n",
    "num_train = len(os.listdir(os.path.join(opt.data_dir, 'train')))\n",
    "\n",
    "num_train_sup = opt.num_train_sup\n",
    "\n",
    "num_train_sup_per_label = num_train_sup // len(labels)\n",
    "\n",
    "ratio_unsup_sup = (num_train - num_train_sup) // num_train_sup\n",
    "\n",
    "# select labeled data\n",
    "data_rng = np.random.RandomState(opt.seed_val)\n",
    "indx = data_rng.permutation(range(0, num_train))\n",
    "indx_sup = []\n",
    "\n",
    "for c in labels:\n",
    "    c_count = 0\n",
    "    for i in indx:\n",
    "        if idx_label[i] == c and c_count < num_train_sup_per_label:\n",
    "            indx_sup.append(i)\n",
    "            c_count += 1\n",
    "        if c_count >= num_train_sup_per_label:\n",
    "            break\n",
    "            \n",
    "label_count = dict()\n",
    "# print(indx_sup)\n",
    "# for i in indx_sup:\n",
    "#     print(idx_label[i])\n",
    "\n",
    "def mkdir_if_not_exist(path):\n",
    "    if not os.path.exists(os.path.join(*path)):\n",
    "        os.makedirs(os.path.join(*path))\n",
    "        \n",
    "for train_file in os.listdir(os.path.join(opt.data_dir, 'train')):\n",
    "    idx = int(train_file.split('.')[0])\n",
    "    label = idx_label[idx]\n",
    "    if idx in indx_sup:\n",
    "        mkdir_if_not_exist([opt.data_dir, 'train_valid_sup_nsup_%i_seed_%i'%(opt.num_train_sup, opt.seed_val), label])\n",
    "        for i in range(ratio_unsup_sup):\n",
    "            shutil.copy(os.path.join(opt.data_dir,'train', train_file),\n",
    "                        os.path.join(opt.data_dir, 'train_valid_sup_nsup_%i_seed_%i'%(opt.num_train_sup, opt.seed_val), label, '%i_%i.png'%(idx, i)))\n",
    "    else:\n",
    "        mkdir_if_not_exist([opt.data_dir, 'train_valid_unsup_nsup_%i_seed_%i'%(opt.num_train_sup, opt.seed_val), label])\n",
    "        shutil.copy(os.path.join(opt.data_dir,'train', train_file),\n",
    "                   os.path.join(opt.data_dir, 'train_valid_unsup_nsup_%i_seed_%i'%(opt.num_train_sup, opt.seed_val), label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0\n",
      "1 1\n",
      "2 2\n",
      "3 3\n",
      "4 4\n",
      "5 5\n",
      "6 6\n",
      "7 7\n",
      "8 8\n",
      "9 9\n",
      "Creating .rec file from /tanData/datasets/svhn/svhn_train_valid_sup_nsup_1000_seed_0.lst in /tanData/datasets/svhn\n",
      "time: 0.00288820266724  count: 0\n",
      "time: 0.0664539337158  count: 1000\n",
      "time: 0.0585939884186  count: 2000\n",
      "time: 0.0557899475098  count: 3000\n",
      "time: 0.0568461418152  count: 4000\n",
      "time: 0.05894780159  count: 5000\n",
      "time: 0.0648090839386  count: 6000\n",
      "time: 0.0610620975494  count: 7000\n",
      "time: 0.0570049285889  count: 8000\n",
      "time: 0.0585730075836  count: 9000\n",
      "time: 0.0569579601288  count: 10000\n",
      "time: 0.0546579360962  count: 11000\n",
      "time: 0.0615811347961  count: 12000\n",
      "time: 0.0521509647369  count: 13000\n",
      "time: 0.0509130954742  count: 14000\n",
      "time: 0.0539779663086  count: 15000\n",
      "time: 0.0516760349274  count: 16000\n",
      "time: 0.0499148368835  count: 17000\n",
      "time: 0.0468111038208  count: 18000\n",
      "time: 0.0478849411011  count: 19000\n",
      "time: 0.0483860969543  count: 20000\n",
      "time: 0.0504598617554  count: 21000\n",
      "time: 0.0529749393463  count: 22000\n",
      "time: 0.0521230697632  count: 23000\n",
      "time: 0.0522220134735  count: 24000\n",
      "time: 0.0479030609131  count: 25000\n",
      "time: 0.0529260635376  count: 26000\n",
      "time: 0.0528860092163  count: 27000\n",
      "time: 0.0585708618164  count: 28000\n",
      "time: 0.0539560317993  count: 29000\n",
      "time: 0.0517640113831  count: 30000\n",
      "time: 0.0567710399628  count: 31000\n",
      "time: 0.183146953583  count: 32000\n",
      "time: 0.0287508964539  count: 33000\n",
      "time: 0.0265340805054  count: 34000\n",
      "time: 0.0163099765778  count: 35000\n",
      "time: 0.0108330249786  count: 36000\n",
      "time: 0.0299389362335  count: 37000\n",
      "time: 0.0531339645386  count: 38000\n",
      "time: 0.0509920120239  count: 39000\n",
      "time: 0.0519840717316  count: 40000\n",
      "time: 0.0525019168854  count: 41000\n",
      "time: 0.0525391101837  count: 42000\n",
      "time: 0.052549123764  count: 43000\n",
      "time: 0.0518848896027  count: 44000\n",
      "time: 0.049290895462  count: 45000\n",
      "time: 0.0517830848694  count: 46000\n",
      "time: 0.0510160923004  count: 47000\n",
      "time: 0.0563678741455  count: 48000\n",
      "time: 0.0527701377869  count: 49000\n",
      "time: 0.0504968166351  count: 50000\n",
      "time: 0.0472769737244  count: 51000\n",
      "time: 0.0483601093292  count: 52000\n",
      "time: 0.049968957901  count: 53000\n",
      "time: 0.0538079738617  count: 54000\n",
      "time: 0.0509490966797  count: 55000\n",
      "time: 0.0498199462891  count: 56000\n",
      "time: 0.0497550964355  count: 57000\n",
      "time: 0.0535290241241  count: 58000\n",
      "time: 0.0581169128418  count: 59000\n",
      "time: 0.0473599433899  count: 60000\n",
      "time: 0.052304983139  count: 61000\n",
      "time: 0.0527610778809  count: 62000\n",
      "time: 0.0503981113434  count: 63000\n",
      "time: 0.0519669055939  count: 64000\n",
      "time: 0.0533430576324  count: 65000\n",
      "time: 0.0510590076447  count: 66000\n",
      "time: 0.0511178970337  count: 67000\n",
      "time: 0.0510399341583  count: 68000\n",
      "time: 0.0501761436462  count: 69000\n",
      "time: 0.0515048503876  count: 70000\n",
      "time: 0.0525190830231  count: 71000\n",
      "0 0\n",
      "1 1\n",
      "2 2\n",
      "3 3\n",
      "4 4\n",
      "5 5\n",
      "6 6\n",
      "7 7\n",
      "8 8\n",
      "9 9\n",
      "Creating .rec file from /tanData/datasets/svhn/svhn_train_valid_unsup_nsup_1000_seed_0.lst in /tanData/datasets/svhn\n",
      "time: 0.00091814994812  count: 0\n",
      "time: 0.0731809139252  count: 1000\n",
      "time: 0.0586371421814  count: 2000\n",
      "time: 0.0571298599243  count: 3000\n",
      "time: 0.0586740970612  count: 4000\n",
      "time: 0.0594339370728  count: 5000\n",
      "time: 0.0588510036469  count: 6000\n",
      "time: 0.0548129081726  count: 7000\n",
      "time: 0.0536000728607  count: 8000\n",
      "time: 0.0479791164398  count: 9000\n",
      "time: 0.0500078201294  count: 10000\n",
      "time: 0.0517790317535  count: 11000\n",
      "time: 0.051078081131  count: 12000\n",
      "time: 0.0517859458923  count: 13000\n",
      "time: 0.0528700351715  count: 14000\n",
      "time: 0.0522511005402  count: 15000\n",
      "time: 0.0531759262085  count: 16000\n",
      "time: 0.0543029308319  count: 17000\n",
      "time: 0.0531489849091  count: 18000\n",
      "time: 0.0552201271057  count: 19000\n",
      "time: 0.053190946579  count: 20000\n",
      "time: 0.0525529384613  count: 21000\n",
      "time: 0.0532419681549  count: 22000\n",
      "time: 0.0500509738922  count: 23000\n",
      "time: 0.0508871078491  count: 24000\n",
      "time: 0.050931930542  count: 25000\n",
      "time: 0.0482590198517  count: 26000\n",
      "time: 0.0489690303802  count: 27000\n",
      "time: 0.0531768798828  count: 28000\n",
      "time: 0.0482001304626  count: 29000\n",
      "time: 0.0520100593567  count: 30000\n",
      "time: 0.0499739646912  count: 31000\n",
      "time: 0.0520009994507  count: 32000\n",
      "time: 0.054939031601  count: 33000\n",
      "time: 0.17876291275  count: 34000\n",
      "time: 0.0285980701447  count: 35000\n",
      "time: 0.0249419212341  count: 36000\n",
      "time: 0.0127279758453  count: 37000\n",
      "time: 0.0112500190735  count: 38000\n",
      "time: 0.0421681404114  count: 39000\n",
      "time: 0.0516369342804  count: 40000\n",
      "time: 0.0533790588379  count: 41000\n",
      "time: 0.051442861557  count: 42000\n",
      "time: 0.0555739402771  count: 43000\n",
      "time: 0.0539000034332  count: 44000\n",
      "time: 0.0578830242157  count: 45000\n",
      "time: 0.0542471408844  count: 46000\n",
      "time: 0.0516059398651  count: 47000\n",
      "time: 0.0555279254913  count: 48000\n",
      "time: 0.0546860694885  count: 49000\n",
      "time: 0.0514500141144  count: 50000\n",
      "time: 0.0524559020996  count: 51000\n",
      "time: 0.0516550540924  count: 52000\n",
      "time: 0.0555119514465  count: 53000\n",
      "time: 0.0546550750732  count: 54000\n",
      "time: 0.0519759654999  count: 55000\n",
      "time: 0.0593321323395  count: 56000\n",
      "time: 0.0588757991791  count: 57000\n",
      "time: 0.0582010746002  count: 58000\n",
      "time: 0.0558109283447  count: 59000\n",
      "time: 0.0510799884796  count: 60000\n",
      "time: 0.0518081188202  count: 61000\n",
      "time: 0.0500240325928  count: 62000\n",
      "time: 0.0513348579407  count: 63000\n",
      "time: 0.0510201454163  count: 64000\n",
      "time: 0.0527899265289  count: 65000\n",
      "time: 0.0522420406342  count: 66000\n",
      "time: 0.0546228885651  count: 67000\n",
      "time: 0.0529940128326  count: 68000\n",
      "time: 0.0546231269836  count: 69000\n",
      "time: 0.0550098419189  count: 70000\n",
      "time: 0.0559420585632  count: 71000\n",
      "time: 0.0558180809021  count: 72000\n"
     ]
    }
   ],
   "source": [
    "%%bash -s \"$opt.num_train_sup\" \"$opt.seed_val\"\n",
    "DATA_DIR=/tanData/datasets/svhn\n",
    "data_name=( \"train_valid_sup_nsup_$1_seed_$2\" \"train_valid_unsup_nsup_$1_seed_$2\" )\n",
    "list_name=( \"svhn_train_valid_sup_nsup_$1_seed_$2\" \"svhn_train_valid_unsup_nsup_$1_seed_$2\" )\n",
    "MX_DIR=/mxnet\n",
    "\n",
    "for ((i=0;i<${#data_name[@]};++i)); do\n",
    "    # clean stuffs\n",
    "    rm -rf ${DATA_DIR}/${list_name[i]}.*\n",
    "    # make list for all classes\n",
    "    python ${MX_DIR}/tools/im2rec.py --list --exts '.png' --recursive ${DATA_DIR}/${list_name[i]} ${DATA_DIR}/${data_name[i]}\n",
    "    # make .rec file for all classes\n",
    "    python ${MX_DIR}/tools/im2rec.py --exts '.png' --quality 95 --num-thread 16 --color 1 ${DATA_DIR}/${list_name[i]} ${DATA_DIR}/${data_name[i]}\n",
    "    # remove folders\n",
    "    rm -rf ${DATA_DIR}/${data_name[i]}\n",
    "done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gpu_device(ctx=mx.gpu(0)):\n",
    "    try:\n",
    "        _ = mx.nd.array([1, 2, 3], ctx=ctx)\n",
    "    except mx.MXNetError:\n",
    "        return None\n",
    "    return ctx\n",
    "\n",
    "assert gpu_device(opt.ctx), 'No GPU device found!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = os.path.join(opt.log_dir, opt.exp_name)\n",
    "if not os.path.exists(log_dir):\n",
    "    os.makedirs(log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_shape = (3, 32, 32)\n",
    "train_data_unsup = mx.io.ImageRecordIter(\n",
    "    path_imgrec = os.path.join(opt.data_dir,'svhn_train_valid_unsup_nsup_%i_seed_%i.rec'%(opt.num_train_sup, opt.seed_val)),\n",
    "    data_shape  = data_shape,\n",
    "    batch_size  = opt.batch_size // 2,\n",
    "    mean_r             = 115.4,\n",
    "    mean_g             = 115.4,\n",
    "    mean_b             = 119.6,\n",
    "    std_r              = 56.0,\n",
    "    std_g              = 57.8,\n",
    "    std_b              = 58.3,\n",
    "    shuffle = True,\n",
    "    ## Data augmentation\n",
    "    rand_crop   = True,\n",
    "    max_crop_size = 32,\n",
    "    min_crop_size = 32,\n",
    "    pad = 4,\n",
    "    fill_value = 0,\n",
    "    rand_mirror = False)\n",
    "train_data_sup = mx.io.ImageRecordIter(\n",
    "    path_imgrec = os.path.join(opt.data_dir,'svhn_train_valid_sup_nsup_%i_seed_%i.rec'%(opt.num_train_sup, opt.seed_val)),\n",
    "    data_shape  = data_shape,\n",
    "    batch_size  = opt.batch_size // 2,\n",
    "    mean_r             = 115.4,\n",
    "    mean_g             = 115.4,\n",
    "    mean_b             = 119.6,\n",
    "    std_r              = 56.0,\n",
    "    std_g              = 57.8,\n",
    "    std_b              = 58.3,\n",
    "    shuffle = True,\n",
    "    ## Data augmentation\n",
    "    rand_crop   = True,\n",
    "    max_crop_size = 32,\n",
    "    min_crop_size = 32,\n",
    "    pad = 4,\n",
    "    fill_value = 0,\n",
    "    rand_mirror = False)\n",
    "valid_data = mx.io.ImageRecordIter(\n",
    "    path_imgrec = os.path.join(opt.data_dir,'svhn_val.rec'),\n",
    "    data_shape  = data_shape,\n",
    "    batch_size  = opt.batch_size // 2,\n",
    "    mean_r             = 115.4,\n",
    "    mean_g             = 115.4,\n",
    "    mean_b             = 119.6,\n",
    "    std_r              = 56.0,\n",
    "    std_g              = 57.8,\n",
    "    std_b              = 58.3,\n",
    "    ## No data augmentation\n",
    "    rand_crop   = False,\n",
    "    rand_mirror = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "L2_loss = gluon.loss.L2Loss()\n",
    "L1_loss = gluon.loss.L1Loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Normal(mx.init.Initializer):\n",
    "    \"\"\"Initializes weights with random values sampled from a normal distribution\n",
    "    with a mean and standard deviation of `sigma`.\n",
    "    \"\"\"\n",
    "    def __init__(self, mean=0, sigma=0.01):\n",
    "        super(Normal, self).__init__(sigma=sigma)\n",
    "        self.sigma = sigma\n",
    "        self.mean = mean\n",
    "\n",
    "    def _init_weight(self, _, arr):\n",
    "        mx.random.normal(self.mean, self.sigma, out=arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from resnet import ResNet164_v2\n",
    "from mxnet.gluon.model_zoo import vision as models\n",
    "from vgg_ld_opt import VGG_DRM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "writer = SummaryWriter(os.path.join(opt.log_dir, opt.exp_name))\n",
    "\n",
    "def get_acc(output, label):\n",
    "    pred = output.argmax(1, keepdims=False)\n",
    "    correct = (pred == label).sum()\n",
    "    return correct.asscalar()\n",
    "\n",
    "def extract_acts(net, x, layer_indx):\n",
    "    start_layer = 0\n",
    "    out = []\n",
    "    for i in layer_indx:\n",
    "        for block in net.features._children[start_layer:i]:\n",
    "            x = block(x)\n",
    "        out.append(x)\n",
    "        start_layer = i\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, train_data_unsup, train_data_sup, valid_data, num_epochs, lr, wd, ctx, lr_decay, relu_indx):\n",
    "    trainer = gluon.Trainer(\n",
    "        net.collect_params(), 'adam', {'learning_rate': 0.001, 'wd': wd})\n",
    "    \n",
    "    prev_time = datetime.datetime.now()\n",
    "    best_valid_acc = 0\n",
    "    iter_indx = 0\n",
    "    \n",
    "    for epoch in range(num_epochs-100):\n",
    "        train_data_unsup.reset(); train_data_sup.reset()\n",
    "        train_loss = 0; train_loss_xentropy = 0; train_loss_drm = 0; train_loss_pn = 0; train_loss_kl = 0; train_loss_nn = 0\n",
    "        correct = 0; total = 0\n",
    "        num_batch_train = 0\n",
    "        \n",
    "        if epoch == 20:\n",
    "            sgd_lr = 0.15\n",
    "            decay_val = np.exp(np.log(sgd_lr / 0.0001) / (num_epochs - 2))\n",
    "            sgd_lr = sgd_lr * decay_val\n",
    "            trainer = gluon.Trainer(net.collect_params(), 'SGD', {'learning_rate': sgd_lr, 'wd': wd})\n",
    "            \n",
    "        if epoch >= 20:\n",
    "            trainer.set_learning_rate(trainer.learning_rate / decay_val)\n",
    "        \n",
    "        for batch_unsup, batch_sup in zip(train_data_unsup, train_data_sup):\n",
    "            assert batch_unsup.data[0].shape[0] == batch_sup.data[0].shape[0], \"batch_unsup and batch_sup must have the same size\"\n",
    "            bs = batch_unsup.data[0].shape[0]\n",
    "            data_unsup = batch_unsup.data[0].as_in_context(ctx)\n",
    "            label_unsup = batch_unsup.label[0].as_in_context(ctx)\n",
    "            data_sup = batch_sup.data[0].as_in_context(ctx)\n",
    "            label_sup = batch_sup.label[0].as_in_context(ctx)\n",
    "            with autograd.record():\n",
    "                [output_unsup, xhat_unsup, _, loss_pn_unsup, loss_nn_unsup] = net(data_unsup)\n",
    "                loss_drm_unsup = L2_loss(xhat_unsup, data_unsup)\n",
    "                softmax_unsup = nd.softmax(output_unsup)\n",
    "                loss_kl_unsup = -nd.sum(nd.log(10.0*softmax_unsup + 1e-8) * softmax_unsup, axis=1)\n",
    "                loss_unsup = opt.alpha_drm * loss_drm_unsup + opt.alpha_kl * loss_kl_unsup + opt.alpha_nn * loss_nn_unsup + opt.alpha_pn * loss_pn_unsup\n",
    "                \n",
    "                [output_sup, xhat_sup, _, loss_pn_sup, loss_nn_sup] = net(data_sup, label_sup)\n",
    "                loss_xentropy_sup = criterion(output_sup, label_sup)\n",
    "                loss_drm_sup = L2_loss(xhat_sup, data_sup)\n",
    "                softmax_sup = nd.softmax(output_sup)\n",
    "                loss_kl_sup = -nd.sum(nd.log(10.0*softmax_sup + 1e-8) * softmax_sup, axis=1)\n",
    "                loss_sup = loss_xentropy_sup + opt.alpha_drm * loss_drm_sup + opt.alpha_kl * loss_kl_sup + opt.alpha_nn * loss_nn_sup + opt.alpha_pn * loss_pn_sup\n",
    "                \n",
    "                loss = loss_unsup + loss_sup\n",
    "                \n",
    "            loss.backward()\n",
    "            trainer.step(bs)\n",
    "            \n",
    "            loss_drm = loss_drm_unsup + loss_drm_sup\n",
    "            loss_pn = loss_pn_unsup + loss_pn_sup\n",
    "            loss_xentropy = loss_xentropy_sup\n",
    "            loss_kl = loss_kl_unsup + loss_kl_sup\n",
    "            loss_nn = loss_nn_unsup + loss_nn_sup\n",
    "            \n",
    "            train_loss_xentropy += nd.mean(loss_xentropy).asscalar()\n",
    "            train_loss_drm += nd.mean(loss_drm).asscalar()\n",
    "            train_loss_pn += nd.mean(loss_pn).asscalar()\n",
    "            train_loss_kl += nd.mean(loss_kl).asscalar()\n",
    "            train_loss_nn += nd.mean(loss_nn).asscalar()\n",
    "            train_loss += nd.mean(loss).asscalar()\n",
    "            correct += (get_acc(output_sup, label_sup) + get_acc(output_unsup, label_unsup))/2\n",
    "            \n",
    "            total += bs\n",
    "            num_batch_train += 1\n",
    "            iter_indx += 1\n",
    "        \n",
    "        writer.add_scalars('loss', {'train': train_loss / num_batch_train}, epoch)\n",
    "        writer.add_scalars('loss_xentropy', {'train': train_loss_xentropy / num_batch_train}, epoch)\n",
    "        writer.add_scalars('loss_drm', {'train': train_loss_drm / num_batch_train}, epoch)\n",
    "        writer.add_scalars('loss_pn', {'train': train_loss_pn / num_batch_train}, epoch)\n",
    "        writer.add_scalars('loss_kl', {'train': train_loss_kl / num_batch_train}, epoch)\n",
    "        writer.add_scalars('loss_nn', {'train': train_loss_nn / num_batch_train}, epoch)\n",
    "        writer.add_scalars('acc', {'train': correct / total}, epoch)\n",
    "        \n",
    "        cur_time = datetime.datetime.now()\n",
    "        h, remainder = divmod((cur_time - prev_time).seconds, 3600)\n",
    "        m, s = divmod(remainder, 60)\n",
    "        time_str = \"Time %02d:%02d:%02d\" % (h, m, s)\n",
    "        if valid_data is not None:\n",
    "            valid_data.reset()\n",
    "            valid_loss = 0; valid_loss_xentropy = 0; valid_loss_drm = 0; valid_loss_pn = 0; valid_loss_kl = 0; valid_loss_nn = 0\n",
    "            valid_correct = 0; valid_total = 0\n",
    "            num_batch_valid = 0\n",
    "            for batch in valid_data:\n",
    "                bs = batch.data[0].shape[0]\n",
    "                data = batch.data[0].as_in_context(ctx)\n",
    "                label = batch.label[0].as_in_context(ctx)\n",
    "                [output, xhat, _, loss_pn, loss_nn] = net(data, label)\n",
    "                loss_xentropy = criterion(output, label)\n",
    "                loss_drm = L2_loss(xhat, data)\n",
    "                softmax_val = nd.softmax(output)\n",
    "                loss_kl = -nd.sum(nd.log(10.0*softmax_val + 1e-8) * softmax_val, axis=1)\n",
    "                loss = loss_xentropy + opt.alpha_drm * loss_drm + opt.alpha_kl * loss_kl + opt.alpha_nn * loss_nn + opt.alpha_pn * loss_pn\n",
    "                \n",
    "                valid_loss_xentropy += nd.mean(loss_xentropy).asscalar()\n",
    "                valid_loss_drm += nd.mean(loss_drm).asscalar()\n",
    "                valid_loss_pn += nd.mean(loss_pn).asscalar()\n",
    "                valid_loss_kl += nd.mean(loss_kl).asscalar()\n",
    "                valid_loss_nn += nd.mean(loss_nn).asscalar()\n",
    "                valid_loss += nd.mean(loss).asscalar()\n",
    "                valid_correct += get_acc(output, label)\n",
    "                \n",
    "                valid_total += bs\n",
    "                num_batch_valid += 1\n",
    "            valid_acc = valid_correct / valid_total\n",
    "            if valid_acc > best_valid_acc:\n",
    "                best_valid_acc = valid_acc\n",
    "                net.collect_params().save('%s/%s_best.params'%(opt.model_dir, opt.exp_name))\n",
    "            writer.add_scalars('loss', {'valid': valid_loss / num_batch_valid}, epoch)\n",
    "            writer.add_scalars('loss_xentropy', {'valid': valid_loss_xentropy / num_batch_valid}, epoch)\n",
    "            writer.add_scalars('loss_drm', {'valid': valid_loss_drm / num_batch_valid}, epoch)\n",
    "            writer.add_scalars('loss_pn', {'valid': valid_loss_pn / num_batch_valid}, epoch)\n",
    "            writer.add_scalars('loss_kl', {'valid': valid_loss_kl / num_batch_valid}, epoch)\n",
    "            writer.add_scalars('loss_nn', {'valid': valid_loss_nn / num_batch_valid}, epoch)\n",
    "            writer.add_scalars('acc', {'valid': valid_acc}, epoch)\n",
    "            epoch_str = (\"Epoch %d. Train Loss: %f, Train Xent: %f, Train Reconst: %f, Train Pn: %f, Train acc %f, Valid Loss: %f, Valid acc %f, Best valid acc %f, \"\n",
    "                         % (epoch, train_loss / num_batch_train, train_loss_xentropy / num_batch_train, train_loss_drm / num_batch_train, train_loss_pn / num_batch_train,\n",
    "                            correct / total, valid_loss / num_batch_valid, valid_acc, best_valid_acc))\n",
    "            if not epoch % 20:\n",
    "                net.collect_params().save('%s/%s_epoch_%i.params'%(opt.model_dir, opt.exp_name, epoch))\n",
    "        else:\n",
    "            epoch_str = (\"Epoch %d. Loss: %f, Train acc %f, \"\n",
    "                         % (epoch, train_loss / num_batch_train,\n",
    "                            correct / total))\n",
    "        prev_time = cur_time\n",
    "        print(epoch_str + time_str + ', lr ' + str(trainer.learning_rate))\n",
    "        \n",
    "    return best_valid_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 500\n",
    "learning_rate = 0.15\n",
    "weight_decay = 5e-4\n",
    "lr_decay = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_train(num_exp, ctx):\n",
    "    valid_acc = 0\n",
    "    for i in range(num_exp):\n",
    "        ### CIFAR VGG_DRM\n",
    "        model = VGG_DRM('AllConv13', batch_size=opt.batch_size // 2, num_class=10, use_bias=opt.use_bias, use_bn=opt.use_bn, do_topdown=opt.do_topdown, do_countpath=opt.do_countpath, do_pn=opt.do_pn, relu_td=opt.relu_td, do_nn=opt.do_nn)\n",
    "        for param in model.collect_params().values():\n",
    "            if param.name.find('conv') != -1 or param.name.find('dense') != -1:\n",
    "                if param.name.find('weight') != -1:\n",
    "                    param.initialize(init=mx.initializer.Xavier(), ctx=ctx)\n",
    "                else:\n",
    "                    param.initialize(init=mx.init.Zero(), ctx=ctx)\n",
    "            elif param.name.find('batchnorm') != -1 or param.name.find('instancenorm') != -1:\n",
    "                if param.name.find('gamma') != -1:\n",
    "                    param.initialize(init=Normal(mean=1, sigma=0.02), ctx=ctx)\n",
    "                else:\n",
    "                    param.initialize(init=mx.init.Zero(), ctx=ctx)\n",
    "            elif param.name.find('biasadder') != -1:\n",
    "                param.initialize(init=mx.init.Zero(), ctx=ctx)\n",
    "                \n",
    "        # model.hybridize()\n",
    "        \n",
    "        relu_indx = []\n",
    "        \n",
    "        for i in range(len(model.features._children)):\n",
    "            if model.features._children[i].name.find('relu') != -1:\n",
    "                relu_indx.append(i)\n",
    "                \n",
    "        acc = train(model, train_data_unsup, train_data_sup, valid_data, num_epochs, learning_rate, weight_decay, ctx, lr_decay, relu_indx)\n",
    "        print('Validation Accuracy - Run %i = %f'%(i, acc))\n",
    "        valid_acc += acc\n",
    "\n",
    "    print('Validation Accuracy = %f'%(valid_acc/num_exp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_train(1, ctx=opt.ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
