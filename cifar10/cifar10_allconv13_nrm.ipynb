{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import mxnet as mx\n",
    "from mxnet import image\n",
    "from mxnet import nd, gluon, autograd, init\n",
    "from mxnet.gluon.data.vision import ImageFolderDataset\n",
    "from mxnet.gluon.data import DataLoader\n",
    "from mxnet.gluon import nn\n",
    "from tensorboardX import SummaryWriter\n",
    "import numpy as np\n",
    "import shutil\n",
    "import _pickle as cPickle\n",
    "from sklearn import preprocessing\n",
    "from mxnet.gluon.parameter import Parameter, ParameterDict\n",
    "import subprocess\n",
    "\n",
    "from IPython.core.debugger import Tracer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up parameters\n",
    "class Options:\n",
    "    def __init__(self):\n",
    "        self.seed_val = 0\n",
    "        self.num_train_sup = 4000\n",
    "        self.batch_size = 128\n",
    "        self.mount_point = '/datasets' # change this to your mount_point where you store your datasets\n",
    "        self.data_dir = os.path.join(self.mount_point,'datasets/cifar10')\n",
    "        self.log_dir = os.path.join(self.mount_point,'logs')\n",
    "        self.model_dir = os.path.join(self.mount_point,'models')\n",
    "        self.exp_name = 'cifar10_nlabels_%i_allconv13_lddrm_mm_pathnorm_seed_%i'%(self.num_train_sup, self.seed_val)\n",
    "        self.ctx = mx.gpu(2)\n",
    "        self.alpha_drm = 0.5\n",
    "        self.alpha_pn = 1.0\n",
    "        self.alpha_kl = 0.5\n",
    "        self.alpha_nn = 0.5\n",
    "        \n",
    "        self.use_bias = True\n",
    "        self.use_bn = True\n",
    "        self.do_topdown = True\n",
    "        self.do_countpath = False\n",
    "        self.do_pn = True\n",
    "        self.relu_td = False\n",
    "        self.do_nn = True\n",
    "\n",
    "opt = Options()\n",
    "if not os.path.exists(opt.log_dir):\n",
    "    os.makedirs(opt.log_dir)\n",
    "if not os.path.exists(opt.model_dir):\n",
    "    os.makedirs(opt.model_dir)\n",
    "if not os.path.exists(os.path.join(opt.mount_point,'datasets')):\n",
    "    os.makedirs(os.path.join(opt.mount_point,'datasets'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n",
      "11000\n",
      "12000\n",
      "13000\n",
      "14000\n",
      "15000\n",
      "16000\n",
      "17000\n",
      "18000\n",
      "19000\n",
      "20000\n",
      "21000\n",
      "22000\n",
      "23000\n",
      "24000\n",
      "25000\n",
      "26000\n",
      "27000\n",
      "28000\n",
      "29000\n",
      "30000\n",
      "31000\n",
      "32000\n",
      "33000\n",
      "34000\n",
      "35000\n",
      "36000\n",
      "37000\n",
      "38000\n",
      "39000\n",
      "40000\n",
      "41000\n",
      "42000\n",
      "43000\n",
      "44000\n",
      "45000\n",
      "46000\n",
      "47000\n",
      "48000\n",
      "49000\n",
      "50000\n"
     ]
    }
   ],
   "source": [
    "# This cell and the next cell prepare the data step 1\n",
    "with open(os.path.join(opt.data_dir, 'trainLabels.csv'), 'r') as f:\n",
    "    lines = f.readlines()[1:]\n",
    "    tokens = [i.rstrip().split(',') for i in lines]\n",
    "    idx_label = dict((int(idx), label) for idx, label in tokens)\n",
    "labels = set(idx_label.values())\n",
    "\n",
    "num_train = len(os.listdir(os.path.join(opt.data_dir, 'train')))\n",
    "\n",
    "num_train_tuning = int(num_train * (1 - 0.1))\n",
    "\n",
    "num_train_tuning_per_label = num_train_tuning // len(labels)\n",
    "\n",
    "num_train_sup = opt.num_train_sup\n",
    "\n",
    "num_train_sup_per_label = num_train_sup // len(labels)\n",
    "\n",
    "ratio_unsup_sup = (num_train - num_train_sup) // num_train_sup\n",
    "\n",
    "# select labeled data\n",
    "data_rng = np.random.RandomState(opt.seed_val)\n",
    "indx = data_rng.permutation(range(1, num_train + 1))\n",
    "indx_sup = []\n",
    "\n",
    "for c in labels:\n",
    "    c_count = 0\n",
    "    for i in indx:\n",
    "        if idx_label[i] == c and c_count < num_train_sup_per_label:\n",
    "            indx_sup.append(i)\n",
    "            c_count += 1\n",
    "        if c_count >= num_train_sup_per_label:\n",
    "            break\n",
    "            \n",
    "label_count = dict()\n",
    "# print(indx_sup)\n",
    "# for i in indx_sup:\n",
    "#     print(idx_label[i])\n",
    "\n",
    "def mkdir_if_not_exist(path):\n",
    "    if not os.path.exists(os.path.join(*path)):\n",
    "        os.makedirs(os.path.join(*path))\n",
    "\n",
    "count = 0\n",
    "for train_file in os.listdir(os.path.join(opt.data_dir, 'train')):\n",
    "    count +=1\n",
    "    if count % 1000 == 0:\n",
    "        print(count)\n",
    "    idx = int(train_file.split('.')[0])\n",
    "    label = idx_label[idx]\n",
    "    if idx in indx_sup:\n",
    "        mkdir_if_not_exist([opt.data_dir, 'train_sup_nsup_%i_seed_%i'%(opt.num_train_sup, opt.seed_val), label])\n",
    "        for i in range(ratio_unsup_sup):\n",
    "            shutil.copy(os.path.join(opt.data_dir,'train', train_file),\n",
    "                        os.path.join(opt.data_dir, 'train_sup_nsup_%i_seed_%i'%(opt.num_train_sup, opt.seed_val), label, '%i_%i.png'%(idx, i)))\n",
    "    else:\n",
    "        mkdir_if_not_exist([opt.data_dir, 'train_unsup_nsup_%i_seed_%i'%(opt.num_train_sup, opt.seed_val), label])\n",
    "        shutil.copy(os.path.join(opt.data_dir,'train', train_file),\n",
    "                   os.path.join(opt.data_dir, 'train_unsup_nsup_%i_seed_%i'%(opt.num_train_sup, opt.seed_val), label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "airplane 0\n",
      "automobile 1\n",
      "bird 2\n",
      "cat 3\n",
      "deer 4\n",
      "dog 5\n",
      "frog 6\n",
      "horse 7\n",
      "ship 8\n",
      "truck 9\n",
      "Creating .rec file from /datasets/datasets/cifar10/cifar10_train_sup_nsup_4000_seed_0.lst in /datasets/datasets/cifar10\n",
      "time: 0.0535910129547  count: 0\n",
      "time: 2.67630290985  count: 1000\n",
      "time: 2.96007514  count: 2000\n",
      "time: 2.41852998734  count: 3000\n",
      "time: 2.23183202744  count: 4000\n",
      "time: 2.13034081459  count: 5000\n",
      "time: 2.54197406769  count: 6000\n",
      "time: 2.44519996643  count: 7000\n",
      "time: 10.2404489517  count: 8000\n",
      "time: 2.40316104889  count: 9000\n",
      "time: 2.02689099312  count: 10000\n",
      "time: 2.07211399078  count: 11000\n",
      "time: 2.76019310951  count: 12000\n",
      "time: 2.19062590599  count: 13000\n",
      "time: 2.5096449852  count: 14000\n",
      "time: 2.40052819252  count: 15000\n",
      "time: 2.5314309597  count: 16000\n",
      "time: 2.3257689476  count: 17000\n",
      "time: 2.65269708633  count: 18000\n",
      "time: 1.96410298347  count: 19000\n",
      "time: 2.290184021  count: 20000\n",
      "time: 2.4212269783  count: 21000\n",
      "time: 2.39680981636  count: 22000\n",
      "time: 2.23287510872  count: 23000\n",
      "time: 2.59660196304  count: 24000\n",
      "time: 2.89724111557  count: 25000\n",
      "time: 2.42701792717  count: 26000\n",
      "time: 2.21362996101  count: 27000\n",
      "time: 2.27411103249  count: 28000\n",
      "time: 2.45464396477  count: 29000\n",
      "time: 2.33134412766  count: 30000\n",
      "time: 1.73117995262  count: 31000\n",
      "time: 2.21997690201  count: 32000\n",
      "time: 2.46092796326  count: 33000\n",
      "time: 2.18188905716  count: 34000\n",
      "time: 2.17542505264  count: 35000\n",
      "time: 9.38654088974  count: 36000\n",
      "time: 2.1628279686  count: 37000\n",
      "time: 2.27845907211  count: 38000\n",
      "time: 2.45791101456  count: 39000\n",
      "time: 2.17333006859  count: 40000\n",
      "time: 2.74550890923  count: 41000\n",
      "time: 2.63755106926  count: 42000\n",
      "time: 2.25566291809  count: 43000\n",
      "airplane 0\n",
      "automobile 1\n",
      "bird 2\n",
      "cat 3\n",
      "deer 4\n",
      "dog 5\n",
      "frog 6\n",
      "horse 7\n",
      "ship 8\n",
      "truck 9\n",
      "Creating .rec file from /datasets/datasets/cifar10/cifar10_train_unsup_nsup_4000_seed_0.lst in /datasets/datasets/cifar10\n",
      "time: 0.146204948425  count: 0\n",
      "time: 2.86047506332  count: 1000\n",
      "time: 2.92058992386  count: 2000\n",
      "time: 2.4118001461  count: 3000\n",
      "time: 2.4519469738  count: 4000\n",
      "time: 2.46889090538  count: 5000\n",
      "time: 2.60237503052  count: 6000\n",
      "time: 2.47156000137  count: 7000\n",
      "time: 2.19283008575  count: 8000\n",
      "time: 9.6243519783  count: 9000\n",
      "time: 2.364112854  count: 10000\n",
      "time: 2.58317399025  count: 11000\n",
      "time: 2.64790296555  count: 12000\n",
      "time: 2.91359615326  count: 13000\n",
      "time: 2.34053897858  count: 14000\n",
      "time: 2.67600893974  count: 15000\n",
      "time: 2.74183297157  count: 16000\n",
      "time: 2.44546198845  count: 17000\n",
      "time: 2.18828296661  count: 18000\n",
      "time: 2.38818311691  count: 19000\n",
      "time: 2.23045492172  count: 20000\n",
      "time: 2.51884913445  count: 21000\n",
      "time: 2.40662789345  count: 22000\n",
      "time: 2.74282693863  count: 23000\n",
      "time: 2.46049213409  count: 24000\n",
      "time: 2.6483130455  count: 25000\n",
      "time: 2.31271886826  count: 26000\n",
      "time: 2.61864304543  count: 27000\n",
      "time: 2.57347798347  count: 28000\n",
      "time: 2.30053806305  count: 29000\n",
      "time: 2.10464787483  count: 30000\n",
      "time: 2.57112717628  count: 31000\n",
      "time: 2.86647582054  count: 32000\n",
      "time: 2.52777409554  count: 33000\n",
      "time: 2.38824105263  count: 34000\n",
      "time: 9.97028684616  count: 35000\n",
      "time: 2.29585504532  count: 36000\n",
      "time: 2.5185110569  count: 37000\n",
      "time: 2.64561390877  count: 38000\n",
      "time: 2.82915019989  count: 39000\n",
      "time: 2.47913479805  count: 40000\n",
      "time: 2.61056303978  count: 41000\n",
      "time: 2.47240710258  count: 42000\n",
      "time: 2.75418186188  count: 43000\n",
      "time: 2.48807907104  count: 44000\n",
      "time: 2.42786192894  count: 45000\n"
     ]
    }
   ],
   "source": [
    "%%bash -s \"$opt.num_train_sup\" \"$opt.seed_val\"\n",
    "DATA_DIR=/datasets/datasets/cifar10\n",
    "data_name=( \"train_sup_nsup_$1_seed_$2\" \"train_unsup_nsup_$1_seed_$2\" )\n",
    "list_name=( \"cifar10_train_sup_nsup_$1_seed_$2\" \"cifar10_train_unsup_nsup_$1_seed_$2\" )\n",
    "MX_DIR=/mxnet\n",
    "\n",
    "for ((i=0;i<${#data_name[@]};++i)); do\n",
    "    # clean stuffs\n",
    "    rm -rf ${DATA_DIR}/${list_name[i]}.*\n",
    "    # make list for all classes\n",
    "    python ${MX_DIR}/tools/im2rec.py --list --exts '.png' --recursive ${DATA_DIR}/${list_name[i]} ${DATA_DIR}/${data_name[i]}\n",
    "    # make .rec file for all classes\n",
    "    python ${MX_DIR}/tools/im2rec.py --exts '.png' --quality 95 --num-thread 16 --color 1 ${DATA_DIR}/${list_name[i]} ${DATA_DIR}/${data_name[i]}\n",
    "    # remove folders\n",
    "    rm -rf ${DATA_DIR}/${data_name[i]}\n",
    "done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gpu_device(ctx=mx.gpu(0)):\n",
    "    try:\n",
    "        _ = mx.nd.array([1, 2, 3], ctx=ctx)\n",
    "    except mx.MXNetError:\n",
    "        return None\n",
    "    return ctx\n",
    "\n",
    "assert gpu_device(opt.ctx), 'No GPU device found!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = os.path.join(opt.log_dir, opt.exp_name)\n",
    "if not os.path.exists(log_dir):\n",
    "    os.makedirs(log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set data iterators\n",
    "data_shape = (3, 32, 32)\n",
    "train_data_unsup = mx.io.ImageRecordIter(\n",
    "    path_imgrec = os.path.join(opt.data_dir,'cifar10_train_unsup_nsup_%i_seed_%i.rec'%(opt.num_train_sup, opt.seed_val)),\n",
    "    data_shape  = data_shape,\n",
    "    batch_size  = opt.batch_size // 2,\n",
    "    mean_r             = 125.3,\n",
    "    mean_g             = 123.0,\n",
    "    mean_b             = 113.9,\n",
    "    std_r              = 63.0,\n",
    "    std_g              = 62.1,\n",
    "    std_b              = 66.7,\n",
    "    shuffle = True,\n",
    "    ## Data augmentation\n",
    "    rand_crop   = True,\n",
    "    max_crop_size = 32,\n",
    "    min_crop_size = 32,\n",
    "    pad = 4,\n",
    "    fill_value = 0,\n",
    "    rand_mirror = True)\n",
    "train_data_sup = mx.io.ImageRecordIter(\n",
    "    path_imgrec = os.path.join(opt.data_dir,'cifar10_train_sup_nsup_%i_seed_%i.rec'%(opt.num_train_sup, opt.seed_val)),\n",
    "    data_shape  = data_shape,\n",
    "    batch_size  = opt.batch_size // 2,\n",
    "    mean_r             = 125.3,\n",
    "    mean_g             = 123.0,\n",
    "    mean_b             = 113.9,\n",
    "    std_r              = 63.0,\n",
    "    std_g              = 62.1,\n",
    "    std_b              = 66.7,\n",
    "    shuffle = True,\n",
    "    ## Data augmentation\n",
    "    rand_crop   = True,\n",
    "    max_crop_size = 32,\n",
    "    min_crop_size = 32,\n",
    "    pad = 4,\n",
    "    fill_value = 0,\n",
    "    rand_mirror = True)\n",
    "valid_data = mx.io.ImageRecordIter(\n",
    "    path_imgrec = os.path.join(opt.data_dir,'cifar10_val.rec'),\n",
    "    data_shape  = data_shape,\n",
    "    batch_size  = opt.batch_size // 2,\n",
    "    mean_r             = 125.3,\n",
    "    mean_g             = 123.0,\n",
    "    mean_b             = 113.9,\n",
    "    std_r              = 63.0,\n",
    "    std_g              = 62.1,\n",
    "    std_b              = 66.7,\n",
    "    ## No data augmentation\n",
    "    rand_crop   = False,\n",
    "    rand_mirror = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set losses\n",
    "criterion = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "L2_loss = gluon.loss.L2Loss()\n",
    "L1_loss = gluon.loss.L1Loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the NRM\n",
    "from mxnet.gluon.model_zoo import vision as models\n",
    "from nrm import NRM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "writer = SummaryWriter(os.path.join(opt.log_dir, opt.exp_name))\n",
    "\n",
    "# this function is to initialize the networks with normal distribution\n",
    "class Normal(mx.init.Initializer):\n",
    "    \"\"\"Initializes weights with random values sampled from a normal distribution\n",
    "    with a mean and standard deviation of `sigma`.\n",
    "    \"\"\"\n",
    "    def __init__(self, mean=0, sigma=0.01):\n",
    "        super(Normal, self).__init__(sigma=sigma)\n",
    "        self.sigma = sigma\n",
    "        self.mean = mean\n",
    "\n",
    "    def _init_weight(self, _, arr):\n",
    "        mx.random.normal(self.mean, self.sigma, out=arr)\n",
    "\n",
    "# compute accuracy of the model\n",
    "def get_acc(output, label):\n",
    "    pred = output.argmax(1, keepdims=False)\n",
    "    correct = (pred == label).sum()\n",
    "    return correct.asscalar()\n",
    "\n",
    "# extract activations from the model\n",
    "def extract_acts(net, x, layer_indx):\n",
    "    start_layer = 0\n",
    "    out = []\n",
    "    for i in layer_indx:\n",
    "        for block in net.features._children[start_layer:i]:\n",
    "            x = block(x)\n",
    "        out.append(x)\n",
    "        start_layer = i\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main training loop\n",
    "def train(net, train_data_unsup, train_data_sup, valid_data, num_epochs, lr, wd, ctx, lr_decay, relu_indx):\n",
    "    trainer = gluon.Trainer(\n",
    "        net.collect_params(), 'adam', {'learning_rate': 0.001, 'wd': wd})\n",
    "    \n",
    "    prev_time = datetime.datetime.now()\n",
    "    best_valid_acc = 0\n",
    "    iter_indx = 0\n",
    "    \n",
    "    for epoch in range(num_epochs-100):\n",
    "        train_data_unsup.reset(); train_data_sup.reset()\n",
    "        train_loss = 0; train_loss_xentropy = 0; train_loss_drm = 0; train_loss_pn = 0; train_loss_kl = 0; train_loss_nn = 0\n",
    "        correct = 0; total = 0\n",
    "        num_batch_train = 0\n",
    "        \n",
    "        if epoch == 20:\n",
    "            sgd_lr = 0.15\n",
    "            decay_val = np.exp(np.log(sgd_lr / 0.0001) / (num_epochs - 2))\n",
    "            sgd_lr = sgd_lr * decay_val\n",
    "            trainer = gluon.Trainer(net.collect_params(), 'SGD', {'learning_rate': sgd_lr, 'wd': wd})\n",
    "            \n",
    "        if epoch >= 20:\n",
    "            trainer.set_learning_rate(trainer.learning_rate / decay_val)\n",
    "        \n",
    "        for batch_unsup, batch_sup in zip(train_data_unsup, train_data_sup):\n",
    "            assert batch_unsup.data[0].shape[0] == batch_sup.data[0].shape[0], \"batch_unsup and batch_sup must have the same size\"\n",
    "            bs = batch_unsup.data[0].shape[0]\n",
    "            data_unsup = batch_unsup.data[0].as_in_context(ctx)\n",
    "            label_unsup = batch_unsup.label[0].as_in_context(ctx)\n",
    "            data_sup = batch_sup.data[0].as_in_context(ctx)\n",
    "            label_sup = batch_sup.label[0].as_in_context(ctx)\n",
    "            with autograd.record():\n",
    "                [output_unsup, xhat_unsup, _, loss_pn_unsup, loss_nn_unsup] = net(data_unsup)\n",
    "                loss_drm_unsup = L2_loss(xhat_unsup, data_unsup)\n",
    "                softmax_unsup = nd.softmax(output_unsup)\n",
    "                loss_kl_unsup = -nd.sum(nd.log(10.0*softmax_unsup + 1e-8) * softmax_unsup, axis=1)\n",
    "                loss_unsup = opt.alpha_drm * loss_drm_unsup + opt.alpha_kl * loss_kl_unsup + opt.alpha_nn * loss_nn_unsup + opt.alpha_pn * loss_pn_unsup\n",
    "                \n",
    "                [output_sup, xhat_sup, _, loss_pn_sup, loss_nn_sup] = net(data_sup, label_sup)\n",
    "                loss_xentropy_sup = criterion(output_sup, label_sup)\n",
    "                loss_drm_sup = L2_loss(xhat_sup, data_sup)\n",
    "                softmax_sup = nd.softmax(output_sup)\n",
    "                loss_kl_sup = -nd.sum(nd.log(10.0*softmax_sup + 1e-8) * softmax_sup, axis=1)\n",
    "                loss_sup = loss_xentropy_sup + opt.alpha_drm * loss_drm_sup + opt.alpha_kl * loss_kl_sup + opt.alpha_nn * loss_nn_sup + opt.alpha_pn * loss_pn_sup\n",
    "                \n",
    "                loss = loss_unsup + loss_sup\n",
    "                \n",
    "            loss.backward()\n",
    "            trainer.step(bs)\n",
    "            \n",
    "            loss_drm = loss_drm_unsup + loss_drm_sup\n",
    "            loss_pn = loss_pn_unsup + loss_pn_sup\n",
    "            loss_xentropy = loss_xentropy_sup\n",
    "            loss_kl = loss_kl_unsup + loss_kl_sup\n",
    "            loss_nn = loss_nn_unsup + loss_nn_sup\n",
    "            \n",
    "            train_loss_xentropy += nd.mean(loss_xentropy).asscalar()\n",
    "            train_loss_drm += nd.mean(loss_drm).asscalar()\n",
    "            train_loss_pn += nd.mean(loss_pn).asscalar()\n",
    "            train_loss_kl += nd.mean(loss_kl).asscalar()\n",
    "            train_loss_nn += nd.mean(loss_nn).asscalar()\n",
    "            train_loss += nd.mean(loss).asscalar()\n",
    "            correct += (get_acc(output_sup, label_sup) + get_acc(output_unsup, label_unsup))/2\n",
    "            \n",
    "            total += bs\n",
    "            num_batch_train += 1\n",
    "            iter_indx += 1\n",
    "        \n",
    "        writer.add_scalars('loss', {'train': train_loss / num_batch_train}, epoch)\n",
    "        writer.add_scalars('loss_xentropy', {'train': train_loss_xentropy / num_batch_train}, epoch)\n",
    "        writer.add_scalars('loss_drm', {'train': train_loss_drm / num_batch_train}, epoch)\n",
    "        writer.add_scalars('loss_pn', {'train': train_loss_pn / num_batch_train}, epoch)\n",
    "        writer.add_scalars('loss_kl', {'train': train_loss_kl / num_batch_train}, epoch)\n",
    "        writer.add_scalars('loss_nn', {'train': train_loss_nn / num_batch_train}, epoch)\n",
    "        writer.add_scalars('acc', {'train': correct / total}, epoch)\n",
    "        \n",
    "        cur_time = datetime.datetime.now()\n",
    "        h, remainder = divmod((cur_time - prev_time).seconds, 3600)\n",
    "        m, s = divmod(remainder, 60)\n",
    "        time_str = \"Time %02d:%02d:%02d\" % (h, m, s)\n",
    "        if valid_data is not None:\n",
    "            valid_data.reset()\n",
    "            valid_loss = 0; valid_loss_xentropy = 0; valid_loss_drm = 0; valid_loss_pn = 0; valid_loss_kl = 0; valid_loss_nn = 0\n",
    "            valid_correct = 0; valid_total = 0\n",
    "            num_batch_valid = 0\n",
    "            for batch in valid_data:\n",
    "                bs = batch.data[0].shape[0]\n",
    "                data = batch.data[0].as_in_context(ctx)\n",
    "                label = batch.label[0].as_in_context(ctx)\n",
    "                [output, xhat, _, loss_pn, loss_nn] = net(data, label)\n",
    "                loss_xentropy = criterion(output, label)\n",
    "                loss_drm = L2_loss(xhat, data)\n",
    "                softmax_val = nd.softmax(output)\n",
    "                loss_kl = -nd.sum(nd.log(10.0*softmax_val + 1e-8) * softmax_val, axis=1)\n",
    "                loss = loss_xentropy + opt.alpha_drm * loss_drm + opt.alpha_kl * loss_kl + opt.alpha_nn * loss_nn + opt.alpha_pn * loss_pn\n",
    "                \n",
    "                valid_loss_xentropy += nd.mean(loss_xentropy).asscalar()\n",
    "                valid_loss_drm += nd.mean(loss_drm).asscalar()\n",
    "                valid_loss_pn += nd.mean(loss_pn).asscalar()\n",
    "                valid_loss_kl += nd.mean(loss_kl).asscalar()\n",
    "                valid_loss_nn += nd.mean(loss_nn).asscalar()\n",
    "                valid_loss += nd.mean(loss).asscalar()\n",
    "                valid_correct += get_acc(output, label)\n",
    "                \n",
    "                valid_total += bs\n",
    "                num_batch_valid += 1\n",
    "            valid_acc = valid_correct / valid_total\n",
    "            if valid_acc > best_valid_acc:\n",
    "                best_valid_acc = valid_acc\n",
    "                net.collect_params().save('%s/%s_best.params'%(opt.model_dir, opt.exp_name))\n",
    "            writer.add_scalars('loss', {'valid': valid_loss / num_batch_valid}, epoch)\n",
    "            writer.add_scalars('loss_xentropy', {'valid': valid_loss_xentropy / num_batch_valid}, epoch)\n",
    "            writer.add_scalars('loss_drm', {'valid': valid_loss_drm / num_batch_valid}, epoch)\n",
    "            writer.add_scalars('loss_pn', {'valid': valid_loss_pn / num_batch_valid}, epoch)\n",
    "            writer.add_scalars('loss_kl', {'valid': valid_loss_kl / num_batch_valid}, epoch)\n",
    "            writer.add_scalars('loss_nn', {'valid': valid_loss_nn / num_batch_valid}, epoch)\n",
    "            writer.add_scalars('acc', {'valid': valid_acc}, epoch)\n",
    "            epoch_str = (\"Epoch %d. Train Loss: %f, Train Xent: %f, Train Reconst: %f, Train Pn: %f, Train acc %f, Valid Loss: %f, Valid acc %f, Best valid acc %f, \"\n",
    "                         % (epoch, train_loss / num_batch_train, train_loss_xentropy / num_batch_train, train_loss_drm / num_batch_train, train_loss_pn / num_batch_train,\n",
    "                            correct / total, valid_loss / num_batch_valid, valid_acc, best_valid_acc))\n",
    "            if not epoch % 20:\n",
    "                net.collect_params().save('%s/%s_epoch_%i.params'%(opt.model_dir, opt.exp_name, epoch))\n",
    "        else:\n",
    "            epoch_str = (\"Epoch %d. Loss: %f, Train acc %f, \"\n",
    "                         % (epoch, train_loss / num_batch_train,\n",
    "                            correct / total))\n",
    "        prev_time = cur_time\n",
    "        print(epoch_str + time_str + ', lr ' + str(trainer.learning_rate))\n",
    "        \n",
    "    return best_valid_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 500\n",
    "learning_rate = 0.15\n",
    "weight_decay = 5e-4\n",
    "lr_decay = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_train(num_exp, ctx):\n",
    "    valid_acc = 0\n",
    "    for i in range(num_exp):\n",
    "        model = NRM('AllConv13', batch_size=opt.batch_size // 2, num_class=10, use_bias=opt.use_bias, use_bn=opt.use_bn, do_topdown=opt.do_topdown, do_countpath=opt.do_countpath, do_pn=opt.do_pn, relu_td=opt.relu_td, do_nn=opt.do_nn)\n",
    "        for param in model.collect_params().values():\n",
    "            if param.name.find('conv') != -1 or param.name.find('dense') != -1:\n",
    "                if param.name.find('weight') != -1:\n",
    "                    param.initialize(init=mx.initializer.Xavier(), ctx=ctx)\n",
    "                else:\n",
    "                    param.initialize(init=mx.init.Zero(), ctx=ctx)\n",
    "            elif param.name.find('batchnorm') != -1 or param.name.find('instancenorm') != -1:\n",
    "                if param.name.find('gamma') != -1:\n",
    "                    param.initialize(init=Normal(mean=1, sigma=0.02), ctx=ctx)\n",
    "                else:\n",
    "                    param.initialize(init=mx.init.Zero(), ctx=ctx)\n",
    "            elif param.name.find('biasadder') != -1:\n",
    "                param.initialize(init=mx.init.Zero(), ctx=ctx)\n",
    "        \n",
    "        # turn the following on for faster training but turn it off when you need to debug\n",
    "        # model.hybridize()\n",
    "        \n",
    "        relu_indx = []\n",
    "        \n",
    "        for i in range(len(model.features._children)):\n",
    "            if model.features._children[i].name.find('relu') != -1:\n",
    "                relu_indx.append(i)\n",
    "                \n",
    "        acc = train(model, train_data_unsup, train_data_sup, valid_data, num_epochs, learning_rate, weight_decay, ctx, lr_decay, relu_indx)\n",
    "        print('Validation Accuracy - Run %i = %f'%(i, acc))\n",
    "        valid_acc += acc\n",
    "\n",
    "    print('Validation Accuracy = %f'%(valid_acc/num_exp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0. Train Loss: 2.690662, Train Xent: 1.832340, Train Reconst: 0.437935, Train Pn: 0.017602, Train acc 0.415300, Valid Loss: 34.628414, Valid acc 0.507265, Best valid acc 0.507265, Time 00:03:03, lr 0.001\n",
      "Epoch 1. Train Loss: -0.139283, Train Xent: 1.148368, Train Reconst: 0.113638, Train Pn: 0.016606, Train acc 0.620121, Valid Loss: 34.974091, Valid acc 0.655248, Best valid acc 0.655248, Time 00:03:15, lr 0.001\n",
      "Epoch 2. Train Loss: -0.838509, Train Xent: 0.818256, Train Reconst: 0.051142, Train Pn: 0.011596, Train acc 0.706407, Valid Loss: 18.860687, Valid acc 0.709936, Best valid acc 0.709936, Time 00:03:01, lr 0.001\n",
      "Epoch 3. Train Loss: -1.167126, Train Xent: 0.642630, Train Reconst: 0.034016, Train Pn: 0.011473, Train acc 0.744178, Valid Loss: 15.969164, Valid acc 0.728265, Best valid acc 0.728265, Time 00:03:01, lr 0.001\n",
      "Epoch 4. Train Loss: -1.396407, Train Xent: 0.514232, Train Reconst: 0.026620, Train Pn: 0.010414, Train acc 0.770848, Valid Loss: 16.767806, Valid acc 0.738356, Best valid acc 0.738356, Time 00:03:01, lr 0.001\n",
      "Epoch 5. Train Loss: -1.523270, Train Xent: 0.436204, Train Reconst: 0.022574, Train Pn: 0.009563, Train acc 0.788574, Valid Loss: 12.111967, Valid acc 0.752003, Best valid acc 0.752003, Time 00:03:02, lr 0.001\n",
      "Epoch 6. Train Loss: -1.622843, Train Xent: 0.376321, Train Reconst: 0.020219, Train Pn: 0.008988, Train acc 0.799657, Valid Loss: 10.441814, Valid acc 0.744992, Best valid acc 0.752003, Time 00:03:01, lr 0.001\n",
      "Epoch 7. Train Loss: -1.691625, Train Xent: 0.332813, Train Reconst: 0.017868, Train Pn: 0.008614, Train acc 0.809646, Valid Loss: 11.824573, Valid acc 0.762320, Best valid acc 0.762320, Time 00:03:01, lr 0.001\n",
      "Epoch 8. Train Loss: -1.743094, Train Xent: 0.301766, Train Reconst: 0.016848, Train Pn: 0.008186, Train acc 0.816520, Valid Loss: 13.523417, Valid acc 0.751493, Best valid acc 0.762320, Time 00:03:02, lr 0.001\n",
      "Epoch 9. Train Loss: -1.778981, Train Xent: 0.279423, Train Reconst: 0.016292, Train Pn: 0.008101, Train acc 0.820199, Valid Loss: 16.856542, Valid acc 0.755108, Best valid acc 0.762320, Time 00:03:02, lr 0.001\n",
      "Epoch 10. Train Loss: -1.797737, Train Xent: 0.263235, Train Reconst: 0.016768, Train Pn: 0.008116, Train acc 0.824321, Valid Loss: 8.427146, Valid acc 0.767027, Best valid acc 0.767027, Time 00:03:01, lr 0.001\n",
      "Epoch 11. Train Loss: -1.824812, Train Xent: 0.246006, Train Reconst: 0.016689, Train Pn: 0.008127, Train acc 0.825441, Valid Loss: 9.682054, Valid acc 0.756711, Best valid acc 0.767027, Time 00:03:02, lr 0.001\n",
      "Epoch 12. Train Loss: -1.844252, Train Xent: 0.235561, Train Reconst: 0.019841, Train Pn: 0.008100, Train acc 0.828704, Valid Loss: 10.209948, Valid acc 0.759355, Best valid acc 0.767027, Time 00:03:02, lr 0.001\n",
      "Epoch 13. Train Loss: -1.868135, Train Xent: 0.218682, Train Reconst: 0.020166, Train Pn: 0.007866, Train acc 0.832935, Valid Loss: 10.702895, Valid acc 0.760216, Best valid acc 0.767027, Time 00:03:01, lr 0.001\n",
      "Epoch 14. Train Loss: -1.890649, Train Xent: 0.202717, Train Reconst: 0.020671, Train Pn: 0.007939, Train acc 0.835245, Valid Loss: 9.706601, Valid acc 0.767328, Best valid acc 0.767328, Time 00:03:01, lr 0.001\n",
      "Epoch 15. Train Loss: -1.894334, Train Xent: 0.203193, Train Reconst: 0.020016, Train Pn: 0.007728, Train acc 0.835414, Valid Loss: 8.914279, Valid acc 0.763421, Best valid acc 0.767328, Time 00:03:02, lr 0.001\n",
      "Epoch 16. Train Loss: -1.898068, Train Xent: 0.202233, Train Reconst: 0.019288, Train Pn: 0.008010, Train acc 0.836199, Valid Loss: 9.364774, Valid acc 0.748308, Best valid acc 0.767328, Time 00:03:01, lr 0.001\n",
      "Epoch 17. Train Loss: -1.921611, Train Xent: 0.185515, Train Reconst: 0.018937, Train Pn: 0.007471, Train acc 0.839861, Valid Loss: 5.735079, Valid acc 0.774038, Best valid acc 0.774038, Time 00:03:01, lr 0.001\n",
      "Epoch 18. Train Loss: -1.938454, Train Xent: 0.174079, Train Reconst: 0.019555, Train Pn: 0.007659, Train acc 0.842660, Valid Loss: 8.320557, Valid acc 0.747396, Best valid acc 0.774038, Time 00:03:02, lr 0.001\n",
      "Epoch 19. Train Loss: -1.920902, Train Xent: 0.187264, Train Reconst: 0.024328, Train Pn: 0.008160, Train acc 0.837746, Valid Loss: 5.959392, Valid acc 0.770833, Best valid acc 0.774038, Time 00:03:02, lr 0.001\n",
      "Epoch 20. Train Loss: -1.907946, Train Xent: 0.169915, Train Reconst: 0.045023, Train Pn: 0.019197, Train acc 0.841252, Valid Loss: 5.146283, Valid acc 0.771895, Best valid acc 0.774038, Time 00:03:00, lr 0.15\n",
      "Epoch 21. Train Loss: -2.007079, Train Xent: 0.112595, Train Reconst: 0.027975, Train Pn: 0.016980, Train acc 0.853632, Valid Loss: 5.180415, Valid acc 0.761919, Best valid acc 0.774038, Time 00:03:17, lr 0.14781331798330893\n",
      "Epoch 22. Train Loss: -2.017123, Train Xent: 0.110509, Train Reconst: 0.026115, Train Pn: 0.016916, Train acc 0.857388, Valid Loss: 4.927830, Valid acc 0.773337, Best valid acc 0.774038, Time 00:02:59, lr 0.14565851315489867\n",
      "Epoch 23. Train Loss: -2.023854, Train Xent: 0.108644, Train Reconst: 0.023629, Train Pn: 0.016757, Train acc 0.857294, Valid Loss: 4.080536, Valid acc 0.775240, Best valid acc 0.775240, Time 00:02:59, lr 0.14353512081294018\n",
      "Epoch 24. Train Loss: -2.024774, Train Xent: 0.108792, Train Reconst: 0.023210, Train Pn: 0.016444, Train acc 0.855639, Valid Loss: 4.051789, Valid acc 0.769805, Best valid acc 0.775240, Time 00:03:00, lr 0.14144268302997196\n",
      "Epoch 25. Train Loss: -2.029434, Train Xent: 0.105286, Train Reconst: 0.024597, Train Pn: 0.016477, Train acc 0.857828, Valid Loss: 4.431804, Valid acc 0.748898, Best valid acc 0.775240, Time 00:02:59, lr 0.13938074855414412\n",
      "Epoch 26. Train Loss: -2.040197, Train Xent: 0.098239, Train Reconst: 0.026147, Train Pn: 0.015798, Train acc 0.858205, Valid Loss: 3.859282, Valid acc 0.769631, Best valid acc 0.775240, Time 00:02:59, lr 0.13734887271190221\n",
      "Epoch 27. Train Loss: -2.026363, Train Xent: 0.110505, Train Reconst: 0.023655, Train Pn: 0.016389, Train acc 0.858044, Valid Loss: 4.031162, Valid acc 0.786859, Best valid acc 0.786859, Time 00:03:00, lr 0.1353466173120895\n",
      "Epoch 28. Train Loss: -2.033948, Train Xent: 0.104901, Train Reconst: 0.021130, Train Pn: 0.016025, Train acc 0.857547, Valid Loss: 3.744544, Valid acc 0.778065, Best valid acc 0.786859, Time 00:03:00, lr 0.1333735505514474\n",
      "Epoch 29. Train Loss: -2.043251, Train Xent: 0.100870, Train Reconst: 0.020931, Train Pn: 0.015195, Train acc 0.859307, Valid Loss: 3.896312, Valid acc 0.770232, Best valid acc 0.786859, Time 00:02:59, lr 0.1314292469214935\n",
      "Epoch 30. Train Loss: -2.052895, Train Xent: 0.092734, Train Reconst: 0.021871, Train Pn: 0.014851, Train acc 0.861339, Valid Loss: 4.200058, Valid acc 0.780749, Best valid acc 0.786859, Time 00:02:59, lr 0.12951328711675697\n",
      "Epoch 31. Train Loss: -2.057052, Train Xent: 0.091306, Train Reconst: 0.021152, Train Pn: 0.014698, Train acc 0.861934, Valid Loss: 3.734603, Valid acc 0.776142, Best valid acc 0.786859, Time 00:02:59, lr 0.12762525794435192\n",
      "Epoch 32. Train Loss: -2.056261, Train Xent: 0.092293, Train Reconst: 0.020875, Train Pn: 0.014595, Train acc 0.861022, Valid Loss: 3.967727, Valid acc 0.769805, Best valid acc 0.786859, Time 00:03:00, lr 0.12576475223486877\n",
      "Epoch 33. Train Loss: -2.049863, Train Xent: 0.096798, Train Reconst: 0.021803, Train Pn: 0.014801, Train acc 0.860728, Valid Loss: 3.775237, Valid acc 0.762119, Best valid acc 0.786859, Time 00:03:00, lr 0.12393136875456481\n",
      "Epoch 34. Train Loss: -2.049430, Train Xent: 0.097569, Train Reconst: 0.020683, Train Pn: 0.014457, Train acc 0.862271, Valid Loss: 3.836216, Valid acc 0.767428, Best valid acc 0.786859, Time 00:02:59, lr 0.12212471211883472\n",
      "Epoch 35. Train Loss: -2.078567, Train Xent: 0.077651, Train Reconst: 0.019523, Train Pn: 0.013488, Train acc 0.864640, Valid Loss: 3.476021, Valid acc 0.777644, Best valid acc 0.786859, Time 00:02:59, lr 0.12034439270694253\n",
      "Epoch 36. Train Loss: -2.075397, Train Xent: 0.081622, Train Reconst: 0.019057, Train Pn: 0.013474, Train acc 0.865121, Valid Loss: 3.789583, Valid acc 0.779658, Best valid acc 0.786859, Time 00:03:00, lr 0.11859002657799667\n",
      "Epoch 37. Train Loss: -2.061855, Train Xent: 0.090635, Train Reconst: 0.019918, Train Pn: 0.014254, Train acc 0.863901, Valid Loss: 2.574438, Valid acc 0.791266, Best valid acc 0.791266, Time 00:02:59, lr 0.11686123538814987\n",
      "Epoch 38. Train Loss: -2.068367, Train Xent: 0.086204, Train Reconst: 0.020851, Train Pn: 0.013518, Train acc 0.864792, Valid Loss: 2.891087, Valid acc 0.785857, Best valid acc 0.791266, Time 00:02:59, lr 0.11515764630900607\n",
      "Epoch 39. Train Loss: -2.072470, Train Xent: 0.083770, Train Reconst: 0.020281, Train Pn: 0.013383, Train acc 0.864765, Valid Loss: 3.444078, Valid acc 0.786558, Best valid acc 0.791266, Time 00:02:59, lr 0.11347889194721691\n",
      "Epoch 40. Train Loss: -2.070219, Train Xent: 0.086000, Train Reconst: 0.020459, Train Pn: 0.013597, Train acc 0.863293, Valid Loss: 3.496804, Valid acc 0.758459, Best valid acc 0.791266, Time 00:03:00, lr 0.1118246102652502\n",
      "Epoch 41. Train Loss: -2.095685, Train Xent: 0.068195, Train Reconst: 0.018757, Train Pn: 0.012092, Train acc 0.868518, Valid Loss: 3.176798, Valid acc 0.774139, Best valid acc 0.791266, Time 00:02:59, lr 0.11019444450331348\n",
      "Epoch 42. Train Loss: -2.079592, Train Xent: 0.080120, Train Reconst: 0.019421, Train Pn: 0.013046, Train acc 0.866381, Valid Loss: 3.092407, Valid acc 0.784856, Best valid acc 0.791266, Time 00:02:59, lr 0.10858804310241577\n",
      "Epoch 43. Train Loss: -2.086674, Train Xent: 0.074453, Train Reconst: 0.019755, Train Pn: 0.012836, Train acc 0.866266, Valid Loss: 3.495582, Valid acc 0.750801, Best valid acc 0.791266, Time 00:02:58, lr 0.10700505962855092\n",
      "Epoch 44. Train Loss: -2.082009, Train Xent: 0.079224, Train Reconst: 0.018400, Train Pn: 0.012895, Train acc 0.865461, Valid Loss: 2.942080, Valid acc 0.784534, Best valid acc 0.791266, Time 00:03:00, lr 0.10544515269798621\n",
      "Epoch 45. Train Loss: -2.083170, Train Xent: 0.078098, Train Reconst: 0.020658, Train Pn: 0.012645, Train acc 0.867949, Valid Loss: 2.701708, Valid acc 0.791567, Best valid acc 0.791567, Time 00:02:59, lr 0.10390798590364002\n",
      "Epoch 46. Train Loss: -2.097735, Train Xent: 0.067173, Train Reconst: 0.020776, Train Pn: 0.011702, Train acc 0.869674, Valid Loss: 2.268199, Valid acc 0.795873, Best valid acc 0.795873, Time 00:02:59, lr 0.10239322774253283\n",
      "Epoch 47. Train Loss: -2.096041, Train Xent: 0.068656, Train Reconst: 0.020520, Train Pn: 0.011698, Train acc 0.869394, Valid Loss: 2.689259, Valid acc 0.772035, Best valid acc 0.795873, Time 00:02:59, lr 0.10090055154429584\n",
      "Epoch 48. Train Loss: -2.091108, Train Xent: 0.073510, Train Reconst: 0.019176, Train Pn: 0.012179, Train acc 0.868153, Valid Loss: 2.739292, Valid acc 0.801055, Best valid acc 0.801055, Time 00:02:59, lr 0.0994296354007217\n",
      "Epoch 49. Train Loss: -2.103839, Train Xent: 0.065093, Train Reconst: 0.019891, Train Pn: 0.011246, Train acc 0.871600, Valid Loss: 2.963833, Valid acc 0.784856, Best valid acc 0.801055, Time 00:03:00, lr 0.09798016209634232\n",
      "Epoch 50. Train Loss: -2.100092, Train Xent: 0.068121, Train Reconst: 0.020219, Train Pn: 0.011502, Train acc 0.871241, Valid Loss: 2.698897, Valid acc 0.782252, Best valid acc 0.801055, Time 00:03:00, lr 0.09655181904001867\n",
      "Epoch 51. Train Loss: -2.101174, Train Xent: 0.068641, Train Reconst: 0.018948, Train Pn: 0.011432, Train acc 0.869519, Valid Loss: 3.021388, Valid acc 0.767127, Best valid acc 0.801055, Time 00:02:59, lr 0.09514429819752787\n",
      "Epoch 52. Train Loss: -2.095450, Train Xent: 0.071265, Train Reconst: 0.018194, Train Pn: 0.011628, Train acc 0.869958, Valid Loss: 2.784949, Valid acc 0.784932, Best valid acc 0.801055, Time 00:02:59, lr 0.09375729602513302\n",
      "Epoch 53. Train Loss: -2.107767, Train Xent: 0.062766, Train Reconst: 0.017977, Train Pn: 0.011042, Train acc 0.872282, Valid Loss: 2.856880, Valid acc 0.796675, Best valid acc 0.801055, Time 00:02:59, lr 0.09239051340412144\n",
      "Epoch 54. Train Loss: -2.108127, Train Xent: 0.064392, Train Reconst: 0.018627, Train Pn: 0.011108, Train acc 0.872411, Valid Loss: 2.592246, Valid acc 0.793069, Best valid acc 0.801055, Time 00:02:59, lr 0.09104365557629712\n",
      "Epoch 55. Train Loss: -2.115815, Train Xent: 0.058438, Train Reconst: 0.019371, Train Pn: 0.010467, Train acc 0.874534, Valid Loss: 2.605157, Valid acc 0.795974, Best valid acc 0.801055, Time 00:02:58, lr 0.08971643208041376\n",
      "Epoch 56. Train Loss: -2.113608, Train Xent: 0.058950, Train Reconst: 0.021097, Train Pn: 0.010619, Train acc 0.872956, Valid Loss: 2.841405, Valid acc 0.741640, Best valid acc 0.801055, Time 00:02:59, lr 0.08840855668953425\n",
      "Epoch 57. Train Loss: -2.112939, Train Xent: 0.060551, Train Reconst: 0.018442, Train Pn: 0.010483, Train acc 0.872521, Valid Loss: 3.176849, Valid acc 0.769431, Best valid acc 0.801055, Time 00:02:59, lr 0.08711974734930347\n",
      "Epoch 58. Train Loss: -2.118363, Train Xent: 0.057303, Train Reconst: 0.017905, Train Pn: 0.010533, Train acc 0.873433, Valid Loss: 2.684753, Valid acc 0.788462, Best valid acc 0.801055, Time 00:02:59, lr 0.08584972611712087\n",
      "Epoch 59. Train Loss: -2.125908, Train Xent: 0.052476, Train Reconst: 0.017069, Train Pn: 0.009783, Train acc 0.875534, Valid Loss: 2.880113, Valid acc 0.787660, Best valid acc 0.801055, Time 00:03:00, lr 0.08459821910219979\n",
      "Epoch 60. Train Loss: -2.118472, Train Xent: 0.058196, Train Reconst: 0.017771, Train Pn: 0.010235, Train acc 0.872456, Valid Loss: 3.087928, Valid acc 0.791998, Best valid acc 0.801055, Time 00:02:59, lr 0.08336495640650066\n",
      "Epoch 61. Train Loss: -2.110476, Train Xent: 0.063122, Train Reconst: 0.018656, Train Pn: 0.010637, Train acc 0.871554, Valid Loss: 3.139651, Valid acc 0.756110, Best valid acc 0.801055, Time 00:02:59, lr 0.08214967206652513\n",
      "Epoch 62. Train Loss: -2.121400, Train Xent: 0.056609, Train Reconst: 0.016992, Train Pn: 0.009897, Train acc 0.873899, Valid Loss: 2.544067, Valid acc 0.777945, Best valid acc 0.801055, Time 00:03:00, lr 0.08095210399595887\n",
      "Epoch 63. Train Loss: -2.125673, Train Xent: 0.054523, Train Reconst: 0.017642, Train Pn: 0.009646, Train acc 0.876524, Valid Loss: 2.632959, Valid acc 0.790264, Best valid acc 0.801055, Time 00:02:59, lr 0.07977199392915042\n",
      "Epoch 64. Train Loss: -2.133725, Train Xent: 0.049169, Train Reconst: 0.016932, Train Pn: 0.009263, Train acc 0.878486, Valid Loss: 2.548202, Valid acc 0.795979, Best valid acc 0.801055, Time 00:02:59, lr 0.07860908736541401\n",
      "Epoch 65. Train Loss: -2.130511, Train Xent: 0.051488, Train Reconst: 0.017428, Train Pn: 0.009494, Train acc 0.876501, Valid Loss: 2.508942, Valid acc 0.801182, Best valid acc 0.801182, Time 00:03:00, lr 0.07746313351414437\n",
      "Epoch 66. Train Loss: -2.133735, Train Xent: 0.048970, Train Reconst: 0.016214, Train Pn: 0.009216, Train acc 0.875443, Valid Loss: 2.524264, Valid acc 0.790164, Best valid acc 0.801182, Time 00:02:59, lr 0.07633388524073158\n",
      "Epoch 67. Train Loss: -2.143807, Train Xent: 0.042233, Train Reconst: 0.018083, Train Pn: 0.008455, Train acc 0.877934, Valid Loss: 3.046129, Valid acc 0.785857, Best valid acc 0.801182, Time 00:02:58, lr 0.07522109901326446\n",
      "Epoch 68. Train Loss: -2.132095, Train Xent: 0.049275, Train Reconst: 0.019117, Train Pn: 0.009284, Train acc 0.876215, Valid Loss: 2.682997, Valid acc 0.789510, Best valid acc 0.801182, Time 00:02:59, lr 0.07412453485001083\n",
      "Epoch 69. Train Loss: -2.139305, Train Xent: 0.046475, Train Reconst: 0.018256, Train Pn: 0.008694, Train acc 0.877445, Valid Loss: 2.430467, Valid acc 0.792668, Best valid acc 0.801182, Time 00:02:59, lr 0.07304395626766344\n"
     ]
    }
   ],
   "source": [
    "run_train(1, ctx=opt.ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
