{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import mxnet as mx\n",
    "from mxnet import image\n",
    "from mxnet import nd, gluon, autograd, init\n",
    "from mxnet.gluon.data.vision import ImageFolderDataset\n",
    "from mxnet.gluon.data import DataLoader\n",
    "from mxnet.gluon import nn\n",
    "from tensorboardX import SummaryWriter\n",
    "import numpy as np\n",
    "import shutil\n",
    "import _pickle as cPickle\n",
    "from sklearn import preprocessing\n",
    "from mxnet.gluon.parameter import Parameter, ParameterDict\n",
    "from common.util import download_file\n",
    "import subprocess\n",
    "\n",
    "from IPython.core.debugger import Tracer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Options:\n",
    "    def __init__(self):\n",
    "        self.seed_val = 0\n",
    "        self.num_train_sup = 4000\n",
    "        self.batch_size = 128\n",
    "        self.data_dir = '/tanData/datasets/cifar10'\n",
    "        self.log_dir = '/tanData/logs'\n",
    "        self.model_dir ='/tanData/models'\n",
    "        self.exp_name = 'cifar10_nlabels_%i_allconv13_lddrm_mm_pathnorm_meanteacher_seed_%i'%(self.num_train_sup, self.seed_val)\n",
    "        self.ctx = mx.gpu(2)\n",
    "        self.alpha_drm = 0.5\n",
    "        self.alpha_pn = 1.0\n",
    "        self.alpha_kl = 0.5\n",
    "        self.alpha_nn = 0.5\n",
    "        self.alpha_consistent = 33.0\n",
    "        \n",
    "        self.use_bias = True\n",
    "        self.use_bn = True\n",
    "        self.do_topdown = True\n",
    "        self.do_countpath = False\n",
    "        self.do_pn = True\n",
    "        self.relu_td = False\n",
    "        self.do_nn = True\n",
    "\n",
    "opt = Options()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Preparing the data step 1\n",
    "# with open(os.path.join(opt.data_dir, 'trainLabels.csv'), 'r') as f:\n",
    "#     lines = f.readlines()[1:]\n",
    "#     tokens = [i.rstrip().split(',') for i in lines]\n",
    "#     idx_label = dict((int(idx), label) for idx, label in tokens)\n",
    "# labels = set(idx_label.values())\n",
    "\n",
    "# num_train = len(os.listdir(os.path.join(opt.data_dir, 'train')))\n",
    "\n",
    "# num_train_tuning = int(num_train * (1 - 0.1))\n",
    "\n",
    "# num_train_tuning_per_label = num_train_tuning // len(labels)\n",
    "\n",
    "# num_train_sup = opt.num_train_sup\n",
    "\n",
    "# num_train_sup_per_label = num_train_sup // len(labels)\n",
    "\n",
    "# ratio_unsup_sup = (num_train - num_train_sup) // num_train_sup\n",
    "\n",
    "# # select labeled data\n",
    "# data_rng = np.random.RandomState(opt.seed_val)\n",
    "# indx = data_rng.permutation(range(1, num_train + 1))\n",
    "# indx_sup = []\n",
    "\n",
    "# for c in labels:\n",
    "#     c_count = 0\n",
    "#     for i in indx:\n",
    "#         if idx_label[i] == c and c_count < num_train_sup_per_label:\n",
    "#             indx_sup.append(i)\n",
    "#             c_count += 1\n",
    "#         if c_count >= num_train_sup_per_label:\n",
    "#             break\n",
    "            \n",
    "# label_count = dict()\n",
    "# # print(indx_sup)\n",
    "# # for i in indx_sup:\n",
    "# #     print(idx_label[i])\n",
    "\n",
    "# def mkdir_if_not_exist(path):\n",
    "#     if not os.path.exists(os.path.join(*path)):\n",
    "#         os.makedirs(os.path.join(*path))\n",
    "        \n",
    "# for train_file in os.listdir(os.path.join(opt.data_dir, 'train')):\n",
    "#     idx = int(train_file.split('.')[0])\n",
    "#     label = idx_label[idx]\n",
    "#     if idx in indx_sup:\n",
    "#         mkdir_if_not_exist([opt.data_dir, 'train_valid_sup_nsup_%i_seed_%i'%(opt.num_train_sup, opt.seed_val), label])\n",
    "#         for i in range(ratio_unsup_sup):\n",
    "#             shutil.copy(os.path.join(opt.data_dir,'train', train_file),\n",
    "#                         os.path.join(opt.data_dir, 'train_valid_sup_nsup_%i_seed_%i'%(opt.num_train_sup, opt.seed_val), label, '%i_%i.png'%(idx, i)))\n",
    "#     else:\n",
    "#         mkdir_if_not_exist([opt.data_dir, 'train_valid_unsup_nsup_%i_seed_%i'%(opt.num_train_sup, opt.seed_val), label])\n",
    "#         shutil.copy(os.path.join(opt.data_dir,'train', train_file),\n",
    "#                    os.path.join(opt.data_dir, 'train_valid_unsup_nsup_%i_seed_%i'%(opt.num_train_sup, opt.seed_val), label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%bash -s \"$opt.num_train_sup\" \"$opt.seed_val\"\n",
    "# DATA_DIR=/tanData/datasets/cifar10\n",
    "# data_name=( \"train_valid_sup_nsup_$1_seed_$2\" \"train_valid_unsup_nsup_$1_seed_$2\" )\n",
    "# list_name=( \"cifar10_train_valid_sup_nsup_$1_seed_$2\" \"cifar10_train_valid_unsup_nsup_$1_seed_$2\" )\n",
    "# MX_DIR=/mxnet\n",
    "\n",
    "# for ((i=0;i<${#data_name[@]};++i)); do\n",
    "#     # clean stuffs\n",
    "#     rm -rf ${DATA_DIR}/${list_name[i]}.*\n",
    "#     # make list for all classes\n",
    "#     python ${MX_DIR}/tools/im2rec.py --list --exts '.png' --recursive ${DATA_DIR}/${list_name[i]} ${DATA_DIR}/${data_name[i]}\n",
    "#     # make .rec file for all classes\n",
    "#     python ${MX_DIR}/tools/im2rec.py --exts '.png' --quality 95 --num-thread 16 --color 1 ${DATA_DIR}/${list_name[i]} ${DATA_DIR}/${data_name[i]}\n",
    "#     # remove folders\n",
    "#     rm -rf ${DATA_DIR}/${data_name[i]}\n",
    "# done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gpu_device(ctx=mx.gpu(0)):\n",
    "    try:\n",
    "        _ = mx.nd.array([1, 2, 3], ctx=ctx)\n",
    "    except mx.MXNetError:\n",
    "        return None\n",
    "    return ctx\n",
    "\n",
    "assert gpu_device(opt.ctx), 'No GPU device found!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = os.path.join(opt.log_dir, opt.exp_name)\n",
    "if not os.path.exists(log_dir):\n",
    "    os.makedirs(log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_shape = (3, 32, 32)\n",
    "train_data_unsup = mx.io.ImageRecordIter(\n",
    "    path_imgrec = os.path.join(opt.data_dir,'cifar10_train_valid_unsup_nsup_%i_seed_%i.rec'%(opt.num_train_sup, opt.seed_val)),\n",
    "    data_shape  = data_shape,\n",
    "    batch_size  = opt.batch_size // 2,\n",
    "    mean_r             = 125.3,\n",
    "    mean_g             = 123.0,\n",
    "    mean_b             = 113.9,\n",
    "    std_r              = 63.0,\n",
    "    std_g              = 62.1,\n",
    "    std_b              = 66.7,\n",
    "    shuffle = True,\n",
    "    ## Data augmentation\n",
    "    rand_crop   = True,\n",
    "    max_crop_size = 32,\n",
    "    min_crop_size = 32,\n",
    "    pad = 4,\n",
    "    fill_value = 0,\n",
    "    rand_mirror = True)\n",
    "train_data_sup = mx.io.ImageRecordIter(\n",
    "    path_imgrec = os.path.join(opt.data_dir,'cifar10_train_valid_sup_nsup_%i_seed_%i.rec'%(opt.num_train_sup, opt.seed_val)),\n",
    "    data_shape  = data_shape,\n",
    "    batch_size  = opt.batch_size // 2,\n",
    "    mean_r             = 125.3,\n",
    "    mean_g             = 123.0,\n",
    "    mean_b             = 113.9,\n",
    "    std_r              = 63.0,\n",
    "    std_g              = 62.1,\n",
    "    std_b              = 66.7,\n",
    "    shuffle = True,\n",
    "    ## Data augmentation\n",
    "    rand_crop   = True,\n",
    "    max_crop_size = 32,\n",
    "    min_crop_size = 32,\n",
    "    pad = 4,\n",
    "    fill_value = 0,\n",
    "    rand_mirror = True)\n",
    "valid_data = mx.io.ImageRecordIter(\n",
    "    path_imgrec = os.path.join(opt.data_dir,'cifar10_val.rec'),\n",
    "    data_shape  = data_shape,\n",
    "    batch_size  = opt.batch_size // 2,\n",
    "    mean_r             = 125.3,\n",
    "    mean_g             = 123.0,\n",
    "    mean_b             = 113.9,\n",
    "    std_r              = 63.0,\n",
    "    std_g              = 62.1,\n",
    "    std_b              = 66.7,\n",
    "    ## No data augmentation\n",
    "    rand_crop   = False,\n",
    "    rand_mirror = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "L2_loss = gluon.loss.L2Loss()\n",
    "L1_loss = gluon.loss.L1Loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Normal(mx.init.Initializer):\n",
    "    \"\"\"Initializes weights with random values sampled from a normal distribution\n",
    "    with a mean and standard deviation of `sigma`.\n",
    "    \"\"\"\n",
    "    def __init__(self, mean=0, sigma=0.01):\n",
    "        super(Normal, self).__init__(sigma=sigma)\n",
    "        self.sigma = sigma\n",
    "        self.mean = mean\n",
    "\n",
    "    def _init_weight(self, _, arr):\n",
    "        mx.random.normal(self.mean, self.sigma, out=arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from resnet import ResNet164_v2\n",
    "from mxnet.gluon.model_zoo import vision as models\n",
    "from vgg_ld_opt import VGG_DRM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "writer = SummaryWriter(os.path.join(opt.log_dir, opt.exp_name))\n",
    "\n",
    "def get_acc(output, label):\n",
    "    pred = output.argmax(1, keepdims=False)\n",
    "    correct = (pred == label).sum()\n",
    "    return correct.asscalar()\n",
    "\n",
    "def extract_acts(net, x, layer_indx):\n",
    "    start_layer = 0\n",
    "    out = []\n",
    "    for i in layer_indx:\n",
    "        for block in net.features._children[start_layer:i]:\n",
    "            x = block(x)\n",
    "        out.append(x)\n",
    "        start_layer = i\n",
    "    return out\n",
    "\n",
    "def update_ema_variables(model, ema_model, alpha, global_step):\n",
    "    # Use the true average until the exponential average is more correct\n",
    "    alpha = min(1 - 1 / (global_step + 1), alpha)\n",
    "    for ema_param, param in zip(ema_model.collect_params().values(), model.collect_params().values()):\n",
    "        ema_param.set_data(alpha * ema_param.data() + (1 - alpha) * param.data())\n",
    "        \n",
    "def test(net, valid_data, ctx):\n",
    "    valid_data.reset()\n",
    "    valid_loss = 0; valid_loss_xentropy = 0; valid_loss_drm = 0; valid_loss_pn = 0; valid_loss_kl = 0; valid_loss_nn = 0\n",
    "    valid_correct = 0; valid_total = 0\n",
    "    num_batch_valid = 0\n",
    "    for batch in valid_data:\n",
    "        bs = batch.data[0].shape[0]\n",
    "        data = batch.data[0].as_in_context(ctx)\n",
    "        label = batch.label[0].as_in_context(ctx)\n",
    "        [output, xhat, _, loss_pn, loss_nn] = net(data, label)\n",
    "        loss_xentropy = criterion(output, label)\n",
    "        loss_drm = L2_loss(xhat, data)\n",
    "        softmax_val = nd.softmax(output)\n",
    "        loss_kl = -nd.sum(nd.log(10.0*softmax_val + 1e-8) * softmax_val, axis=1)\n",
    "        loss = loss_xentropy + opt.alpha_drm * loss_drm + opt.alpha_kl * loss_kl + opt.alpha_nn * loss_nn + opt.alpha_pn * loss_pn\n",
    "\n",
    "        valid_loss_xentropy += nd.mean(loss_xentropy).asscalar()\n",
    "        valid_loss_drm += nd.mean(loss_drm).asscalar()\n",
    "        valid_loss_pn += nd.mean(loss_pn).asscalar()\n",
    "        valid_loss_kl += nd.mean(loss_kl).asscalar()\n",
    "        valid_loss_nn += nd.mean(loss_nn).asscalar()\n",
    "        valid_loss += nd.mean(loss).asscalar()\n",
    "        valid_correct += get_acc(output, label)\n",
    "\n",
    "        valid_total += bs\n",
    "        num_batch_valid += 1\n",
    "    valid_acc = valid_correct / valid_total\n",
    "    return valid_acc, valid_loss_xentropy, valid_loss_drm, valid_loss_pn, valid_loss_kl, valid_loss_nn, valid_loss, num_batch_valid\n",
    "\n",
    "def write_results(writer, name, valid_acc, valid_loss_xentropy, valid_loss_drm, valid_loss_pn, valid_loss_kl, valid_loss_nn, valid_loss, num_batch_valid, epoch):\n",
    "    writer.add_scalars('loss', {'%s'%name: valid_loss / num_batch_valid}, epoch)\n",
    "    writer.add_scalars('loss_xentropy', {'%s'%name: valid_loss_xentropy / num_batch_valid}, epoch)\n",
    "    writer.add_scalars('loss_drm', {'%s'%name: valid_loss_drm / num_batch_valid}, epoch)\n",
    "    writer.add_scalars('loss_pn', {'%s'%name: valid_loss_pn / num_batch_valid}, epoch)\n",
    "    writer.add_scalars('loss_kl', {'%s'%name: valid_loss_kl / num_batch_valid}, epoch)\n",
    "    writer.add_scalars('loss_nn', {'%s'%name: valid_loss_nn / num_batch_valid}, epoch)\n",
    "    writer.add_scalars('acc', {'%s'%name: valid_acc}, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, ema_net, train_data_unsup, train_data_sup, valid_data, num_epochs, lr, wd, ctx, lr_decay, relu_indx, ema_decay):\n",
    "    trainer = gluon.Trainer(\n",
    "        net.collect_params(), 'adam', {'learning_rate': 0.001, 'wd': wd})\n",
    "    \n",
    "    prev_time = datetime.datetime.now()\n",
    "    best_valid_acc = 0\n",
    "    best_valid_acc_ema = 0\n",
    "    iter_indx = 0\n",
    "    global_step = 0\n",
    "    \n",
    "    for epoch in range(num_epochs-100):\n",
    "        train_data_unsup.reset(); train_data_sup.reset()\n",
    "        train_loss = 0; train_loss_xentropy = 0; train_loss_drm = 0; train_loss_pn = 0; train_loss_kl = 0; train_loss_nn = 0\n",
    "        correct = 0; total = 0\n",
    "        num_batch_train = 0\n",
    "        \n",
    "        if epoch == 20:\n",
    "            sgd_lr = 0.15\n",
    "            decay_val = np.exp(np.log(sgd_lr / 0.0001) / (num_epochs - 2))\n",
    "            sgd_lr = sgd_lr * decay_val\n",
    "            trainer = gluon.Trainer(net.collect_params(), 'SGD', {'learning_rate': sgd_lr, 'wd': wd})\n",
    "            \n",
    "        if epoch >= 20:\n",
    "            trainer.set_learning_rate(trainer.learning_rate / decay_val)\n",
    "        \n",
    "        for batch_unsup, batch_sup in zip(train_data_unsup, train_data_sup):\n",
    "            assert batch_unsup.data[0].shape[0] == batch_sup.data[0].shape[0], \"batch_unsup and batch_sup must have the same size\"\n",
    "            bs = batch_unsup.data[0].shape[0]\n",
    "            data_unsup = batch_unsup.data[0].as_in_context(ctx)\n",
    "            label_unsup = batch_unsup.label[0].as_in_context(ctx)\n",
    "            data_sup = batch_sup.data[0].as_in_context(ctx)\n",
    "            label_sup = batch_sup.label[0].as_in_context(ctx)\n",
    "            with autograd.record():\n",
    "                [output_unsup, xhat_unsup, _, loss_pn_unsup, loss_nn_unsup] = net(data_unsup)\n",
    "                if global_step == 0:\n",
    "                    for ema_param, param in zip(ema_net.collect_params().values(),net.collect_params().values()):\n",
    "                        ema_param.initialize(init=mx.initializer.Constant(param.data()), ctx=ctx)\n",
    "                [output_unsup_ema, _, _, _, _] = ema_net(data_unsup)\n",
    "                loss_drm_unsup = L2_loss(xhat_unsup, data_unsup)\n",
    "                softmax_unsup = nd.softmax(output_unsup)\n",
    "                softmax_unsup_ema = nd.softmax(output_unsup_ema)\n",
    "                loss_kl_unsup = -nd.sum(nd.log(10.0*softmax_unsup + 1e-8) * softmax_unsup, axis=1)\n",
    "                loss_consistent_unsup = L2_loss(softmax_unsup, softmax_unsup_ema)\n",
    "                loss_unsup = opt.alpha_drm * loss_drm_unsup + opt.alpha_kl * loss_kl_unsup + opt.alpha_nn * loss_nn_unsup + opt.alpha_pn * loss_pn_unsup + opt.alpha_consistent * loss_consistent_unsup\n",
    "                \n",
    "                [output_sup, xhat_sup, _, loss_pn_sup, loss_nn_sup] = net(data_sup, label_sup)\n",
    "                [output_sup_ema, _, _, _, _] = ema_net(data_sup, label_sup)\n",
    "                loss_xentropy_sup = criterion(output_sup, label_sup)\n",
    "                loss_drm_sup = L2_loss(xhat_sup, data_sup)\n",
    "                softmax_sup = nd.softmax(output_sup)\n",
    "                softmax_sup_ema = nd.softmax(output_sup_ema)\n",
    "                loss_kl_sup = -nd.sum(nd.log(10.0*softmax_sup + 1e-8) * softmax_sup, axis=1)\n",
    "                loss_consistent_sup = L2_loss(softmax_sup, softmax_sup_ema)\n",
    "                loss_sup = loss_xentropy_sup + opt.alpha_drm * loss_drm_sup + opt.alpha_kl * loss_kl_sup + opt.alpha_nn * loss_nn_sup + opt.alpha_pn * loss_pn_sup + opt.alpha_consistent * loss_consistent_sup\n",
    "                \n",
    "                loss = loss_unsup + loss_sup\n",
    "                \n",
    "            loss.backward()\n",
    "            trainer.step(bs)\n",
    "            global_step += 1\n",
    "            update_ema_variables(net, ema_net, ema_decay, global_step)\n",
    "            \n",
    "            loss_drm = loss_drm_unsup + loss_drm_sup\n",
    "            loss_pn = loss_pn_unsup + loss_pn_sup\n",
    "            loss_xentropy = loss_xentropy_sup\n",
    "            loss_kl = loss_kl_unsup + loss_kl_sup\n",
    "            loss_nn = loss_nn_unsup + loss_nn_sup\n",
    "            \n",
    "            train_loss_xentropy += nd.mean(loss_xentropy).asscalar()\n",
    "            train_loss_drm += nd.mean(loss_drm).asscalar()\n",
    "            train_loss_pn += nd.mean(loss_pn).asscalar()\n",
    "            train_loss_kl += nd.mean(loss_kl).asscalar()\n",
    "            train_loss_nn += nd.mean(loss_nn).asscalar()\n",
    "            train_loss += nd.mean(loss).asscalar()\n",
    "            correct += (get_acc(output_sup, label_sup) + get_acc(output_unsup, label_unsup))/2\n",
    "            \n",
    "            total += bs\n",
    "            num_batch_train += 1\n",
    "            iter_indx += 1\n",
    "        \n",
    "        writer.add_scalars('loss', {'train': train_loss / num_batch_train}, epoch)\n",
    "        writer.add_scalars('loss_xentropy', {'train': train_loss_xentropy / num_batch_train}, epoch)\n",
    "        writer.add_scalars('loss_drm', {'train': train_loss_drm / num_batch_train}, epoch)\n",
    "        writer.add_scalars('loss_pn', {'train': train_loss_pn / num_batch_train}, epoch)\n",
    "        writer.add_scalars('loss_kl', {'train': train_loss_kl / num_batch_train}, epoch)\n",
    "        writer.add_scalars('loss_nn', {'train': train_loss_nn / num_batch_train}, epoch)\n",
    "        writer.add_scalars('acc', {'train': correct / total}, epoch)\n",
    "        \n",
    "        cur_time = datetime.datetime.now()\n",
    "        h, remainder = divmod((cur_time - prev_time).seconds, 3600)\n",
    "        m, s = divmod(remainder, 60)\n",
    "        time_str = \"Time %02d:%02d:%02d\" % (h, m, s)\n",
    "        if valid_data is not None:\n",
    "            valid_acc, valid_loss_xentropy, valid_loss_drm, valid_loss_pn, valid_loss_kl, valid_loss_nn, valid_loss, num_batch_valid = test(net, valid_data, ctx)\n",
    "            if valid_acc > best_valid_acc:\n",
    "                best_valid_acc = valid_acc\n",
    "                net.collect_params().save('%s/%s_best.params'%(opt.model_dir, opt.exp_name))\n",
    "            write_results(writer, 'valid', valid_acc, valid_loss_xentropy, valid_loss_drm, valid_loss_pn, valid_loss_kl, valid_loss_nn, valid_loss, num_batch_valid, epoch)\n",
    "            epoch_str = (\"Epoch %d. Train Loss: %f, Train Xent: %f, Train Reconst: %f, Train Pn: %f, Train acc %f, Valid Loss: %f, Valid acc %f, Best valid acc %f, \"\n",
    "                         % (epoch, train_loss / num_batch_train, train_loss_xentropy / num_batch_train, train_loss_drm / num_batch_train, train_loss_pn / num_batch_train,\n",
    "                            correct / total, valid_loss / num_batch_valid, valid_acc, best_valid_acc))\n",
    "            \n",
    "            valid_acc, valid_loss_xentropy, valid_loss_drm, valid_loss_pn, valid_loss_kl, valid_loss_nn, valid_loss, num_batch_valid = test(ema_net, valid_data, ctx)\n",
    "            if valid_acc > best_valid_acc_ema:\n",
    "                best_valid_acc_ema = valid_acc\n",
    "                net.collect_params().save('%s/%s_ema_best.params'%(opt.model_dir, opt.exp_name))\n",
    "            write_results(writer, 'valid_ema', valid_acc, valid_loss_xentropy, valid_loss_drm, valid_loss_pn, valid_loss_kl, valid_loss_nn, valid_loss, num_batch_valid, epoch)    \n",
    "            epoch_str_ema = (\"Epoch %d. Valid Loss EMA: %f, Valid acc EMA %f, Best valid acc EMA %f, \"\n",
    "                         % (epoch, valid_loss / num_batch_valid, valid_acc, best_valid_acc))\n",
    "            if not epoch % 20:\n",
    "                net.collect_params().save('%s/%s_epoch_%i.params'%(opt.model_dir, opt.exp_name, epoch))\n",
    "                net.collect_params().save('%s/%s_ema_epoch_%i.params'%(opt.model_dir, opt.exp_name, epoch))\n",
    "        else:\n",
    "            epoch_str = (\"Epoch %d. Loss: %f, Train acc %f, \"\n",
    "                         % (epoch, train_loss / num_batch_train,\n",
    "                            correct / total))\n",
    "        prev_time = cur_time\n",
    "        print(epoch_str + time_str + ', lr ' + str(trainer.learning_rate))\n",
    "        print(epoch_str_ema + time_str)\n",
    "        \n",
    "    return best_valid_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 500\n",
    "learning_rate = 0.15\n",
    "weight_decay = 5e-4\n",
    "lr_decay = 0.1\n",
    "ema_decay = 0.999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_train(num_exp, ctx):\n",
    "    valid_acc = 0\n",
    "    for i in range(num_exp):\n",
    "        ### CIFAR VGG_DRM\n",
    "        model = VGG_DRM('AllConv13', batch_size=opt.batch_size // 2, num_class=10, use_bias=opt.use_bias, use_bn=opt.use_bn, do_topdown=opt.do_topdown, do_countpath=opt.do_countpath, do_pn=opt.do_pn, relu_td=opt.relu_td, do_nn=opt.do_nn)\n",
    "        for param in model.collect_params().values():\n",
    "            if param.name.find('conv') != -1 or param.name.find('dense') != -1:\n",
    "                if param.name.find('weight') != -1:\n",
    "                    param.initialize(init=mx.initializer.Xavier(), ctx=ctx)\n",
    "                else:\n",
    "                    param.initialize(init=mx.init.Zero(), ctx=ctx)\n",
    "            elif param.name.find('batchnorm') != -1 or param.name.find('instancenorm') != -1:\n",
    "                if param.name.find('gamma') != -1:\n",
    "                    param.initialize(init=Normal(mean=1, sigma=0.02), ctx=ctx)\n",
    "                else:\n",
    "                    param.initialize(init=mx.init.Zero(), ctx=ctx)\n",
    "            elif param.name.find('biasadder') != -1:\n",
    "                param.initialize(init=mx.init.Zero(), ctx=ctx)\n",
    "        \n",
    "        ema_model = VGG_DRM('AllConv13', batch_size=opt.batch_size // 2, num_class=10, use_bias=opt.use_bias, use_bn=opt.use_bn, do_topdown=opt.do_topdown, do_countpath=opt.do_countpath, do_pn=opt.do_pn, relu_td=opt.relu_td, do_nn=opt.do_nn)\n",
    "        # model.hybridize()\n",
    "        \n",
    "        relu_indx = []\n",
    "        \n",
    "        for i in range(len(model.features._children)):\n",
    "            if model.features._children[i].name.find('relu') != -1:\n",
    "                relu_indx.append(i)\n",
    "                \n",
    "        acc = train(model, ema_model, train_data_unsup, train_data_sup, valid_data, num_epochs, learning_rate, weight_decay, ctx, lr_decay, relu_indx, ema_decay)\n",
    "        print('Validation Accuracy - Run %i = %f'%(i, acc))\n",
    "        valid_acc += acc\n",
    "\n",
    "    print('Validation Accuracy = %f'%(valid_acc/num_exp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:23: DeprecationWarning: `Tracer` is deprecated since version 5.1, directly use `IPython.core.debugger.Pdb.set_trace()`\n",
      "/usr/local/lib/python3.5/dist-packages/IPython/core/debugger.py:168: DeprecationWarning: The `color_scheme` argument is deprecated since version 5.1\n",
      "  self.debugger = Pdb(colors)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m<ipython-input-11-936e8ed02083>\u001b[0m(24)\u001b[0;36mupdate_ema_variables\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     22 \u001b[0;31m    \u001b[0;32mfor\u001b[0m \u001b[0mema_param\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mema_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     23 \u001b[0;31m        \u001b[0mTracer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 24 \u001b[0;31m        \u001b[0mema_param\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mema_param\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     25 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     26 \u001b[0;31m\u001b[0;32mdef\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "\n",
      "[[[[ 0.00696448  0.01324663  0.0307022 ]\n",
      "   [ 0.04911821  0.01466179  0.05106999]\n",
      "   [ 0.00640372  0.04954424 -0.01089257]]\n",
      "\n",
      "  [[ 0.01762949  0.02081548 -0.01649587]\n",
      "   [-0.00890476 -0.02888681  0.05589633]\n",
      "   [-0.0632461   0.06615321 -0.03243633]]\n",
      "\n",
      "  [[-0.01663002 -0.00318664  0.04162195]\n",
      "   [ 0.04453876  0.00412259 -0.00285676]\n",
      "   [ 0.00970828 -0.01529696  0.06072213]]]\n",
      "\n",
      "\n",
      " [[[ 0.04795013 -0.06120255 -0.02319955]\n",
      "   [-0.05890644  0.02114047 -0.06845298]\n",
      "   [-0.01879868  0.04745663  0.06522474]]\n",
      "\n",
      "  [[ 0.03968609 -0.05131305  0.05279159]\n",
      "   [ 0.0528023   0.068287   -0.00376548]\n",
      "   [ 0.04268254  0.04293253 -0.00549594]]\n",
      "\n",
      "  [[ 0.00292163  0.04002459  0.02552169]\n",
      "   [-0.05446281  0.03147883  0.01996327]\n",
      "   [ 0.0117022  -0.05088467  0.00533224]]]\n",
      "\n",
      "\n",
      " [[[ 0.06344326  0.03689805  0.00311722]\n",
      "   [-0.05622724 -0.01217563 -0.00376657]\n",
      "   [-0.03359209 -0.04475262  0.03912637]]\n",
      "\n",
      "  [[ 0.03380236 -0.01183891  0.07093598]\n",
      "   [ 0.03143486  0.06171525 -0.07132132]\n",
      "   [-0.05305745 -0.02820225  0.07120074]]\n",
      "\n",
      "  [[-0.0503992  -0.03765358 -0.05816321]\n",
      "   [-0.01475538 -0.04476291 -0.01599237]\n",
      "   [-0.02203467  0.02421857 -0.01472873]]]\n",
      "\n",
      "\n",
      " ...\n",
      "\n",
      "\n",
      " [[[-0.00154684  0.0345665   0.06681287]\n",
      "   [ 0.02252969 -0.00877029 -0.04261471]\n",
      "   [ 0.03010297  0.0440136   0.0472231 ]]\n",
      "\n",
      "  [[ 0.04086255  0.0109887  -0.01498952]\n",
      "   [ 0.05658969  0.00151467  0.06279789]\n",
      "   [ 0.04225464  0.0407942   0.04946016]]\n",
      "\n",
      "  [[ 0.02550392  0.00872681  0.04940352]\n",
      "   [-0.00643817 -0.06648869 -0.02109099]\n",
      "   [ 0.03914028  0.01224887  0.03199252]]]\n",
      "\n",
      "\n",
      " [[[ 0.0051006  -0.00671811  0.04671992]\n",
      "   [-0.00161751 -0.03359401  0.05316024]\n",
      "   [-0.00308477  0.03362215 -0.02171731]]\n",
      "\n",
      "  [[ 0.01409359 -0.05215091  0.0550102 ]\n",
      "   [-0.0342453  -0.029319   -0.06454135]\n",
      "   [-0.06998832 -0.01508173  0.00375927]]\n",
      "\n",
      "  [[-0.0183172   0.06992392  0.00809439]\n",
      "   [-0.06189484  0.05671264  0.068427  ]\n",
      "   [-0.04746152  0.03483297 -0.01323935]]]\n",
      "\n",
      "\n",
      " [[[-0.00048888  0.02173705  0.06906885]\n",
      "   [ 0.00966612  0.01611008  0.00949059]\n",
      "   [ 0.0318876   0.00347488 -0.05219782]]\n",
      "\n",
      "  [[ 0.01534901 -0.00896003 -0.05304269]\n",
      "   [ 0.05959062 -0.04432555  0.03265344]\n",
      "   [-0.00839107  0.05970035  0.05165331]]\n",
      "\n",
      "  [[ 0.01084733  0.05663992  0.0507386 ]\n",
      "   [-0.03867506  0.04468184 -0.00481089]\n",
      "   [-0.05121938 -0.00420017  0.04206724]]]]\n",
      "<NDArray 128x3x3x3 @gpu(2)>\n",
      "\n",
      "[[[[ 0.00796448  0.01424664  0.0317022 ]\n",
      "   [ 0.05011822  0.01566179  0.05206999]\n",
      "   [ 0.00740372  0.05054424 -0.00989257]]\n",
      "\n",
      "  [[ 0.01862949  0.02181549 -0.01549587]\n",
      "   [-0.00790476 -0.0278868   0.05689633]\n",
      "   [-0.0622461   0.06715322 -0.03143632]]\n",
      "\n",
      "  [[-0.01563001 -0.00218663  0.04262196]\n",
      "   [ 0.04553876  0.00512259 -0.00185676]\n",
      "   [ 0.01070828 -0.01429696  0.06172213]]]\n",
      "\n",
      "\n",
      " [[[ 0.04895014 -0.06020255 -0.02219954]\n",
      "   [-0.05790644  0.02214048 -0.06745297]\n",
      "   [-0.01779867  0.04845664  0.06622474]]\n",
      "\n",
      "  [[ 0.0406861  -0.05031305  0.05379159]\n",
      "   [ 0.05380231  0.069287   -0.00276547]\n",
      "   [ 0.04368254  0.04393253 -0.00449594]]\n",
      "\n",
      "  [[ 0.00392163  0.04102459  0.02652169]\n",
      "   [-0.0534628   0.03247884  0.02096328]\n",
      "   [ 0.01270221 -0.04988467  0.00633224]]]\n",
      "\n",
      "\n",
      " [[[ 0.06244325  0.03589805  0.00211721]\n",
      "   [-0.05722725 -0.01317564 -0.00476657]\n",
      "   [-0.0345921  -0.04575263  0.03812636]]\n",
      "\n",
      "  [[ 0.03280235 -0.01283891  0.06993598]\n",
      "   [ 0.03043486  0.06071524 -0.07232133]\n",
      "   [-0.05405745 -0.02920226  0.07020073]]\n",
      "\n",
      "  [[-0.05139921 -0.03865359 -0.05916321]\n",
      "   [-0.01575538 -0.04576292 -0.01699237]\n",
      "   [-0.02303467  0.02321856 -0.01572874]]]\n",
      "\n",
      "\n",
      " ...\n",
      "\n",
      "\n",
      " [[[-0.00054692  0.03356789  0.06781282]\n",
      "   [ 0.02352966 -0.00777032 -0.04161476]\n",
      "   [ 0.03110294  0.04501358  0.04822306]]\n",
      "\n",
      "  [[ 0.04186253  0.01198866 -0.01398954]\n",
      "   [ 0.05758968  0.00251466  0.06379787]\n",
      "   [ 0.04325463  0.04179419  0.05046014]]\n",
      "\n",
      "  [[ 0.0265039   0.00972678  0.0504035 ]\n",
      "   [-0.00543818 -0.0654887  -0.020091  ]\n",
      "   [ 0.04014027  0.01324886  0.0329925 ]]]\n",
      "\n",
      "\n",
      " [[[ 0.0041006  -0.00571811  0.04771992]\n",
      "   [-0.00261752 -0.03459401  0.05416024]\n",
      "   [-0.00408478  0.03262215 -0.02271731]]\n",
      "\n",
      "  [[ 0.01309358 -0.05115093  0.05601021]\n",
      "   [-0.03524531 -0.030319   -0.06354135]\n",
      "   [-0.07098833 -0.01608174  0.00275927]]\n",
      "\n",
      "  [[-0.0193172   0.07092392  0.0090944 ]\n",
      "   [-0.06289485  0.05571264  0.069427  ]\n",
      "   [-0.04846152  0.03383296 -0.01423935]]]\n",
      "\n",
      "\n",
      " [[[ 0.00051113  0.02273706  0.07006885]\n",
      "   [ 0.01066613  0.01711008  0.0104906 ]\n",
      "   [ 0.0328876   0.00447488 -0.05119782]]\n",
      "\n",
      "  [[ 0.01634901 -0.00796003 -0.05204269]\n",
      "   [ 0.06059062 -0.04332555  0.03365344]\n",
      "   [-0.00739107  0.06070036  0.05265331]]\n",
      "\n",
      "  [[ 0.01184733  0.05763993  0.0517386 ]\n",
      "   [-0.03767506  0.04568184 -0.00381089]\n",
      "   [-0.05021938 -0.00320017  0.04306724]]]]\n",
      "<NDArray 128x3x3x3 @gpu(2)>\n",
      "\n",
      "[[[[ 7.4644834e-03  1.3746634e-02  3.1202197e-02]\n",
      "   [ 4.9618214e-02  1.5161790e-02  5.1569991e-02]\n",
      "   [ 6.9037229e-03  5.0044239e-02 -1.0392573e-02]]\n",
      "\n",
      "  [[ 1.8129490e-02  2.1315485e-02 -1.5995871e-02]\n",
      "   [-8.4047616e-03 -2.8386805e-02  5.6396328e-02]\n",
      "   [-6.2746100e-02  6.6653214e-02 -3.1936325e-02]]\n",
      "\n",
      "  [[-1.6130015e-02 -2.6866347e-03  4.2121954e-02]\n",
      "   [ 4.5038760e-02  4.6225861e-03 -2.3567600e-03]\n",
      "   [ 1.0208278e-02 -1.4796957e-02  6.1222129e-02]]]\n",
      "\n",
      "\n",
      " [[[ 4.8450135e-02 -6.0702547e-02 -2.2699544e-02]\n",
      "   [-5.8406442e-02  2.1640474e-02 -6.7952976e-02]\n",
      "   [-1.8298673e-02  4.7956631e-02  6.5724738e-02]]\n",
      "\n",
      "  [[ 4.0186092e-02 -5.0813049e-02  5.3291589e-02]\n",
      "   [ 5.3302303e-02  6.8787001e-02 -3.2654759e-03]\n",
      "   [ 4.3182537e-02  4.3432526e-02 -4.9959403e-03]]\n",
      "\n",
      "  [[ 3.4216288e-03  4.0524587e-02  2.6021691e-02]\n",
      "   [-5.3962804e-02  3.1978831e-02  2.0463275e-02]\n",
      "   [ 1.2202205e-02 -5.0384670e-02  5.8322418e-03]]]\n",
      "\n",
      "\n",
      " [[[ 6.2943257e-02  3.6398053e-02  2.6172160e-03]\n",
      "   [-5.6727245e-02 -1.2675634e-02 -4.2665694e-03]\n",
      "   [-3.4092098e-02 -4.5252621e-02  3.8626365e-02]]\n",
      "\n",
      "  [[ 3.3302359e-02 -1.2338912e-02  7.0435978e-02]\n",
      "   [ 3.0934861e-02  6.1215244e-02 -7.1821332e-02]\n",
      "   [-5.3557448e-02 -2.8702257e-02  7.0700735e-02]]\n",
      "\n",
      "  [[-5.0899208e-02 -3.8153581e-02 -5.8663212e-02]\n",
      "   [-1.5255379e-02 -4.5262918e-02 -1.6492369e-02]\n",
      "   [-2.2534670e-02  2.3718564e-02 -1.5228735e-02]]]\n",
      "\n",
      "\n",
      " ...\n",
      "\n",
      "\n",
      " [[[-1.0468808e-03  3.4067191e-02  6.7312844e-02]\n",
      "   [ 2.3029678e-02 -8.2703056e-03 -4.2114731e-02]\n",
      "   [ 3.0602956e-02  4.4513587e-02  4.7723081e-02]]\n",
      "\n",
      "  [[ 4.1362539e-02  1.1488681e-02 -1.4489529e-02]\n",
      "   [ 5.7089686e-02  2.0146645e-03  6.3297883e-02]\n",
      "   [ 4.2754635e-02  4.1294195e-02  4.9960151e-02]]\n",
      "\n",
      "  [[ 2.6003910e-02  9.2267916e-03  4.9903512e-02]\n",
      "   [-5.9381714e-03 -6.5988697e-02 -2.0590995e-02]\n",
      "   [ 3.9640278e-02  1.2748862e-02  3.2492511e-02]]]\n",
      "\n",
      "\n",
      " [[[ 4.6005975e-03 -6.2181139e-03  4.7219917e-02]\n",
      "   [-2.1175165e-03 -3.4094013e-02  5.3660236e-02]\n",
      "   [-3.5847744e-03  3.3122152e-02 -2.2217315e-02]]\n",
      "\n",
      "  [[ 1.3593582e-02 -5.1650919e-02  5.5510201e-02]\n",
      "   [-3.4745306e-02 -2.9818999e-02 -6.4041346e-02]\n",
      "   [-7.0488319e-02 -1.5581736e-02  3.2592707e-03]]\n",
      "\n",
      "  [[-1.8817199e-02  7.0423923e-02  8.5943956e-03]\n",
      "   [-6.2394846e-02  5.6212641e-02  6.8926997e-02]\n",
      "   [-4.7961518e-02  3.4332968e-02 -1.3739350e-02]]]\n",
      "\n",
      "\n",
      " [[[ 1.1124532e-05  2.2237055e-02  6.9568850e-02]\n",
      "   [ 1.0166124e-02  1.6610079e-02  9.9905962e-03]\n",
      "   [ 3.2387599e-02  3.9748778e-03 -5.1697820e-02]]\n",
      "\n",
      "  [[ 1.5849009e-02 -8.4600290e-03 -5.2542694e-02]\n",
      "   [ 6.0090624e-02 -4.3825552e-02  3.3153437e-02]\n",
      "   [-7.8910738e-03  6.0200356e-02  5.2153312e-02]]\n",
      "\n",
      "  [[ 1.1347332e-02  5.7139926e-02  5.1238596e-02]\n",
      "   [-3.8175061e-02  4.5181841e-02 -4.3108906e-03]\n",
      "   [-5.0719380e-02 -3.7001665e-03  4.2567238e-02]]]]\n",
      "<NDArray 128x3x3x3 @gpu(2)>\n",
      "\n",
      "[[[[ 7.4644834e-03  1.3746634e-02  3.1202197e-02]\n",
      "   [ 4.9618214e-02  1.5161790e-02  5.1569991e-02]\n",
      "   [ 6.9037229e-03  5.0044239e-02 -1.0392573e-02]]\n",
      "\n",
      "  [[ 1.8129490e-02  2.1315485e-02 -1.5995871e-02]\n",
      "   [-8.4047616e-03 -2.8386805e-02  5.6396328e-02]\n",
      "   [-6.2746100e-02  6.6653214e-02 -3.1936325e-02]]\n",
      "\n",
      "  [[-1.6130015e-02 -2.6866347e-03  4.2121954e-02]\n",
      "   [ 4.5038760e-02  4.6225861e-03 -2.3567600e-03]\n",
      "   [ 1.0208278e-02 -1.4796957e-02  6.1222129e-02]]]\n",
      "\n",
      "\n",
      " [[[ 4.8450135e-02 -6.0702547e-02 -2.2699544e-02]\n",
      "   [-5.8406442e-02  2.1640474e-02 -6.7952976e-02]\n",
      "   [-1.8298673e-02  4.7956631e-02  6.5724738e-02]]\n",
      "\n",
      "  [[ 4.0186092e-02 -5.0813049e-02  5.3291589e-02]\n",
      "   [ 5.3302303e-02  6.8787001e-02 -3.2654759e-03]\n",
      "   [ 4.3182537e-02  4.3432526e-02 -4.9959403e-03]]\n",
      "\n",
      "  [[ 3.4216288e-03  4.0524587e-02  2.6021691e-02]\n",
      "   [-5.3962804e-02  3.1978831e-02  2.0463275e-02]\n",
      "   [ 1.2202205e-02 -5.0384670e-02  5.8322418e-03]]]\n",
      "\n",
      "\n",
      " [[[ 6.2943257e-02  3.6398053e-02  2.6172160e-03]\n",
      "   [-5.6727245e-02 -1.2675634e-02 -4.2665694e-03]\n",
      "   [-3.4092098e-02 -4.5252621e-02  3.8626365e-02]]\n",
      "\n",
      "  [[ 3.3302359e-02 -1.2338912e-02  7.0435978e-02]\n",
      "   [ 3.0934861e-02  6.1215244e-02 -7.1821332e-02]\n",
      "   [-5.3557448e-02 -2.8702257e-02  7.0700735e-02]]\n",
      "\n",
      "  [[-5.0899208e-02 -3.8153581e-02 -5.8663212e-02]\n",
      "   [-1.5255379e-02 -4.5262918e-02 -1.6492369e-02]\n",
      "   [-2.2534670e-02  2.3718564e-02 -1.5228735e-02]]]\n",
      "\n",
      "\n",
      " ...\n",
      "\n",
      "\n",
      " [[[-1.0468808e-03  3.4067191e-02  6.7312844e-02]\n",
      "   [ 2.3029678e-02 -8.2703056e-03 -4.2114731e-02]\n",
      "   [ 3.0602956e-02  4.4513587e-02  4.7723081e-02]]\n",
      "\n",
      "  [[ 4.1362539e-02  1.1488681e-02 -1.4489529e-02]\n",
      "   [ 5.7089686e-02  2.0146645e-03  6.3297883e-02]\n",
      "   [ 4.2754635e-02  4.1294195e-02  4.9960151e-02]]\n",
      "\n",
      "  [[ 2.6003910e-02  9.2267916e-03  4.9903512e-02]\n",
      "   [-5.9381714e-03 -6.5988697e-02 -2.0590995e-02]\n",
      "   [ 3.9640278e-02  1.2748862e-02  3.2492511e-02]]]\n",
      "\n",
      "\n",
      " [[[ 4.6005975e-03 -6.2181139e-03  4.7219917e-02]\n",
      "   [-2.1175165e-03 -3.4094013e-02  5.3660236e-02]\n",
      "   [-3.5847744e-03  3.3122152e-02 -2.2217315e-02]]\n",
      "\n",
      "  [[ 1.3593582e-02 -5.1650919e-02  5.5510201e-02]\n",
      "   [-3.4745306e-02 -2.9818999e-02 -6.4041346e-02]\n",
      "   [-7.0488319e-02 -1.5581736e-02  3.2592707e-03]]\n",
      "\n",
      "  [[-1.8817199e-02  7.0423923e-02  8.5943956e-03]\n",
      "   [-6.2394846e-02  5.6212641e-02  6.8926997e-02]\n",
      "   [-4.7961518e-02  3.4332968e-02 -1.3739350e-02]]]\n",
      "\n",
      "\n",
      " [[[ 1.1124532e-05  2.2237055e-02  6.9568850e-02]\n",
      "   [ 1.0166124e-02  1.6610079e-02  9.9905962e-03]\n",
      "   [ 3.2387599e-02  3.9748778e-03 -5.1697820e-02]]\n",
      "\n",
      "  [[ 1.5849009e-02 -8.4600290e-03 -5.2542694e-02]\n",
      "   [ 6.0090624e-02 -4.3825552e-02  3.3153437e-02]\n",
      "   [-7.8910738e-03  6.0200356e-02  5.2153312e-02]]\n",
      "\n",
      "  [[ 1.1347332e-02  5.7139926e-02  5.1238596e-02]\n",
      "   [-3.8175061e-02  4.5181841e-02 -4.3108906e-03]\n",
      "   [-5.0719380e-02 -3.7001665e-03  4.2567238e-02]]]]\n",
      "<NDArray 128x3x3x3 @gpu(2)>\n",
      "Exiting Debugger.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py:1736: DeprecationWarning: `BdbQuit_IPython_excepthook` is deprecated since version 5.1\n",
      "  stb = handler(self,etype,value,tb,tb_offset=tb_offset)\n"
     ]
    }
   ],
   "source": [
    "run_train(1, ctx=opt.ctx)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
