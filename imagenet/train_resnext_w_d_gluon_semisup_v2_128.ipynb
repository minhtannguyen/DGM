{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse,logging, os, math\n",
    "import mxnet as mx\n",
    "from mxnet import image\n",
    "from mxnet import nd, gluon, autograd, init\n",
    "from mxnet.gluon.data.vision import ImageFolderDataset\n",
    "from mxnet.gluon.data import DataLoader\n",
    "from mxnet.gluon import nn\n",
    "from tensorboardX import SummaryWriter\n",
    "import numpy as np\n",
    "import shutil\n",
    "import _pickle as cPickle\n",
    "from mxnet.gluon.parameter import Parameter, ParameterDict\n",
    "import subprocess\n",
    "import time\n",
    "from mxnet.gluon.loss import Loss, _apply_weighting, _reshape_like\n",
    "\n",
    "from IPython.core.debugger import Tracer\n",
    "\n",
    "from resnext_w_d_maxmin_v2_128 import resnext\n",
    "\n",
    "# import os\n",
    "# os.environ['MXNET_GPU_MEM_POOL_TYPE'] = 'Round'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "formatter = logging.Formatter('%(asctime)s - %(message)s')\n",
    "console = logging.StreamHandler()\n",
    "console.setFormatter(formatter)\n",
    "logger.addHandler(console)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Options:\n",
    "    def __init__(self):\n",
    "        self.gpus = '0,1,2,3,4,5,6,7' #the gpus will be used, e.g \"0,1,2,3\"\n",
    "        self.data_dir = '/tanData/datasets/imagenet/data' #the input data directory\n",
    "        self.log_dir = '/tanData/logs'\n",
    "        self.model_dir ='/tanData/models'\n",
    "        self.exp_name = 'semisup_exp1_128K'\n",
    "        self.data_type = 'imagenet' #the dataset type\n",
    "        self.depth = 50 #the depth of resnet\n",
    "        self.batch_size = 56 #the batch size\n",
    "        self.num_group = 64 #the number of convolution groups\n",
    "        self.drop_out = 0.0 #the probability of an element to be zeroed\n",
    "        self.alpha_max = 0.5\n",
    "        self.alpha_min = 0.5\n",
    "        self.alpha_drm = 0.5\n",
    "        self.alpha_rpn = 1.0\n",
    "        self.alpha_kl = 0.5\n",
    "        self.alpha_mm = 0.5\n",
    "        self.alpha_min = 0.5\n",
    "        self.alpha_max = 1.0 - self.alpha_min\n",
    "        \n",
    "        self.list_dir = './' #the directory which contain the training list file\n",
    "        self.lr = 0.1 #initialization learning rate\n",
    "        self.mom = 0.9 #momentum for sgd\n",
    "        self.bn_mom = 0.9 #momentum for batch normlization\n",
    "        self.wd = 0.0001 #weight decay for sgd\n",
    "        self.workspace = 512 #memory space size(MB) used in convolution, \n",
    "                            #if xpu memory is oom, then you can try smaller vale, such as --workspace 256 \n",
    "        self.num_classes = 1000 #the class number of your task\n",
    "        self.aug_level = 2 # level 1: use only random crop and random mirror, \n",
    "                           #level 2: add scale/aspect/hsv augmentation based on level 1, \n",
    "                           #level 3: add rotation/shear augmentation based on level 2 \n",
    "        self.num_examples = 1152000 # the number of training examples\n",
    "        self.kv_store = 'device' # the kvstore type'\n",
    "        self.model_load_epoch = 0 # load the model on an epoch using the model-load-prefix\n",
    "        self.frequent = 50 # frequency of logging\n",
    "        self.memonger = False # true means using memonger to save momory, https://github.com/dmlc/mxnet-memonger\n",
    "        self.retrain = False # true means continue training\n",
    "        \n",
    "args = Options()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-09-25 11:37:43,833 - <__main__.Options object at 0x7f03f0ef7908>\n"
     ]
    }
   ],
   "source": [
    "hdlr = logging.FileHandler('./log/log-semi128Kresnext-{}-{}.log'.format(args.data_type, args.depth))\n",
    "hdlr.setFormatter(formatter)\n",
    "logger.addHandler(hdlr)\n",
    "logging.info(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "kv = mx.kvstore.create(args.kv_store)\n",
    "ctx = mx.cpu() if args.gpus is None else [mx.gpu(int(i)) for i in args.gpus.split(',')]\n",
    "batch_size = args.batch_size\n",
    "batch_size *= max(1, len(ctx))\n",
    "begin_epoch = args.model_load_epoch if args.model_load_epoch else 0\n",
    "if not os.path.exists(\"./model\"):\n",
    "    os.mkdir(\"./model\")\n",
    "model_prefix = \"semi128Kresnext_{}_{}_{}_{}\".format(args.data_type, args.depth, kv.rank, args.exp_name)\n",
    "# model_prefix = \"model/se-resnext-{}-{}-{}\".format(args.data_type, args.depth, kv.rank)\n",
    "arg_params = None\n",
    "aux_params = None\n",
    "if args.retrain:\n",
    "    _, arg_params, aux_params = mx.model.load_checkpoint(model_prefix, args.model_load_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_sup = mx.io.ImageRecordIter(\n",
    "    path_imgrec         = os.path.join(args.data_dir, \"train.rec\") if args.data_type == 'cifar10' else\n",
    "                          os.path.join(args.data_dir, \"train_256_q90.rec\") if args.aug_level == 1\n",
    "                          else os.path.join(args.data_dir, \"train_sup_240.rec\") ,\n",
    "    label_width         = 1,\n",
    "    data_name           = 'data',\n",
    "    label_name          = 'softmax_label',\n",
    "    data_shape          = (3, 32, 32) if args.data_type==\"cifar10\" else (3, 128, 128),\n",
    "    batch_size          = batch_size // 2,\n",
    "    pad                 = 4 if args.data_type == \"cifar10\" else 0,\n",
    "    fill_value          = 127,  # only used when pad is valid\n",
    "    rand_crop           = True,\n",
    "    max_random_scale    = 1.0,  # 480 with imagnet, 32 with cifar10\n",
    "    min_random_scale    = 1.0 if args.data_type == \"cifar10\" else 1.0 if args.aug_level == 1 else 0.533,  # 256.0/480.0=0.533, 256.0/384.0=0.667 256.0/256=1.0\n",
    "    max_aspect_ratio    = 0 if args.data_type == \"cifar10\" else 0 if args.aug_level == 1 else 0.25, # 0.25\n",
    "    random_h            = 0 if args.data_type == \"cifar10\" else 0 if args.aug_level == 1 else 36,  # 0.4*90\n",
    "    random_s            = 0 if args.data_type == \"cifar10\" else 0 if args.aug_level == 1 else 50,  # 0.4*127\n",
    "    random_l            = 0 if args.data_type == \"cifar10\" else 0 if args.aug_level == 1 else 50,  # 0.4*127\n",
    "    max_rotate_angle    = 0 if args.aug_level <= 2 else 10,\n",
    "    max_shear_ratio     = 0 if args.aug_level <= 2 else 0.0, #0.1 args.aug_level = 3\n",
    "    rand_mirror         = True,\n",
    "    shuffle             = True,\n",
    "    num_parts           = kv.num_workers,\n",
    "    part_index          = kv.rank)\n",
    "\n",
    "train_data_unsup = mx.io.ImageRecordIter(\n",
    "    path_imgrec         = os.path.join(args.data_dir, \"train.rec\") if args.data_type == 'cifar10' else\n",
    "                          os.path.join(args.data_dir, \"train_256_q90.rec\") if args.aug_level == 1\n",
    "                          else os.path.join(args.data_dir, \"train_unsup_240.rec\") ,\n",
    "    label_width         = 1,\n",
    "    data_name           = 'data',\n",
    "    label_name          = 'softmax_label',\n",
    "    data_shape          = (3, 32, 32) if args.data_type==\"cifar10\" else (3, 128, 128),\n",
    "    batch_size          = batch_size // 2,\n",
    "    pad                 = 4 if args.data_type == \"cifar10\" else 0,\n",
    "    fill_value          = 127,  # only used when pad is valid\n",
    "    rand_crop           = True,\n",
    "    max_random_scale    = 1.0,  # 480 with imagnet, 32 with cifar10\n",
    "    min_random_scale    = 1.0 if args.data_type == \"cifar10\" else 1.0 if args.aug_level == 1 else 0.533,  # 256.0/480.0=0.533, 256.0/384.0=0.667 256.0/256=1.0\n",
    "    max_aspect_ratio    = 0 if args.data_type == \"cifar10\" else 0 if args.aug_level == 1 else 0.25, # 0.25\n",
    "    random_h            = 0 if args.data_type == \"cifar10\" else 0 if args.aug_level == 1 else 36,  # 0.4*90\n",
    "    random_s            = 0 if args.data_type == \"cifar10\" else 0 if args.aug_level == 1 else 50,  # 0.4*127\n",
    "    random_l            = 0 if args.data_type == \"cifar10\" else 0 if args.aug_level == 1 else 50,  # 0.4*127\n",
    "    max_rotate_angle    = 0 if args.aug_level <= 2 else 10,\n",
    "    max_shear_ratio     = 0 if args.aug_level <= 2 else 0.0, #0.1 args.aug_level = 3\n",
    "    rand_mirror         = True,\n",
    "    shuffle             = True,\n",
    "    num_parts           = kv.num_workers,\n",
    "    part_index          = kv.rank)\n",
    "\n",
    "val_data = mx.io.ImageRecordIter(\n",
    "    path_imgrec         = os.path.join(args.data_dir, \"val.rec\") if args.data_type == 'cifar10' else\n",
    "                          os.path.join(args.data_dir, \"val_128.rec\"),\n",
    "    label_width         = 1,\n",
    "    data_name           = 'data',\n",
    "    label_name          = 'softmax_label',\n",
    "    batch_size          = batch_size,\n",
    "    data_shape          = (3, 32, 32) if args.data_type==\"cifar10\" else (3, 128, 128),\n",
    "    rand_crop           = False,\n",
    "    rand_mirror         = False,\n",
    "    num_parts           = kv.num_workers,\n",
    "    part_index          = kv.rank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Normal(mx.init.Initializer):\n",
    "    \"\"\"Initializes weights with random values sampled from a normal distribution\n",
    "    with a mean and standard deviation of `sigma`.\n",
    "    \"\"\"\n",
    "    def __init__(self, mean=0, sigma=0.01):\n",
    "        super(Normal, self).__init__(sigma=sigma)\n",
    "        self.sigma = sigma\n",
    "        self.mean = mean\n",
    "\n",
    "    def _init_weight(self, _, arr):\n",
    "        mx.random.normal(self.mean, self.sigma, out=arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_factor_scheduler(begin_epoch, epoch_size, step=[30, 60, 90, 95, 110, 120], factor=0.1):\n",
    "    step_ = [epoch_size * (x-begin_epoch) for x in step if x-begin_epoch > 0]\n",
    "    return mx.lr_scheduler.MultiFactorScheduler(step=step_, factor=factor) if len(step_) else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class L2Loss16bit(Loss):\n",
    "    def __init__(self, weight=1., batch_axis=0, **kwargs):\n",
    "        super(L2Loss16bit, self).__init__(weight, batch_axis, **kwargs)\n",
    "\n",
    "    def hybrid_forward(self, F, pred, label, sample_weight=None):\n",
    "        label = _reshape_like(F, label, pred)\n",
    "        loss = F.square(pred - label)\n",
    "        loss = _apply_weighting(F, loss, self._weight/2, sample_weight)\n",
    "        return F.mean(F.mean(F.mean(loss, axis=3),axis=2), axis=1)\n",
    "    \n",
    "criterion = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "L2_loss = gluon.loss.L2Loss()\n",
    "L2_loss16 = L2Loss16bit()\n",
    "\n",
    "acc_top1 = mx.metric.Accuracy()\n",
    "acc_top5 = mx.metric.TopKAccuracy(5)\n",
    "import datetime\n",
    "writer = SummaryWriter(os.path.join(args.log_dir, args.exp_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(net, val_data, ctx):\n",
    "    val_data.reset()\n",
    "    \n",
    "    acc_top1_val = mx.metric.Accuracy()\n",
    "    acc_top5_val = mx.metric.TopKAccuracy(5)\n",
    "    acc_top1_val_max = mx.metric.Accuracy()\n",
    "    acc_top5_val_max = mx.metric.TopKAccuracy(5)\n",
    "    acc_top1_val_min = mx.metric.Accuracy()\n",
    "    acc_top5_val_min = mx.metric.TopKAccuracy(5)\n",
    "    acc_top1_val.reset()\n",
    "    acc_top5_val.reset()\n",
    "    acc_top1_val_max.reset()\n",
    "    acc_top5_val_max.reset()\n",
    "    acc_top1_val_min.reset()\n",
    "    acc_top5_val_min.reset()\n",
    "    for i, batch in enumerate(val_data):\n",
    "        data = gluon.utils.split_and_load(batch.data[0], ctx_list=ctx, batch_axis=0)\n",
    "        label = gluon.utils.split_and_load(batch.label[0], ctx_list=ctx, batch_axis=0)\n",
    "        \n",
    "        outputs = []\n",
    "        outputsmax = []\n",
    "        outputsmin = []\n",
    "        for x, y in zip(data, label):\n",
    "            cond = nd.zeros_like(y)\n",
    "            zmax, zmin, _, _, _, _, _ = net(x, y, cond)\n",
    "            z = args.alpha_max * zmax + args.alpha_min * zmin\n",
    "            outputs.append(z)\n",
    "            outputsmax.append(zmax)\n",
    "            outputsmin.append(zmin)\n",
    "            \n",
    "            \n",
    "        acc_top1_val.update(label, outputs)\n",
    "        acc_top5_val.update(label, outputs)\n",
    "        acc_top1_val_max.update(label, outputsmax)\n",
    "        acc_top5_val_max.update(label, outputsmax)\n",
    "        acc_top1_val_min.update(label, outputsmin)\n",
    "        acc_top5_val_min.update(label, outputsmin)\n",
    "        \n",
    "        del data, label, z, zmax, zmin, x, y, cond\n",
    "\n",
    "    _, top1 = acc_top1_val.get()\n",
    "    _, top5 = acc_top5_val.get()\n",
    "    _, top1max = acc_top1_val_max.get()\n",
    "    _, top5max = acc_top5_val_max.get()\n",
    "    _, top1min = acc_top1_val_min.get()\n",
    "    _, top5min = acc_top5_val_min.get()\n",
    "    return (top1, top5, top1max, top5max, top1min, top5min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, train_data_sup, train_data_unsup, val_data, num_epochs, ctx):\n",
    "    epoch_size = max(int(args.num_examples / (batch_size//2) / kv.num_workers), 1)\n",
    "    # lr_sch = multi_factor_scheduler(begin_epoch, epoch_size, step=[15, 30], factor=0.1)\n",
    "    # trainer = gluon.Trainer(net.collect_params(), 'nag', {'learning_rate':args.lr, 'momentum':args.mom, 'wd':args.wd, 'lr_scheduler': lr_sch, 'multi_precision': True})\n",
    "    \n",
    "    trainer = gluon.Trainer(\n",
    "        net.collect_params(), 'adam', {'learning_rate': 0.001, 'wd': args.wd})\n",
    "    prev_time = datetime.datetime.now()\n",
    "    best_top1_val = 0.; best_top1_valmax = 0.; best_top1_valmin = 0.\n",
    "    best_top5_val = 0.; best_top5_valmax = 0.; best_top5_valmin = 0.\n",
    "    log_interval = 100\n",
    "    \n",
    "    for epoch in range(begin_epoch, num_epochs):\n",
    "        train_data_sup.reset()\n",
    "        train_data_unsup.reset()\n",
    "        \n",
    "        if epoch == 10 or epoch == 18:\n",
    "            trainer.set_learning_rate(0.1 * trainer.learning_rate)\n",
    "        \n",
    "        tic = time.time()\n",
    "        btic = time.time()\n",
    "        acc_top1.reset()\n",
    "        acc_top5.reset()\n",
    "        train_loss = 0\n",
    "        num_batch = 0\n",
    "        \n",
    "        for i, (batch_sup, batch_unsup) in enumerate(zip(train_data_sup, train_data_unsup)):\n",
    "            bs = batch_unsup.data[0].shape[0]\n",
    "            \n",
    "            data_sup = gluon.utils.split_and_load(batch_sup.data[0], ctx_list=ctx, batch_axis=0)\n",
    "            label_sup = gluon.utils.split_and_load(batch_sup.label[0], ctx_list=ctx, batch_axis=0)\n",
    "            data_unsup = gluon.utils.split_and_load(batch_unsup.data[0], ctx_list=ctx, batch_axis=0)\n",
    "            label_unsup = gluon.utils.split_and_load(batch_unsup.label[0], ctx_list=ctx, batch_axis=0)\n",
    "            \n",
    "            loss = []\n",
    "            outputs = []\n",
    "            \n",
    "            with autograd.record():\n",
    "                for x_sup, x_unsup, y_sup, y_unsup in zip(data_sup, data_unsup, label_sup, label_unsup):\n",
    "                    cond_sup = nd.ones_like(y_sup)\n",
    "                    zmax_sup, zmin_sup, xhatmax_sup, xhatmin_sup, loss_mmmax_sup, loss_mmmin_sup, xbn_sup = net(x_sup, y_sup, cond_sup)\n",
    "                    loss_xent_sup = args.alpha_max * criterion(zmax_sup, y_sup) + args.alpha_min * criterion(zmin_sup, y_sup)\n",
    "                    loss_drm_sup = args.alpha_max * L2_loss(xhatmax_sup, xbn_sup) + args.alpha_min * L2_loss(xhatmin_sup, xbn_sup)\n",
    "                    softmax_scores_sup = args.alpha_max * nd.softmax(zmax_sup) + args.alpha_min * nd.softmax(zmin_sup)\n",
    "                    loss_kl_sup = -nd.sum(nd.log(1000.0*softmax_scores_sup + 1e-8) * softmax_scores_sup, axis=1)\n",
    "                    loss_mm_sup = args.alpha_max * loss_mmmax_sup + args.alpha_min * loss_mmmin_sup\n",
    "                    loss_total_sup = loss_xent_sup + args.alpha_drm * loss_drm_sup + args.alpha_kl * loss_kl_sup + args.alpha_mm * loss_mm_sup\n",
    "                    \n",
    "                    cond_unsup = nd.zeros_like(y_unsup)\n",
    "                    zmax_unsup, zmin_unsup, xhatmax_unsup, xhatmin_unsup, loss_mmmax_unsup, loss_mmmin_unsup, xbn_unsup = net(x_unsup, y_unsup, cond_unsup)\n",
    "                    loss_drm_unsup = args.alpha_max * L2_loss(xhatmax_unsup, xbn_unsup) + args.alpha_min * L2_loss(xhatmin_unsup, xbn_unsup)\n",
    "                    softmax_scores_unsup = args.alpha_max * nd.softmax(zmax_unsup) + args.alpha_min * nd.softmax(zmin_unsup)\n",
    "                    loss_kl_unsup = -nd.sum(nd.log(1000.0*softmax_scores_unsup + 1e-8) * softmax_scores_unsup, axis=1)\n",
    "                    loss_mm_unsup = args.alpha_max * loss_mmmax_unsup + args.alpha_min * loss_mmmin_unsup\n",
    "                    loss_total_unsup = args.alpha_drm * loss_drm_unsup + args.alpha_kl * loss_kl_unsup + args.alpha_mm * loss_mm_unsup\n",
    "                    \n",
    "                    z_sup = args.alpha_max * zmax_sup + args.alpha_min * zmin_sup\n",
    "\n",
    "                    loss.append(loss_total_sup + loss_total_unsup)\n",
    "                    outputs.append(z_sup)\n",
    "                    \n",
    "            for l in loss:\n",
    "                l.backward()\n",
    "            \n",
    "            trainer.step(bs)\n",
    "            \n",
    "            acc_top1.update(label_sup, outputs)\n",
    "            acc_top5.update(label_sup, outputs)\n",
    "            train_loss += sum([l.sum().asscalar() for l in loss])\n",
    "            num_batch += 1\n",
    "            if log_interval and not i % log_interval:\n",
    "                _, top1 = acc_top1.get()\n",
    "                _, top5 = acc_top5.get()\n",
    "                logging.info('Epoch[%d] Batch [%d] Lr: %f Speed: %f samples/sec   loss=%f   top1-acc=%f     top5-acc=%f'%(\n",
    "                          epoch, i, trainer.learning_rate, batch_size*log_interval/(time.time()-btic), train_loss/(num_batch * batch_size), top1, top5))\n",
    "                btic = time.time()\n",
    "            \n",
    "            del data_sup, label_sup, data_unsup, label_unsup, loss, outputs, \n",
    "            del x_sup, x_unsup, y_sup, y_unsup, cond_sup, cond_unsup, xbn_sup, xbn_unsup\n",
    "            del zmax_sup, zmin_sup, xhatmax_sup, xhatmin_sup, loss_mmmax_sup, loss_mmmin_sup\n",
    "            del loss_xent_sup, loss_drm_sup, softmax_scores_sup, loss_kl_sup, loss_mm_sup, loss_total_sup, z_sup\n",
    "            del zmax_unsup, zmin_unsup, xhatmax_unsup, xhatmin_unsup, loss_mmmax_unsup, loss_mmmin_unsup\n",
    "            del loss_drm_unsup, softmax_scores_unsup, loss_kl_unsup, loss_mm_unsup, loss_total_unsup\n",
    "        \n",
    "        _, top1 = acc_top1.get()\n",
    "        _, top5 = acc_top5.get()\n",
    "        train_loss /= num_batch * batch_size\n",
    "        writer.add_scalars('acc', {'train_top1': top1}, epoch)\n",
    "        writer.add_scalars('acc', {'train_top5': top5}, epoch)\n",
    "        \n",
    "        top1_val, top5_val, top1_valmax, top5_valmax, top1_valmin, top5_valmin = test(net=net, val_data=val_data, ctx=ctx)\n",
    "        \n",
    "        if top1_val > best_top1_val:\n",
    "            best_top1_val = top1_val\n",
    "            net.collect_params().save('%s/%s_best_top1.params'%(args.model_dir, model_prefix))\n",
    "        \n",
    "        if top1_valmax > best_top1_valmax:\n",
    "            best_top1_valmax = top1_valmax\n",
    "            net.collect_params().save('%s/%s_best_top1_max.params'%(args.model_dir, model_prefix))\n",
    "            \n",
    "        if top1_valmin > best_top1_valmin:\n",
    "            best_top1_valmin = top1_valmin\n",
    "            net.collect_params().save('%s/%s_best_top1_min.params'%(args.model_dir, model_prefix))\n",
    "        \n",
    "        if top5_val > best_top5_val:\n",
    "            best_top5_val = top5_val\n",
    "            net.collect_params().save('%s/%s_best_top5.params'%(args.model_dir, model_prefix))\n",
    "        \n",
    "        if top5_valmax > best_top5_valmax:\n",
    "            best_top5_valmax = top5_valmax\n",
    "            net.collect_params().save('%s/%s_best_top5_max.params'%(args.model_dir, model_prefix))\n",
    "        \n",
    "        if top5_valmin > best_top5_valmin:\n",
    "            best_top5_valmin = top5_valmin\n",
    "            net.collect_params().save('%s/%s_best_top5_min.params'%(args.model_dir, model_prefix))\n",
    "        \n",
    "        logging.info('[Epoch %d] training: acc-top1=%f acc-top5=%f loss=%f lr=%f'%(epoch, top1, top5, train_loss, trainer.learning_rate))\n",
    "        logging.info('[Epoch %d] time cost: %f'%(epoch, time.time()-tic))\n",
    "        logging.info('[Epoch %d] validation: acc-top1=%f acc-top5=%f best-acc-top1=%f best-acc-top5=%f'%(epoch, top1_val, top5_val, best_top1_val, best_top5_val))\n",
    "        logging.info('[Epoch %d] validation: acc-top1-max=%f acc-top5-max=%f best-acc-top1-max=%f best-acc-top5-max=%f'%(epoch, top1_valmax, top5_valmax, best_top1_valmax, best_top5_valmax))\n",
    "        logging.info('[Epoch %d] validation: acc-top1-min=%f acc-top5-min=%f best-acc-top1-min=%f best-acc-top5-min=%f'%(epoch, top1_valmin, top5_valmin, best_top1_valmin, best_top5_valmin))\n",
    "        \n",
    "        writer.add_scalars('acc', {'valid_top1': top1_val}, epoch)\n",
    "        writer.add_scalars('acc', {'valid_top5': top5_val}, epoch)\n",
    "        writer.add_scalars('acc', {'valid_top1_max': top1_valmax}, epoch)\n",
    "        writer.add_scalars('acc', {'valid_top5_max': top5_valmax}, epoch)\n",
    "        writer.add_scalars('acc', {'valid_top1_min': top1_valmin}, epoch)\n",
    "        writer.add_scalars('acc', {'valid_top5_min': top5_valmin}, epoch)\n",
    "        \n",
    "        net.collect_params().save('%s/%s_current.params'%(args.model_dir, model_prefix))\n",
    "        if not epoch % 10:\n",
    "            net.collect_params().save('%s/%s_epoch_%i.params'%(args.model_dir, model_prefix, epoch))\n",
    "    \n",
    "    return best_top1_val, best_top5_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio_list = [0.25, 0.125, 0.0625, 0.03125]   # 1/4, 1/8, 1/16, 1/32\n",
    "if args.depth == 18:\n",
    "    units = [2, 2, 2, 2]\n",
    "elif args.depth == 34:\n",
    "    units = [3, 4, 6, 3]\n",
    "elif args.depth == 50:\n",
    "    units = [3, 4, 6, 3]\n",
    "elif args.depth == 101:\n",
    "    units = [3, 4, 23, 3]\n",
    "elif args.depth == 152:\n",
    "    units = [3, 8, 36, 3]\n",
    "elif args.depth == 200:\n",
    "    units = [3, 24, 36, 3]\n",
    "elif args.depth == 269:\n",
    "    units = [3, 30, 48, 8]\n",
    "else:\n",
    "    raise ValueError(\"no experiments done on detph {}, you can do it youself\".format(args.depth))\n",
    "\n",
    "num_epochs = 200 if args.data_type == \"cifar10\" else 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_train(ctx): \n",
    "    model = resnext(units=units, num_stage=4, filter_list=[64, 256, 512, 1024, 2048] if args.depth >=50 else [64, 64, 128, 256, 512], ratio_list=ratio_list, num_class=args.num_classes, num_group=args.num_group, data_type=\"imagenet\", drop_out=args.drop_out, bn_mom=args.bn_mom)\n",
    "    for param in model.collect_params().values():\n",
    "        if param.name.find('conv') != -1 or param.name.find('dense') != -1:\n",
    "            if param.name.find('weight') != -1:\n",
    "                param.initialize(init=mx.init.Xavier(rnd_type='gaussian', factor_type=\"in\", magnitude=2), ctx=ctx)\n",
    "            else:\n",
    "                param.initialize(init=mx.init.Zero(), ctx=ctx)\n",
    "        elif param.name.find('batchnorm') != -1:\n",
    "            if param.name.find('gamma') != -1:\n",
    "                param.initialize(init=Normal(mean=1, sigma=0.02), ctx=ctx)\n",
    "            else:\n",
    "                param.initialize(init=mx.init.Zero(), ctx=ctx)\n",
    "        elif param.name.find('insnorm') != -1:\n",
    "            if param.name.find('gamma') != -1:\n",
    "                param.initialize(init=Normal(mean=1, sigma=0.02), ctx=ctx)\n",
    "            else:\n",
    "                param.initialize(init=mx.init.Zero(), ctx=ctx)\n",
    "        elif param.name.find('biasadder') != -1:\n",
    "            param.initialize(init=mx.init.Zero(), ctx=ctx)\n",
    "        else:\n",
    "            param.initialize(init=mx.init.Xavier(rnd_type='gaussian', factor_type=\"in\", magnitude=2), ctx=ctx)\n",
    "    \n",
    "    # model.collect_params().load('/tanData/models/semi128Kresnext_imagenet_50_0_semisup_exp1_128K_current.params', ctx=ctx, allow_missing=True)\n",
    "    model.hybridize()\n",
    "        \n",
    "    best_top1_val, best_top5_val = train(net=model, train_data_sup=train_data_sup, train_data_unsup=train_data_unsup, val_data=val_data, num_epochs=num_epochs, ctx=ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-09-25 11:39:49,072 - Epoch[0] Batch [0] Lr: 0.001000 Speed: 440.625724 samples/sec   loss=26.252530   top1-acc=0.000000     top5-acc=0.000000\n",
      "2018-09-25 11:44:39,123 - Epoch[0] Batch [100] Lr: 0.001000 Speed: 154.460530 samples/sec   loss=4.011204   top1-acc=0.028598     top5-acc=0.227237\n",
      "2018-09-25 11:49:21,871 - Epoch[0] Batch [200] Lr: 0.001000 Speed: 158.448539 samples/sec   loss=3.117767   top1-acc=0.019412     top5-acc=0.144723\n",
      "2018-09-25 11:54:04,140 - Epoch[0] Batch [300] Lr: 0.001000 Speed: 158.719476 samples/sec   loss=2.801885   top1-acc=0.016404     top5-acc=0.103954\n",
      "2018-09-25 11:58:53,626 - Epoch[0] Batch [400] Lr: 0.001000 Speed: 154.764820 samples/sec   loss=2.614230   top1-acc=0.015464     top5-acc=0.085690\n",
      "2018-09-25 12:03:45,604 - Epoch[0] Batch [500] Lr: 0.001000 Speed: 153.439314 samples/sec   loss=2.496557   top1-acc=0.013518     top5-acc=0.084207\n",
      "2018-09-25 12:08:31,848 - Epoch[0] Batch [600] Lr: 0.001000 Speed: 156.518532 samples/sec   loss=2.398910   top1-acc=0.012620     top5-acc=0.074630\n",
      "2018-09-25 12:13:04,890 - Epoch[0] Batch [700] Lr: 0.001000 Speed: 164.083741 samples/sec   loss=2.329897   top1-acc=0.011259     top5-acc=0.068614\n",
      "2018-09-25 12:17:38,837 - Epoch[0] Batch [800] Lr: 0.001000 Speed: 163.540612 samples/sec   loss=2.275709   top1-acc=0.009971     top5-acc=0.066987\n",
      "2018-09-25 12:22:14,952 - Epoch[0] Batch [900] Lr: 0.001000 Speed: 162.256361 samples/sec   loss=2.220215   top1-acc=0.008929     top5-acc=0.078722\n",
      "2018-09-25 12:26:50,524 - Epoch[0] Batch [1000] Lr: 0.001000 Speed: 162.574071 samples/sec   loss=2.164734   top1-acc=0.008095     top5-acc=0.095940\n",
      "2018-09-25 12:31:22,737 - Epoch[0] Batch [1100] Lr: 0.001000 Speed: 164.581674 samples/sec   loss=2.129668   top1-acc=0.007485     top5-acc=0.093527\n",
      "2018-09-25 12:35:57,791 - Epoch[0] Batch [1200] Lr: 0.001000 Speed: 162.883597 samples/sec   loss=2.085900   top1-acc=0.007843     top5-acc=0.100809\n",
      "2018-09-25 12:40:31,535 - Epoch[0] Batch [1300] Lr: 0.001000 Speed: 163.665746 samples/sec   loss=2.048440   top1-acc=0.007292     top5-acc=0.094073\n",
      "2018-09-25 12:45:06,288 - Epoch[0] Batch [1400] Lr: 0.001000 Speed: 163.062477 samples/sec   loss=2.018030   top1-acc=0.006797     top5-acc=0.088993\n",
      "2018-09-25 12:49:42,506 - Epoch[0] Batch [1500] Lr: 0.001000 Speed: 162.197298 samples/sec   loss=1.991443   top1-acc=0.006356     top5-acc=0.084786\n",
      "2018-09-25 12:54:16,359 - Epoch[0] Batch [1600] Lr: 0.001000 Speed: 163.595968 samples/sec   loss=1.958092   top1-acc=0.006207     top5-acc=0.093031\n",
      "2018-09-25 12:58:50,120 - Epoch[0] Batch [1700] Lr: 0.001000 Speed: 163.652869 samples/sec   loss=1.912605   top1-acc=0.006031     top5-acc=0.123011\n",
      "2018-09-25 13:03:23,806 - Epoch[0] Batch [1800] Lr: 0.001000 Speed: 163.695824 samples/sec   loss=1.891529   top1-acc=0.005776     top5-acc=0.133790\n",
      "2018-09-25 13:08:01,334 - Epoch[0] Batch [1900] Lr: 0.001000 Speed: 161.430963 samples/sec   loss=1.871231   top1-acc=0.005495     top5-acc=0.144790\n",
      "2018-09-25 13:12:37,640 - Epoch[0] Batch [2000] Lr: 0.001000 Speed: 162.144785 samples/sec   loss=1.836391   top1-acc=0.005221     top5-acc=0.150398\n",
      "2018-09-25 13:17:16,497 - Epoch[0] Batch [2100] Lr: 0.001000 Speed: 160.661178 samples/sec   loss=1.805347   top1-acc=0.005063     top5-acc=0.169656\n",
      "2018-09-25 13:21:48,359 - Epoch[0] Batch [2200] Lr: 0.001000 Speed: 164.793170 samples/sec   loss=1.786439   top1-acc=0.004858     top5-acc=0.182671\n",
      "2018-09-25 13:26:19,761 - Epoch[0] Batch [2300] Lr: 0.001000 Speed: 165.076451 samples/sec   loss=1.764279   top1-acc=0.004680     top5-acc=0.198623\n",
      "2018-09-25 13:30:50,513 - Epoch[0] Batch [2400] Lr: 0.001000 Speed: 165.471892 samples/sec   loss=1.745235   top1-acc=0.004520     top5-acc=0.210100\n",
      "2018-09-25 13:35:20,063 - Epoch[0] Batch [2500] Lr: 0.001000 Speed: 166.208669 samples/sec   loss=1.734430   top1-acc=0.004339     top5-acc=0.215093\n",
      "2018-09-25 13:39:52,948 - Epoch[0] Batch [2600] Lr: 0.001000 Speed: 164.176300 samples/sec   loss=1.724010   top1-acc=0.004173     top5-acc=0.224387\n",
      "2018-09-25 13:44:27,154 - Epoch[0] Batch [2700] Lr: 0.001000 Speed: 163.385713 samples/sec   loss=1.710832   top1-acc=0.004018     top5-acc=0.233665\n",
      "2018-09-25 13:48:59,935 - Epoch[0] Batch [2800] Lr: 0.001000 Speed: 164.240255 samples/sec   loss=1.708710   top1-acc=0.003875     top5-acc=0.237742\n",
      "2018-09-25 13:53:31,812 - Epoch[0] Batch [2900] Lr: 0.001000 Speed: 164.784989 samples/sec   loss=1.696419   top1-acc=0.003743     top5-acc=0.243281\n",
      "2018-09-25 13:58:06,644 - Epoch[0] Batch [3000] Lr: 0.001000 Speed: 163.013234 samples/sec   loss=1.693804   top1-acc=0.003622     top5-acc=0.245991\n",
      "2018-09-25 14:02:39,291 - Epoch[0] Batch [3100] Lr: 0.001000 Speed: 164.320602 samples/sec   loss=1.683425   top1-acc=0.003520     top5-acc=0.254451\n",
      "2018-09-25 14:07:13,095 - Epoch[0] Batch [3200] Lr: 0.001000 Speed: 163.626394 samples/sec   loss=1.678382   top1-acc=0.003422     top5-acc=0.258519\n",
      "2018-09-25 14:11:46,029 - Epoch[0] Batch [3300] Lr: 0.001000 Speed: 164.146673 samples/sec   loss=1.680370   top1-acc=0.003320     top5-acc=0.256548\n",
      "2018-09-25 14:16:20,545 - Epoch[0] Batch [3400] Lr: 0.001000 Speed: 163.200077 samples/sec   loss=1.678834   top1-acc=0.003224     top5-acc=0.258662\n",
      "2018-09-25 14:20:55,369 - Epoch[0] Batch [3500] Lr: 0.001000 Speed: 163.017545 samples/sec   loss=1.674027   top1-acc=0.003134     top5-acc=0.262436\n",
      "2018-09-25 14:25:27,770 - Epoch[0] Batch [3600] Lr: 0.001000 Speed: 164.470139 samples/sec   loss=1.675484   top1-acc=0.003055     top5-acc=0.262575\n",
      "2018-09-25 14:29:59,142 - Epoch[0] Batch [3700] Lr: 0.001000 Speed: 165.091045 samples/sec   loss=1.674813   top1-acc=0.002975     top5-acc=0.262937\n",
      "2018-09-25 14:34:31,258 - Epoch[0] Batch [3800] Lr: 0.001000 Speed: 164.642524 samples/sec   loss=1.672334   top1-acc=0.002896     top5-acc=0.265074\n",
      "2018-09-25 14:39:10,752 - Epoch[0] Batch [3900] Lr: 0.001000 Speed: 160.294585 samples/sec   loss=1.668195   top1-acc=0.002822     top5-acc=0.265599\n",
      "2018-09-25 14:43:46,218 - Epoch[0] Batch [4000] Lr: 0.001000 Speed: 162.639362 samples/sec   loss=1.666688   top1-acc=0.002752     top5-acc=0.265387\n",
      "2018-09-25 14:48:20,189 - Epoch[0] Batch [4100] Lr: 0.001000 Speed: 163.526011 samples/sec   loss=1.670735   top1-acc=0.002684     top5-acc=0.259603\n",
      "2018-09-25 14:52:53,183 - Epoch[0] Batch [4200] Lr: 0.001000 Speed: 164.110360 samples/sec   loss=1.677705   top1-acc=0.002621     top5-acc=0.253500\n",
      "2018-09-25 14:57:29,246 - Epoch[0] Batch [4300] Lr: 0.001000 Speed: 162.288071 samples/sec   loss=1.683001   top1-acc=0.002560     top5-acc=0.247783\n",
      "2018-09-25 15:01:59,139 - Epoch[0] Batch [4400] Lr: 0.001000 Speed: 165.997437 samples/sec   loss=1.686427   top1-acc=0.002501     top5-acc=0.242654\n",
      "2018-09-25 15:06:28,782 - Epoch[0] Batch [4500] Lr: 0.001000 Speed: 166.150888 samples/sec   loss=1.688857   top1-acc=0.002447     top5-acc=0.238059\n",
      "2018-09-25 15:11:00,148 - Epoch[0] Batch [4600] Lr: 0.001000 Speed: 165.093871 samples/sec   loss=1.690177   top1-acc=0.002394     top5-acc=0.233758\n",
      "2018-09-25 15:15:33,944 - Epoch[0] Batch [4700] Lr: 0.001000 Speed: 163.632952 samples/sec   loss=1.688484   top1-acc=0.002344     top5-acc=0.230678\n",
      "2018-09-25 15:20:07,110 - Epoch[0] Batch [4800] Lr: 0.001000 Speed: 164.007524 samples/sec   loss=1.687740   top1-acc=0.002295     top5-acc=0.227536\n",
      "2018-09-25 15:24:37,930 - Epoch[0] Batch [4900] Lr: 0.001000 Speed: 165.431564 samples/sec   loss=1.687214   top1-acc=0.002249     top5-acc=0.228071\n",
      "2018-09-25 15:29:12,592 - Epoch[0] Batch [5000] Lr: 0.001000 Speed: 163.114261 samples/sec   loss=1.681877   top1-acc=0.002204     top5-acc=0.229901\n",
      "2018-09-25 15:33:42,219 - Epoch[0] Batch [5100] Lr: 0.001000 Speed: 166.159371 samples/sec   loss=1.669123   top1-acc=0.002169     top5-acc=0.235353\n",
      "2018-09-25 15:37:27,966 - [Epoch 0] training: acc-top1=0.002151 acc-top5=0.236154 loss=1.666692 lr=0.001000\n",
      "2018-09-25 15:37:27,967 - [Epoch 0] time cost: 14360.568743\n",
      "2018-09-25 15:37:27,968 - [Epoch 0] validation: acc-top1=0.000957 acc-top5=0.005022 best-acc-top1=0.000957 best-acc-top5=0.005022\n",
      "2018-09-25 15:37:27,968 - [Epoch 0] validation: acc-top1-max=0.001216 acc-top5-max=0.005082 best-acc-top1-max=0.001216 best-acc-top5-max=0.005082\n",
      "2018-09-25 15:37:27,969 - [Epoch 0] validation: acc-top1-min=0.000937 acc-top5-min=0.005022 best-acc-top1-min=0.000937 best-acc-top5-min=0.005022\n",
      "2018-09-25 15:38:38,081 - Epoch[1] Batch [0] Lr: 0.001000 Speed: 650.953331 samples/sec   loss=1.946362   top1-acc=0.000000     top5-acc=0.000000\n",
      "2018-09-25 15:43:08,884 - Epoch[1] Batch [100] Lr: 0.001000 Speed: 165.437027 samples/sec   loss=-0.074761   top1-acc=0.013128     top5-acc=0.805339\n",
      "2018-09-25 15:47:38,835 - Epoch[1] Batch [200] Lr: 0.001000 Speed: 165.961432 samples/sec   loss=-0.017785   top1-acc=0.007130     top5-acc=0.727745\n",
      "2018-09-25 15:52:12,237 - Epoch[1] Batch [300] Lr: 0.001000 Speed: 163.866172 samples/sec   loss=-0.103637   top1-acc=0.006185     top5-acc=0.793100\n",
      "2018-09-25 15:56:39,765 - Epoch[1] Batch [400] Lr: 0.001000 Speed: 167.465896 samples/sec   loss=-0.065332   top1-acc=0.005110     top5-acc=0.750690\n",
      "2018-09-25 16:01:05,728 - Epoch[1] Batch [500] Lr: 0.001000 Speed: 168.451499 samples/sec   loss=-0.119414   top1-acc=0.005115     top5-acc=0.790036\n",
      "2018-09-25 16:05:36,490 - Epoch[1] Batch [600] Lr: 0.001000 Speed: 165.462426 samples/sec   loss=-0.113132   top1-acc=0.005630     top5-acc=0.799590\n",
      "2018-09-25 16:10:09,432 - Epoch[1] Batch [700] Lr: 0.001000 Speed: 164.142633 samples/sec   loss=-0.129829   top1-acc=0.005904     top5-acc=0.813009\n",
      "2018-09-25 16:14:39,946 - Epoch[1] Batch [800] Lr: 0.001000 Speed: 165.616426 samples/sec   loss=-0.121219   top1-acc=0.005473     top5-acc=0.814328\n",
      "2018-09-25 16:19:11,992 - Epoch[1] Batch [900] Lr: 0.001000 Speed: 164.682433 samples/sec   loss=-0.120605   top1-acc=0.005976     top5-acc=0.819927\n",
      "2018-09-25 16:23:47,388 - Epoch[1] Batch [1000] Lr: 0.001000 Speed: 162.680064 samples/sec   loss=-0.125668   top1-acc=0.005495     top5-acc=0.826548\n",
      "2018-09-25 16:28:23,998 - Epoch[1] Batch [1100] Lr: 0.001000 Speed: 161.968245 samples/sec   loss=-0.159519   top1-acc=0.006435     top5-acc=0.834031\n",
      "2018-09-25 16:32:57,458 - Epoch[1] Batch [1200] Lr: 0.001000 Speed: 163.832553 samples/sec   loss=-0.136233   top1-acc=0.009163     top5-acc=0.820860\n",
      "2018-09-25 16:37:31,747 - Epoch[1] Batch [1300] Lr: 0.001000 Speed: 163.339454 samples/sec   loss=-0.115929   top1-acc=0.008575     top5-acc=0.799351\n",
      "2018-09-25 16:42:06,137 - Epoch[1] Batch [1400] Lr: 0.001000 Speed: 163.275896 samples/sec   loss=-0.112187   top1-acc=0.008441     top5-acc=0.796523\n",
      "2018-09-25 16:46:40,631 - Epoch[1] Batch [1500] Lr: 0.001000 Speed: 163.213920 samples/sec   loss=-0.125280   top1-acc=0.008557     top5-acc=0.804716\n",
      "2018-09-25 16:51:12,396 - Epoch[1] Batch [1600] Lr: 0.001000 Speed: 164.853169 samples/sec   loss=-0.136434   top1-acc=0.008998     top5-acc=0.809767\n",
      "2018-09-25 16:55:46,813 - Epoch[1] Batch [1700] Lr: 0.001000 Speed: 163.260112 samples/sec   loss=-0.148541   top1-acc=0.009107     top5-acc=0.818064\n",
      "2018-09-25 17:00:21,264 - Epoch[1] Batch [1800] Lr: 0.001000 Speed: 163.239765 samples/sec   loss=-0.164852   top1-acc=0.008983     top5-acc=0.826805\n",
      "2018-09-25 17:04:53,585 - Epoch[1] Batch [1900] Lr: 0.001000 Speed: 164.516228 samples/sec   loss=-0.160669   top1-acc=0.009910     top5-acc=0.821090\n",
      "2018-09-25 17:09:26,726 - Epoch[1] Batch [2000] Lr: 0.001000 Speed: 164.021503 samples/sec   loss=-0.164893   top1-acc=0.010829     top5-acc=0.824086\n",
      "2018-09-25 17:14:03,151 - Epoch[1] Batch [2100] Lr: 0.001000 Speed: 162.074827 samples/sec   loss=-0.174599   top1-acc=0.011776     top5-acc=0.828186\n",
      "2018-09-25 17:18:38,597 - Epoch[1] Batch [2200] Lr: 0.001000 Speed: 162.650007 samples/sec   loss=-0.181843   top1-acc=0.012107     top5-acc=0.834416\n",
      "2018-09-25 17:23:13,347 - Epoch[1] Batch [2300] Lr: 0.001000 Speed: 163.061960 samples/sec   loss=-0.200583   top1-acc=0.012145     top5-acc=0.839693\n",
      "2018-09-25 17:27:45,379 - Epoch[1] Batch [2400] Lr: 0.001000 Speed: 164.692853 samples/sec   loss=-0.213792   top1-acc=0.012277     top5-acc=0.845472\n",
      "2018-09-25 17:32:16,267 - Epoch[1] Batch [2500] Lr: 0.001000 Speed: 165.388955 samples/sec   loss=-0.221295   top1-acc=0.012120     top5-acc=0.850360\n",
      "2018-09-25 17:36:47,656 - Epoch[1] Batch [2600] Lr: 0.001000 Speed: 165.083120 samples/sec   loss=-0.231161   top1-acc=0.012076     top5-acc=0.854326\n",
      "2018-09-25 17:41:20,843 - Epoch[1] Batch [2700] Lr: 0.001000 Speed: 163.996133 samples/sec   loss=-0.238466   top1-acc=0.012084     top5-acc=0.858209\n",
      "2018-09-25 17:45:56,451 - Epoch[1] Batch [2800] Lr: 0.001000 Speed: 162.557272 samples/sec   loss=-0.238831   top1-acc=0.012075     top5-acc=0.859548\n",
      "2018-09-25 17:50:28,256 - Epoch[1] Batch [2900] Lr: 0.001000 Speed: 164.827085 samples/sec   loss=-0.245733   top1-acc=0.012186     top5-acc=0.862147\n",
      "2018-09-25 17:55:00,744 - Epoch[1] Batch [3000] Lr: 0.001000 Speed: 164.416554 samples/sec   loss=-0.256565   top1-acc=0.012160     top5-acc=0.866082\n",
      "2018-09-25 17:59:33,042 - Epoch[1] Batch [3100] Lr: 0.001000 Speed: 164.532399 samples/sec   loss=-0.265718   top1-acc=0.012185     top5-acc=0.869545\n",
      "2018-09-25 18:04:03,729 - Epoch[1] Batch [3200] Lr: 0.001000 Speed: 165.510869 samples/sec   loss=-0.269027   top1-acc=0.012531     top5-acc=0.871038\n",
      "2018-09-25 18:08:34,845 - Epoch[1] Batch [3300] Lr: 0.001000 Speed: 165.248551 samples/sec   loss=-0.273425   top1-acc=0.012921     top5-acc=0.872879\n",
      "2018-09-25 18:13:07,587 - Epoch[1] Batch [3400] Lr: 0.001000 Speed: 164.263994 samples/sec   loss=-0.272078   top1-acc=0.012744     top5-acc=0.872383\n",
      "2018-09-25 18:17:42,186 - Epoch[1] Batch [3500] Lr: 0.001000 Speed: 163.152502 samples/sec   loss=-0.276450   top1-acc=0.012903     top5-acc=0.874714\n",
      "2018-09-25 18:22:18,546 - Epoch[1] Batch [3600] Lr: 0.001000 Speed: 162.113681 samples/sec   loss=-0.278068   top1-acc=0.012701     top5-acc=0.877263\n",
      "2018-09-25 18:26:53,718 - Epoch[1] Batch [3700] Lr: 0.001000 Speed: 162.813519 samples/sec   loss=-0.277157   top1-acc=0.012776     top5-acc=0.877118\n",
      "2018-09-25 18:31:26,263 - Epoch[1] Batch [3800] Lr: 0.001000 Speed: 164.381751 samples/sec   loss=-0.284069   top1-acc=0.012877     top5-acc=0.879913\n",
      "2018-09-25 18:35:56,238 - Epoch[1] Batch [3900] Lr: 0.001000 Speed: 165.947458 samples/sec   loss=-0.288896   top1-acc=0.012917     top5-acc=0.881832\n",
      "2018-09-25 18:40:31,177 - Epoch[1] Batch [4000] Lr: 0.001000 Speed: 162.951222 samples/sec   loss=-0.291765   top1-acc=0.013539     top5-acc=0.882279\n",
      "2018-09-25 18:45:03,825 - Epoch[1] Batch [4100] Lr: 0.001000 Speed: 164.319506 samples/sec   loss=-0.280921   top1-acc=0.013273     top5-acc=0.873259\n",
      "2018-09-25 18:49:36,297 - Epoch[1] Batch [4200] Lr: 0.001000 Speed: 164.426561 samples/sec   loss=-0.279720   top1-acc=0.013407     top5-acc=0.872968\n",
      "2018-09-25 18:54:07,723 - Epoch[1] Batch [4300] Lr: 0.001000 Speed: 165.058933 samples/sec   loss=-0.274888   top1-acc=0.013167     top5-acc=0.872563\n",
      "2018-09-25 18:58:42,515 - Epoch[1] Batch [4400] Lr: 0.001000 Speed: 163.040092 samples/sec   loss=-0.276944   top1-acc=0.012991     top5-acc=0.874545\n",
      "2018-09-25 19:03:14,657 - Epoch[1] Batch [4500] Lr: 0.001000 Speed: 164.623686 samples/sec   loss=-0.270034   top1-acc=0.012820     top5-acc=0.873485\n",
      "2018-09-25 19:07:50,553 - Epoch[1] Batch [4600] Lr: 0.001000 Speed: 162.385714 samples/sec   loss=-0.269395   top1-acc=0.012632     top5-acc=0.872992\n",
      "2018-09-25 19:12:28,203 - Epoch[1] Batch [4700] Lr: 0.001000 Speed: 161.359447 samples/sec   loss=-0.272390   top1-acc=0.012438     top5-acc=0.875273\n",
      "2018-09-25 19:16:58,662 - Epoch[1] Batch [4800] Lr: 0.001000 Speed: 165.648790 samples/sec   loss=-0.272420   top1-acc=0.012389     top5-acc=0.876453\n",
      "2018-09-25 19:21:34,528 - Epoch[1] Batch [4900] Lr: 0.001000 Speed: 162.403012 samples/sec   loss=-0.271345   top1-acc=0.012299     top5-acc=0.877488\n",
      "2018-09-25 19:26:08,423 - Epoch[1] Batch [5000] Lr: 0.001000 Speed: 163.571435 samples/sec   loss=-0.278653   top1-acc=0.012623     top5-acc=0.879584\n",
      "2018-09-25 19:30:42,888 - Epoch[1] Batch [5100] Lr: 0.001000 Speed: 163.231427 samples/sec   loss=-0.277738   top1-acc=0.012614     top5-acc=0.880879\n",
      "2018-09-25 19:34:02,224 - [Epoch 1] training: acc-top1=0.012547 acc-top5=0.881135 loss=-0.276794 lr=0.001000\n",
      "2018-09-25 19:34:02,227 - [Epoch 1] time cost: 14192.968110\n",
      "2018-09-25 19:34:02,228 - [Epoch 1] validation: acc-top1=0.000957 acc-top5=0.004843 best-acc-top1=0.000957 best-acc-top5=0.005022\n",
      "2018-09-25 19:34:02,229 - [Epoch 1] validation: acc-top1-max=0.000996 acc-top5-max=0.004863 best-acc-top1-max=0.001216 best-acc-top5-max=0.005082\n",
      "2018-09-25 19:34:02,230 - [Epoch 1] validation: acc-top1-min=0.000996 acc-top5-min=0.004963 best-acc-top1-min=0.000996 best-acc-top5-min=0.005022\n",
      "2018-09-25 19:35:04,793 - Epoch[2] Batch [0] Lr: 0.001000 Speed: 724.204283 samples/sec   loss=0.830300   top1-acc=0.000000     top5-acc=0.919643\n",
      "2018-09-25 19:39:36,272 - Epoch[2] Batch [100] Lr: 0.001000 Speed: 165.024760 samples/sec   loss=-0.508937   top1-acc=0.009326     top5-acc=0.947976\n",
      "2018-09-25 19:44:10,927 - Epoch[2] Batch [200] Lr: 0.001000 Speed: 163.118891 samples/sec   loss=-0.635034   top1-acc=0.013215     top5-acc=0.959821\n",
      "2018-09-25 19:48:44,628 - Epoch[2] Batch [300] Lr: 0.001000 Speed: 163.690550 samples/sec   loss=-0.684500   top1-acc=0.015870     top5-acc=0.968201\n",
      "2018-09-25 19:53:17,904 - Epoch[2] Batch [400] Lr: 0.001000 Speed: 163.943739 samples/sec   loss=-0.673324   top1-acc=0.012480     top5-acc=0.967537\n",
      "2018-09-25 19:57:51,579 - Epoch[2] Batch [500] Lr: 0.001000 Speed: 163.705061 samples/sec   loss=-0.661638   top1-acc=0.012635     top5-acc=0.968500\n",
      "2018-09-25 20:02:24,687 - Epoch[2] Batch [600] Lr: 0.001000 Speed: 164.043639 samples/sec   loss=-0.676294   top1-acc=0.014359     top5-acc=0.971342\n",
      "2018-09-25 20:06:57,490 - Epoch[2] Batch [700] Lr: 0.001000 Speed: 164.224424 samples/sec   loss=-0.673815   top1-acc=0.017246     top5-acc=0.972692\n",
      "2018-09-25 20:11:29,670 - Epoch[2] Batch [800] Lr: 0.001000 Speed: 164.602298 samples/sec   loss=-0.665743   top1-acc=0.015817     top5-acc=0.970912\n",
      "2018-09-25 20:16:07,058 - Epoch[2] Batch [900] Lr: 0.001000 Speed: 161.511910 samples/sec   loss=-0.643045   top1-acc=0.016212     top5-acc=0.969250\n",
      "2018-09-25 20:20:38,827 - Epoch[2] Batch [1000] Lr: 0.001000 Speed: 164.850298 samples/sec   loss=-0.625251   top1-acc=0.014940     top5-acc=0.967127\n",
      "2018-09-25 20:25:10,022 - Epoch[2] Batch [1100] Lr: 0.001000 Speed: 165.199706 samples/sec   loss=-0.637609   top1-acc=0.015477     top5-acc=0.968547\n",
      "2018-09-25 20:29:41,530 - Epoch[2] Batch [1200] Lr: 0.001000 Speed: 165.009903 samples/sec   loss=-0.633230   top1-acc=0.015634     top5-acc=0.968274\n",
      "2018-09-25 20:34:13,281 - Epoch[2] Batch [1300] Lr: 0.001000 Speed: 164.862644 samples/sec   loss=-0.637680   top1-acc=0.015424     top5-acc=0.969697\n",
      "2018-09-25 20:38:47,625 - Epoch[2] Batch [1400] Lr: 0.001000 Speed: 163.302429 samples/sec   loss=-0.621262   top1-acc=0.014581     top5-acc=0.967099\n",
      "2018-09-25 20:43:23,019 - Epoch[2] Batch [1500] Lr: 0.001000 Speed: 162.679998 samples/sec   loss=-0.605239   top1-acc=0.014529     top5-acc=0.965945\n",
      "2018-09-25 20:47:57,415 - Epoch[2] Batch [1600] Lr: 0.001000 Speed: 163.273953 samples/sec   loss=-0.600213   top1-acc=0.014352     top5-acc=0.966438\n",
      "2018-09-25 20:52:32,175 - Epoch[2] Batch [1700] Lr: 0.001000 Speed: 163.057975 samples/sec   loss=-0.598880   top1-acc=0.014170     top5-acc=0.966406\n",
      "2018-09-25 20:57:04,641 - Epoch[2] Batch [1800] Lr: 0.001000 Speed: 164.428269 samples/sec   loss=-0.603293   top1-acc=0.014008     top5-acc=0.967471\n",
      "2018-09-25 21:01:36,511 - Epoch[2] Batch [1900] Lr: 0.001000 Speed: 164.792527 samples/sec   loss=-0.610774   top1-acc=0.014682     top5-acc=0.968271\n",
      "2018-09-25 21:06:10,366 - Epoch[2] Batch [2000] Lr: 0.001000 Speed: 163.596721 samples/sec   loss=-0.613001   top1-acc=0.014700     top5-acc=0.969009\n",
      "2018-09-25 21:10:44,296 - Epoch[2] Batch [2100] Lr: 0.001000 Speed: 163.549492 samples/sec   loss=-0.600368   top1-acc=0.014517     top5-acc=0.966685\n",
      "2018-09-25 21:15:15,869 - Epoch[2] Batch [2200] Lr: 0.001000 Speed: 164.969735 samples/sec   loss=-0.594164   top1-acc=0.014734     top5-acc=0.966912\n",
      "2018-09-25 21:19:49,016 - Epoch[2] Batch [2300] Lr: 0.001000 Speed: 164.020879 samples/sec   loss=-0.588301   top1-acc=0.014567     top5-acc=0.965574\n",
      "2018-09-25 21:24:24,902 - Epoch[2] Batch [2400] Lr: 0.001000 Speed: 162.390984 samples/sec   loss=-0.591677   top1-acc=0.014830     top5-acc=0.965420\n",
      "2018-09-25 21:28:58,758 - Epoch[2] Batch [2500] Lr: 0.001000 Speed: 163.593254 samples/sec   loss=-0.599171   top1-acc=0.014933     top5-acc=0.965946\n",
      "2018-09-25 21:33:32,546 - Epoch[2] Batch [2600] Lr: 0.001000 Speed: 163.635319 samples/sec   loss=-0.599144   top1-acc=0.015356     top5-acc=0.966091\n",
      "2018-09-25 21:38:07,061 - Epoch[2] Batch [2700] Lr: 0.001000 Speed: 163.203405 samples/sec   loss=-0.607522   top1-acc=0.015391     top5-acc=0.966656\n",
      "2018-09-25 21:42:42,175 - Epoch[2] Batch [2800] Lr: 0.001000 Speed: 162.845937 samples/sec   loss=-0.605410   top1-acc=0.015922     top5-acc=0.966798\n",
      "2018-09-25 21:47:17,307 - Epoch[2] Batch [2900] Lr: 0.001000 Speed: 162.834964 samples/sec   loss=-0.605524   top1-acc=0.015903     top5-acc=0.965638\n",
      "2018-09-25 21:51:48,456 - Epoch[2] Batch [3000] Lr: 0.001000 Speed: 165.227810 samples/sec   loss=-0.612847   top1-acc=0.015901     top5-acc=0.966147\n",
      "2018-09-25 21:56:20,342 - Epoch[2] Batch [3100] Lr: 0.001000 Speed: 164.780597 samples/sec   loss=-0.619778   top1-acc=0.015909     top5-acc=0.966428\n",
      "2018-09-25 22:00:52,624 - Epoch[2] Batch [3200] Lr: 0.001000 Speed: 164.539470 samples/sec   loss=-0.616182   top1-acc=0.015518     top5-acc=0.965047\n",
      "2018-09-25 22:05:22,845 - Epoch[2] Batch [3300] Lr: 0.001000 Speed: 165.797050 samples/sec   loss=-0.625597   top1-acc=0.015797     top5-acc=0.965216\n",
      "2018-09-25 22:09:53,913 - Epoch[2] Batch [3400] Lr: 0.001000 Speed: 165.277221 samples/sec   loss=-0.636495   top1-acc=0.016130     top5-acc=0.965703\n",
      "2018-09-25 22:14:25,184 - Epoch[2] Batch [3500] Lr: 0.001000 Speed: 165.152615 samples/sec   loss=-0.646624   top1-acc=0.016087     top5-acc=0.966200\n",
      "2018-09-25 22:18:56,665 - Epoch[2] Batch [3600] Lr: 0.001000 Speed: 165.025918 samples/sec   loss=-0.652813   top1-acc=0.017640     top5-acc=0.965865\n",
      "2018-09-25 22:23:28,659 - Epoch[2] Batch [3700] Lr: 0.001000 Speed: 164.715297 samples/sec   loss=-0.648846   top1-acc=0.017431     top5-acc=0.964599\n",
      "2018-09-25 22:28:03,967 - Epoch[2] Batch [3800] Lr: 0.001000 Speed: 162.731038 samples/sec   loss=-0.654270   top1-acc=0.018215     top5-acc=0.964795\n",
      "2018-09-25 22:32:35,850 - Epoch[2] Batch [3900] Lr: 0.001000 Speed: 164.783600 samples/sec   loss=-0.660357   top1-acc=0.018285     top5-acc=0.965282\n",
      "2018-09-25 22:37:08,228 - Epoch[2] Batch [4000] Lr: 0.001000 Speed: 164.482858 samples/sec   loss=-0.663909   top1-acc=0.018926     top5-acc=0.965467\n",
      "2018-09-25 22:41:41,249 - Epoch[2] Batch [4100] Lr: 0.001000 Speed: 164.097021 samples/sec   loss=-0.659592   top1-acc=0.018862     top5-acc=0.965367\n",
      "2018-09-25 22:46:13,262 - Epoch[2] Batch [4200] Lr: 0.001000 Speed: 164.704256 samples/sec   loss=-0.660323   top1-acc=0.019140     top5-acc=0.965426\n",
      "2018-09-25 22:50:47,672 - Epoch[2] Batch [4300] Lr: 0.001000 Speed: 163.265192 samples/sec   loss=-0.659840   top1-acc=0.019047     top5-acc=0.965561\n",
      "2018-09-25 22:55:18,296 - Epoch[2] Batch [4400] Lr: 0.001000 Speed: 165.550613 samples/sec   loss=-0.664853   top1-acc=0.019461     top5-acc=0.965883\n",
      "2018-09-25 22:59:47,429 - Epoch[2] Batch [4500] Lr: 0.001000 Speed: 166.465927 samples/sec   loss=-0.657410   top1-acc=0.019275     top5-acc=0.964162\n",
      "2018-09-25 23:04:20,086 - Epoch[2] Batch [4600] Lr: 0.001000 Speed: 164.314622 samples/sec   loss=-0.659329   top1-acc=0.019596     top5-acc=0.964384\n",
      "2018-09-25 23:08:52,640 - Epoch[2] Batch [4700] Lr: 0.001000 Speed: 164.376100 samples/sec   loss=-0.663210   top1-acc=0.019459     top5-acc=0.964759\n",
      "2018-09-25 23:13:26,995 - Epoch[2] Batch [4800] Lr: 0.001000 Speed: 163.297889 samples/sec   loss=-0.663083   top1-acc=0.019351     top5-acc=0.964902\n",
      "2018-09-25 23:17:59,116 - Epoch[2] Batch [4900] Lr: 0.001000 Speed: 164.639617 samples/sec   loss=-0.665930   top1-acc=0.019408     top5-acc=0.965117\n",
      "2018-09-25 23:22:29,207 - Epoch[2] Batch [5000] Lr: 0.001000 Speed: 165.876619 samples/sec   loss=-0.667713   top1-acc=0.019592     top5-acc=0.965304\n",
      "2018-09-25 23:27:02,480 - Epoch[2] Batch [5100] Lr: 0.001000 Speed: 163.946135 samples/sec   loss=-0.669564   top1-acc=0.019599     top5-acc=0.965316\n",
      "2018-09-25 23:30:22,252 - [Epoch 2] training: acc-top1=0.019477 acc-top5=0.965242 loss=-0.668043 lr=0.001000\n",
      "2018-09-25 23:30:22,254 - [Epoch 2] time cost: 14179.322294\n",
      "2018-09-25 23:30:22,255 - [Epoch 2] validation: acc-top1=0.001005 acc-top5=0.004887 best-acc-top1=0.001005 best-acc-top5=0.005022\n",
      "2018-09-25 23:30:22,256 - [Epoch 2] validation: acc-top1-max=0.000985 acc-top5-max=0.004987 best-acc-top1-max=0.001216 best-acc-top5-max=0.005082\n",
      "2018-09-25 23:30:22,257 - [Epoch 2] validation: acc-top1-min=0.000985 acc-top5-min=0.004927 best-acc-top1-min=0.000996 best-acc-top5-min=0.005022\n",
      "2018-09-25 23:31:23,956 - Epoch[3] Batch [0] Lr: 0.001000 Speed: 735.087227 samples/sec   loss=0.042376   top1-acc=0.000000     top5-acc=0.959821\n",
      "2018-09-25 23:35:59,628 - Epoch[3] Batch [100] Lr: 0.001000 Speed: 162.514425 samples/sec   loss=-1.024891   top1-acc=0.021791     top5-acc=0.985104\n",
      "2018-09-25 23:40:33,060 - Epoch[3] Batch [200] Lr: 0.001000 Speed: 163.849672 samples/sec   loss=-0.979616   top1-acc=0.015703     top5-acc=0.979877\n",
      "2018-09-25 23:45:06,344 - Epoch[3] Batch [300] Lr: 0.001000 Speed: 163.937240 samples/sec   loss=-0.994835   top1-acc=0.014832     top5-acc=0.980615\n",
      "2018-09-25 23:49:38,098 - Epoch[3] Batch [400] Lr: 0.001000 Speed: 164.859260 samples/sec   loss=-0.990481   top1-acc=0.019416     top5-acc=0.979816\n",
      "2018-09-25 23:54:09,022 - Epoch[3] Batch [500] Lr: 0.001000 Speed: 165.363494 samples/sec   loss=-0.970652   top1-acc=0.021751     top5-acc=0.978846\n",
      "2018-09-25 23:58:38,832 - Epoch[3] Batch [600] Lr: 0.001000 Speed: 166.045761 samples/sec   loss=-0.973652   top1-acc=0.020888     top5-acc=0.978733\n",
      "2018-09-26 00:03:10,183 - Epoch[3] Batch [700] Lr: 0.001000 Speed: 165.105085 samples/sec   loss=-0.964639   top1-acc=0.021646     top5-acc=0.978169\n",
      "2018-09-26 00:07:45,081 - Epoch[3] Batch [800] Lr: 0.001000 Speed: 162.973364 samples/sec   loss=-0.962988   top1-acc=0.023263     top5-acc=0.977985\n",
      "2018-09-26 00:12:18,214 - Epoch[3] Batch [900] Lr: 0.001000 Speed: 164.030027 samples/sec   loss=-0.949263   top1-acc=0.025567     top5-acc=0.976896\n",
      "2018-09-26 00:16:52,657 - Epoch[3] Batch [1000] Lr: 0.001000 Speed: 163.246631 samples/sec   loss=-0.922238   top1-acc=0.023851     top5-acc=0.976778\n",
      "2018-09-26 00:21:25,107 - Epoch[3] Batch [1100] Lr: 0.001000 Speed: 164.438943 samples/sec   loss=-0.900858   top1-acc=0.023351     top5-acc=0.976255\n",
      "2018-09-26 00:25:59,663 - Epoch[3] Batch [1200] Lr: 0.001000 Speed: 163.178138 samples/sec   loss=-0.885510   top1-acc=0.022994     top5-acc=0.975801\n",
      "2018-09-26 00:30:31,358 - Epoch[3] Batch [1300] Lr: 0.001000 Speed: 164.896492 samples/sec   loss=-0.868450   top1-acc=0.023039     top5-acc=0.974731\n",
      "2018-09-26 00:35:04,093 - Epoch[3] Batch [1400] Lr: 0.001000 Speed: 164.267953 samples/sec   loss=-0.862930   top1-acc=0.023003     top5-acc=0.974330\n",
      "2018-09-26 00:39:35,257 - Epoch[3] Batch [1500] Lr: 0.001000 Speed: 165.218783 samples/sec   loss=-0.846500   top1-acc=0.022289     top5-acc=0.973753\n",
      "2018-09-26 00:44:07,056 - Epoch[3] Batch [1600] Lr: 0.001000 Speed: 164.834368 samples/sec   loss=-0.841794   top1-acc=0.022620     top5-acc=0.973677\n",
      "2018-09-26 00:48:40,976 - Epoch[3] Batch [1700] Lr: 0.001000 Speed: 163.556029 samples/sec   loss=-0.826776   top1-acc=0.021660     top5-acc=0.973737\n",
      "2018-09-26 00:53:15,509 - Epoch[3] Batch [1800] Lr: 0.001000 Speed: 163.190929 samples/sec   loss=-0.818236   top1-acc=0.021739     top5-acc=0.973656\n",
      "2018-09-26 00:57:48,251 - Epoch[3] Batch [1900] Lr: 0.001000 Speed: 164.262327 samples/sec   loss=-0.826302   top1-acc=0.022718     top5-acc=0.974102\n",
      "2018-09-26 01:02:22,928 - Epoch[3] Batch [2000] Lr: 0.001000 Speed: 163.105787 samples/sec   loss=-0.814531   top1-acc=0.022625     top5-acc=0.973337\n",
      "2018-09-26 01:06:53,457 - Epoch[3] Batch [2100] Lr: 0.001000 Speed: 165.606802 samples/sec   loss=-0.803012   top1-acc=0.023773     top5-acc=0.972360\n",
      "2018-09-26 01:11:26,396 - Epoch[3] Batch [2200] Lr: 0.001000 Speed: 164.143360 samples/sec   loss=-0.801634   top1-acc=0.023137     top5-acc=0.972129\n",
      "2018-09-26 01:15:58,781 - Epoch[3] Batch [2300] Lr: 0.001000 Speed: 164.476170 samples/sec   loss=-0.794353   top1-acc=0.023088     top5-acc=0.971821\n",
      "2018-09-26 01:20:29,744 - Epoch[3] Batch [2400] Lr: 0.001000 Speed: 165.343860 samples/sec   loss=-0.798988   top1-acc=0.022721     top5-acc=0.972246\n",
      "2018-09-26 01:24:59,141 - Epoch[3] Batch [2500] Lr: 0.001000 Speed: 166.301587 samples/sec   loss=-0.797214   top1-acc=0.022154     top5-acc=0.972240\n",
      "2018-09-26 01:29:27,653 - Epoch[3] Batch [2600] Lr: 0.001000 Speed: 166.850113 samples/sec   loss=-0.797770   top1-acc=0.022891     top5-acc=0.972279\n",
      "2018-09-26 01:34:01,151 - Epoch[3] Batch [2700] Lr: 0.001000 Speed: 163.809484 samples/sec   loss=-0.800684   top1-acc=0.022505     top5-acc=0.972626\n",
      "2018-09-26 01:38:34,467 - Epoch[3] Batch [2800] Lr: 0.001000 Speed: 163.916531 samples/sec   loss=-0.799360   top1-acc=0.023013     top5-acc=0.972516\n",
      "2018-09-26 01:43:08,611 - Epoch[3] Batch [2900] Lr: 0.001000 Speed: 163.424497 samples/sec   loss=-0.796393   top1-acc=0.022849     top5-acc=0.972336\n",
      "2018-09-26 01:47:39,697 - Epoch[3] Batch [3000] Lr: 0.001000 Speed: 165.266511 samples/sec   loss=-0.800103   top1-acc=0.022692     top5-acc=0.972508\n",
      "2018-09-26 01:52:09,500 - Epoch[3] Batch [3100] Lr: 0.001000 Speed: 166.052292 samples/sec   loss=-0.807057   top1-acc=0.023277     top5-acc=0.972749\n",
      "2018-09-26 01:56:45,249 - Epoch[3] Batch [3200] Lr: 0.001000 Speed: 162.471586 samples/sec   loss=-0.809332   top1-acc=0.022950     top5-acc=0.972803\n",
      "2018-09-26 02:01:18,167 - Epoch[3] Batch [3300] Lr: 0.001000 Speed: 164.155980 samples/sec   loss=-0.812653   top1-acc=0.022641     top5-acc=0.972844\n",
      "2018-09-26 02:05:50,357 - Epoch[3] Batch [3400] Lr: 0.001000 Speed: 164.596869 samples/sec   loss=-0.819762   top1-acc=0.022924     top5-acc=0.973029\n",
      "2018-09-26 02:10:22,191 - Epoch[3] Batch [3500] Lr: 0.001000 Speed: 164.809811 samples/sec   loss=-0.824042   top1-acc=0.022649     top5-acc=0.973291\n",
      "2018-09-26 02:14:53,948 - Epoch[3] Batch [3600] Lr: 0.001000 Speed: 164.857051 samples/sec   loss=-0.831535   top1-acc=0.024283     top5-acc=0.973534\n",
      "2018-09-26 02:19:23,601 - Epoch[3] Batch [3700] Lr: 0.001000 Speed: 166.144053 samples/sec   loss=-0.828159   top1-acc=0.024401     top5-acc=0.972661\n",
      "2018-09-26 02:23:55,412 - Epoch[3] Batch [3800] Lr: 0.001000 Speed: 164.823716 samples/sec   loss=-0.834889   top1-acc=0.024199     top5-acc=0.972935\n",
      "2018-09-26 02:28:30,160 - Epoch[3] Batch [3900] Lr: 0.001000 Speed: 163.063403 samples/sec   loss=-0.838003   top1-acc=0.023965     top5-acc=0.973078\n",
      "2018-09-26 02:33:02,074 - Epoch[3] Batch [4000] Lr: 0.001000 Speed: 164.762161 samples/sec   loss=-0.837654   top1-acc=0.024128     top5-acc=0.972990\n",
      "2018-09-26 02:37:37,783 - Epoch[3] Batch [4100] Lr: 0.001000 Speed: 162.496149 samples/sec   loss=-0.839346   top1-acc=0.024220     top5-acc=0.972933\n",
      "2018-09-26 02:42:10,759 - Epoch[3] Batch [4200] Lr: 0.001000 Speed: 164.122352 samples/sec   loss=-0.843197   top1-acc=0.024047     top5-acc=0.972844\n",
      "2018-09-26 02:46:40,407 - Epoch[3] Batch [4300] Lr: 0.001000 Speed: 166.148373 samples/sec   loss=-0.842204   top1-acc=0.023615     top5-acc=0.972796\n",
      "2018-09-26 02:51:14,206 - Epoch[3] Batch [4400] Lr: 0.001000 Speed: 163.630620 samples/sec   loss=-0.844332   top1-acc=0.023235     top5-acc=0.973092\n",
      "2018-09-26 02:55:46,646 - Epoch[3] Batch [4500] Lr: 0.001000 Speed: 164.445431 samples/sec   loss=-0.842618   top1-acc=0.023078     top5-acc=0.972913\n",
      "2018-09-26 03:00:17,510 - Epoch[3] Batch [4600] Lr: 0.001000 Speed: 165.405198 samples/sec   loss=-0.844871   top1-acc=0.022943     top5-acc=0.972952\n",
      "2018-09-26 03:04:52,715 - Epoch[3] Batch [4700] Lr: 0.001000 Speed: 162.792696 samples/sec   loss=-0.845608   top1-acc=0.022658     top5-acc=0.972964\n",
      "2018-09-26 03:09:25,461 - Epoch[3] Batch [4800] Lr: 0.001000 Speed: 164.259568 samples/sec   loss=-0.842941   top1-acc=0.022387     top5-acc=0.972863\n",
      "2018-09-26 03:13:59,664 - Epoch[3] Batch [4900] Lr: 0.001000 Speed: 163.388680 samples/sec   loss=-0.844387   top1-acc=0.022498     top5-acc=0.972907\n",
      "2018-09-26 03:18:33,538 - Epoch[3] Batch [5000] Lr: 0.001000 Speed: 163.583730 samples/sec   loss=-0.848306   top1-acc=0.022448     top5-acc=0.973154\n",
      "2018-09-26 03:23:01,415 - Epoch[3] Batch [5100] Lr: 0.001000 Speed: 167.244729 samples/sec   loss=-0.851704   top1-acc=0.022222     top5-acc=0.973223\n",
      "2018-09-26 03:26:17,914 - [Epoch 3] training: acc-top1=0.022148 acc-top5=0.973112 loss=-0.850253 lr=0.001000\n",
      "2018-09-26 03:26:17,917 - [Epoch 3] time cost: 14154.906461\n",
      "2018-09-26 03:26:17,918 - [Epoch 3] validation: acc-top1=0.001056 acc-top5=0.005022 best-acc-top1=0.001056 best-acc-top5=0.005022\n",
      "2018-09-26 03:26:17,919 - [Epoch 3] validation: acc-top1-max=0.001056 acc-top5-max=0.005022 best-acc-top1-max=0.001216 best-acc-top5-max=0.005082\n",
      "2018-09-26 03:26:17,920 - [Epoch 3] validation: acc-top1-min=0.001016 acc-top5-min=0.004982 best-acc-top1-min=0.001016 best-acc-top5-min=0.005022\n",
      "2018-09-26 03:27:21,089 - Epoch[4] Batch [0] Lr: 0.001000 Speed: 716.692230 samples/sec   loss=-0.076852   top1-acc=0.000000     top5-acc=0.928571\n",
      "2018-09-26 03:31:46,400 - Epoch[4] Batch [100] Lr: 0.001000 Speed: 168.861769 samples/sec   loss=-1.070173   top1-acc=0.013128     top5-acc=0.971579\n",
      "2018-09-26 03:36:15,240 - Epoch[4] Batch [200] Lr: 0.001000 Speed: 166.646941 samples/sec   loss=-1.032368   top1-acc=0.015792     top5-acc=0.969527\n",
      "2018-09-26 03:40:42,945 - Epoch[4] Batch [300] Lr: 0.001000 Speed: 167.354134 samples/sec   loss=-1.074751   top1-acc=0.022944     top5-acc=0.971583\n",
      "2018-09-26 03:45:11,676 - Epoch[4] Batch [400] Lr: 0.001000 Speed: 166.715076 samples/sec   loss=-1.099276   top1-acc=0.027621     top5-acc=0.973404\n",
      "2018-09-26 03:49:40,071 - Epoch[4] Batch [500] Lr: 0.001000 Speed: 166.924608 samples/sec   loss=-1.105648   top1-acc=0.037176     top5-acc=0.974105\n",
      "2018-09-26 03:54:07,905 - Epoch[4] Batch [600] Lr: 0.001000 Speed: 167.273237 samples/sec   loss=-1.103914   top1-acc=0.035098     top5-acc=0.974663\n",
      "2018-09-26 03:58:34,788 - Epoch[4] Batch [700] Lr: 0.001000 Speed: 167.869108 samples/sec   loss=-1.115673   top1-acc=0.033020     top5-acc=0.975055\n",
      "2018-09-26 04:03:06,112 - Epoch[4] Batch [800] Lr: 0.001000 Speed: 165.121403 samples/sec   loss=-1.110046   top1-acc=0.031846     top5-acc=0.975098\n",
      "2018-09-26 04:07:33,004 - Epoch[4] Batch [900] Lr: 0.001000 Speed: 167.863011 samples/sec   loss=-1.088193   top1-acc=0.030398     top5-acc=0.974433\n",
      "2018-09-26 04:12:02,084 - Epoch[4] Batch [1000] Lr: 0.001000 Speed: 166.498684 samples/sec   loss=-1.061673   top1-acc=0.027976     top5-acc=0.975145\n",
      "2018-09-26 04:16:32,447 - Epoch[4] Batch [1100] Lr: 0.001000 Speed: 165.709902 samples/sec   loss=-1.046134   top1-acc=0.027309     top5-acc=0.974861\n",
      "2018-09-26 04:20:56,539 - Epoch[4] Batch [1200] Lr: 0.001000 Speed: 169.644863 samples/sec   loss=-1.030290   top1-acc=0.026042     top5-acc=0.974913\n",
      "2018-09-26 04:25:26,581 - Epoch[4] Batch [1300] Lr: 0.001000 Speed: 165.906941 samples/sec   loss=-1.017552   top1-acc=0.024940     top5-acc=0.974453\n",
      "2018-09-26 04:29:56,081 - Epoch[4] Batch [1400] Lr: 0.001000 Speed: 166.242966 samples/sec   loss=-1.011372   top1-acc=0.024338     top5-acc=0.974734\n",
      "2018-09-26 04:34:25,271 - Epoch[4] Batch [1500] Lr: 0.001000 Speed: 166.429804 samples/sec   loss=-1.007272   top1-acc=0.024017     top5-acc=0.974633\n",
      "2018-09-26 04:38:56,115 - Epoch[4] Batch [1600] Lr: 0.001000 Speed: 165.414854 samples/sec   loss=-1.003868   top1-acc=0.023021     top5-acc=0.974525\n",
      "2018-09-26 04:43:25,694 - Epoch[4] Batch [1700] Lr: 0.001000 Speed: 166.189379 samples/sec   loss=-1.001566   top1-acc=0.022518     top5-acc=0.974613\n",
      "2018-09-26 04:47:55,753 - Epoch[4] Batch [1800] Lr: 0.001000 Speed: 165.895376 samples/sec   loss=-0.996169   top1-acc=0.021635     top5-acc=0.974583\n",
      "2018-09-26 04:52:23,872 - Epoch[4] Batch [1900] Lr: 0.001000 Speed: 167.097816 samples/sec   loss=-0.994807   top1-acc=0.022526     top5-acc=0.974560\n",
      "2018-09-26 04:56:51,493 - Epoch[4] Batch [2000] Lr: 0.001000 Speed: 167.405995 samples/sec   loss=-0.987058   top1-acc=0.021938     top5-acc=0.974769\n",
      "2018-09-26 05:01:22,100 - Epoch[4] Batch [2100] Lr: 0.001000 Speed: 165.557105 samples/sec   loss=-0.977087   top1-acc=0.021807     top5-acc=0.974385\n",
      "2018-09-26 05:05:48,150 - Epoch[4] Batch [2200] Lr: 0.001000 Speed: 168.393148 samples/sec   loss=-0.975253   top1-acc=0.021253     top5-acc=0.974283\n",
      "2018-09-26 05:10:14,243 - Epoch[4] Batch [2300] Lr: 0.001000 Speed: 168.368853 samples/sec   loss=-0.968690   top1-acc=0.020958     top5-acc=0.973818\n",
      "2018-09-26 05:14:42,087 - Epoch[4] Batch [2400] Lr: 0.001000 Speed: 167.265490 samples/sec   loss=-0.978396   top1-acc=0.022470     top5-acc=0.974058\n",
      "2018-09-26 05:19:10,586 - Epoch[4] Batch [2500] Lr: 0.001000 Speed: 166.859171 samples/sec   loss=-0.984137   top1-acc=0.022343     top5-acc=0.974250\n",
      "2018-09-26 05:23:35,847 - Epoch[4] Batch [2600] Lr: 0.001000 Speed: 168.896887 samples/sec   loss=-0.984868   top1-acc=0.022114     top5-acc=0.974333\n",
      "2018-09-26 05:28:06,538 - Epoch[4] Batch [2700] Lr: 0.001000 Speed: 165.510360 samples/sec   loss=-0.990441   top1-acc=0.021793     top5-acc=0.974449\n",
      "2018-09-26 05:32:34,978 - Epoch[4] Batch [2800] Lr: 0.001000 Speed: 166.894341 samples/sec   loss=-0.987213   top1-acc=0.021791     top5-acc=0.974290\n",
      "2018-09-26 05:37:05,034 - Epoch[4] Batch [2900] Lr: 0.001000 Speed: 165.894967 samples/sec   loss=-0.985090   top1-acc=0.021495     top5-acc=0.973971\n",
      "2018-09-26 05:41:33,700 - Epoch[4] Batch [3000] Lr: 0.001000 Speed: 166.754451 samples/sec   loss=-0.989506   top1-acc=0.021233     top5-acc=0.974165\n",
      "2018-09-26 05:45:59,192 - Epoch[4] Batch [3100] Lr: 0.001000 Speed: 168.748191 samples/sec   loss=-0.986146   top1-acc=0.020925     top5-acc=0.973951\n",
      "2018-09-26 05:50:29,151 - Epoch[4] Batch [3200] Lr: 0.001000 Speed: 165.958563 samples/sec   loss=-0.979659   top1-acc=0.020489     top5-acc=0.973627\n",
      "2018-09-26 05:54:54,912 - Epoch[4] Batch [3300] Lr: 0.001000 Speed: 168.578001 samples/sec   loss=-0.980885   top1-acc=0.020282     top5-acc=0.973646\n",
      "2018-09-26 05:59:21,708 - Epoch[4] Batch [3400] Lr: 0.001000 Speed: 167.925204 samples/sec   loss=-0.983646   top1-acc=0.020060     top5-acc=0.973815\n",
      "2018-09-26 06:03:50,847 - Epoch[4] Batch [3500] Lr: 0.001000 Speed: 166.461357 samples/sec   loss=-0.985847   top1-acc=0.019794     top5-acc=0.974048\n",
      "2018-09-26 06:08:19,209 - Epoch[4] Batch [3600] Lr: 0.001000 Speed: 166.944712 samples/sec   loss=-0.985832   top1-acc=0.020147     top5-acc=0.973964\n",
      "2018-09-26 06:12:50,541 - Epoch[4] Batch [3700] Lr: 0.001000 Speed: 165.117270 samples/sec   loss=-0.980846   top1-acc=0.020067     top5-acc=0.972826\n",
      "2018-09-26 06:17:20,787 - Epoch[4] Batch [3800] Lr: 0.001000 Speed: 165.778358 samples/sec   loss=-0.983356   top1-acc=0.020126     top5-acc=0.972811\n",
      "2018-09-26 06:21:45,148 - Epoch[4] Batch [3900] Lr: 0.001000 Speed: 169.468348 samples/sec   loss=-0.983851   top1-acc=0.019800     top5-acc=0.972863\n",
      "2018-09-26 06:26:11,656 - Epoch[4] Batch [4000] Lr: 0.001000 Speed: 168.106168 samples/sec   loss=-0.978896   top1-acc=0.019615     top5-acc=0.972748\n",
      "2018-09-26 06:30:40,786 - Epoch[4] Batch [4100] Lr: 0.001000 Speed: 166.467977 samples/sec   loss=-0.979125   top1-acc=0.019767     top5-acc=0.972629\n",
      "2018-09-26 06:35:08,312 - Epoch[4] Batch [4200] Lr: 0.001000 Speed: 167.464684 samples/sec   loss=-0.978775   top1-acc=0.019845     top5-acc=0.972553\n",
      "2018-09-26 06:39:33,823 - Epoch[4] Batch [4300] Lr: 0.001000 Speed: 168.741173 samples/sec   loss=-0.978239   top1-acc=0.019800     top5-acc=0.972467\n",
      "2018-09-26 06:44:02,961 - Epoch[4] Batch [4400] Lr: 0.001000 Speed: 166.462310 samples/sec   loss=-0.980102   top1-acc=0.019583     top5-acc=0.972598\n",
      "2018-09-26 06:48:30,923 - Epoch[4] Batch [4500] Lr: 0.001000 Speed: 167.194488 samples/sec   loss=-0.977587   top1-acc=0.019487     top5-acc=0.972453\n",
      "2018-09-26 06:53:01,522 - Epoch[4] Batch [4600] Lr: 0.001000 Speed: 165.563235 samples/sec   loss=-0.976600   top1-acc=0.019211     top5-acc=0.972266\n",
      "2018-09-26 06:57:30,110 - Epoch[4] Batch [4700] Lr: 0.001000 Speed: 166.802962 samples/sec   loss=-0.977516   top1-acc=0.019061     top5-acc=0.972165\n",
      "2018-09-26 07:02:01,843 - Epoch[4] Batch [4800] Lr: 0.001000 Speed: 164.874189 samples/sec   loss=-0.975626   top1-acc=0.018881     top5-acc=0.972032\n",
      "2018-09-26 07:06:30,825 - Epoch[4] Batch [4900] Lr: 0.001000 Speed: 166.558461 samples/sec   loss=-0.973599   top1-acc=0.018906     top5-acc=0.971917\n",
      "2018-09-26 07:10:59,018 - Epoch[4] Batch [5000] Lr: 0.001000 Speed: 167.048207 samples/sec   loss=-0.973730   top1-acc=0.018960     top5-acc=0.971907\n",
      "2018-09-26 07:15:25,539 - Epoch[4] Batch [5100] Lr: 0.001000 Speed: 168.097984 samples/sec   loss=-0.976339   top1-acc=0.018921     top5-acc=0.971991\n",
      "2018-09-26 07:18:42,113 - [Epoch 4] training: acc-top1=0.019040 acc-top5=0.972003 loss=-0.977218 lr=0.001000\n",
      "2018-09-26 07:18:42,116 - [Epoch 4] time cost: 13943.536859\n",
      "2018-09-26 07:18:42,118 - [Epoch 4] validation: acc-top1=0.000937 acc-top5=0.005002 best-acc-top1=0.001056 best-acc-top5=0.005022\n",
      "2018-09-26 07:18:42,119 - [Epoch 4] validation: acc-top1-max=0.000937 acc-top5-max=0.004943 best-acc-top1-max=0.001216 best-acc-top5-max=0.005082\n",
      "2018-09-26 07:18:42,120 - [Epoch 4] validation: acc-top1-min=0.000996 acc-top5-min=0.004982 best-acc-top1-min=0.001016 best-acc-top5-min=0.005022\n",
      "2018-09-26 07:19:42,119 - Epoch[5] Batch [0] Lr: 0.001000 Speed: 755.871042 samples/sec   loss=-0.571896   top1-acc=0.000000     top5-acc=0.964286\n",
      "2018-09-26 07:24:14,732 - Epoch[5] Batch [100] Lr: 0.001000 Speed: 164.338188 samples/sec   loss=-1.234324   top1-acc=0.017769     top5-acc=0.975734\n",
      "2018-09-26 07:28:50,290 - Epoch[5] Batch [200] Lr: 0.001000 Speed: 162.584910 samples/sec   loss=-1.196730   top1-acc=0.019234     top5-acc=0.976302\n",
      "2018-09-26 07:33:23,871 - Epoch[5] Batch [300] Lr: 0.001000 Speed: 163.758888 samples/sec   loss=-1.144159   top1-acc=0.017501     top5-acc=0.973956\n",
      "2018-09-26 07:37:57,239 - Epoch[5] Batch [400] Lr: 0.001000 Speed: 163.886431 samples/sec   loss=-1.130660   top1-acc=0.015341     top5-acc=0.973426\n",
      "2018-09-26 07:42:27,393 - Epoch[5] Batch [500] Lr: 0.001000 Speed: 165.833930 samples/sec   loss=-1.100398   top1-acc=0.015799     top5-acc=0.972929\n",
      "2018-09-26 07:46:59,748 - Epoch[5] Batch [600] Lr: 0.001000 Speed: 164.495788 samples/sec   loss=-1.093714   top1-acc=0.014760     top5-acc=0.972256\n",
      "2018-09-26 07:51:33,239 - Epoch[5] Batch [700] Lr: 0.001000 Speed: 163.814377 samples/sec   loss=-1.098920   top1-acc=0.013399     top5-acc=0.973189\n",
      "2018-09-26 07:56:08,220 - Epoch[5] Batch [800] Lr: 0.001000 Speed: 162.924823 samples/sec   loss=-1.086086   top1-acc=0.013599     top5-acc=0.972088\n",
      "2018-09-26 08:00:43,117 - Epoch[5] Batch [900] Lr: 0.001000 Speed: 162.974269 samples/sec   loss=-1.084930   top1-acc=0.014671     top5-acc=0.971782\n"
     ]
    }
   ],
   "source": [
    "run_train(ctx=ctx)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
