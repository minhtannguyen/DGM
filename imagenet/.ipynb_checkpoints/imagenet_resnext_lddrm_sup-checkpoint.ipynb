{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import mxnet as mx\n",
    "from mxnet import image\n",
    "from mxnet import nd, gluon, autograd, init\n",
    "from mxnet.gluon.data.vision import ImageFolderDataset\n",
    "from mxnet.gluon.data import DataLoader\n",
    "from mxnet.gluon import nn\n",
    "from tensorboardX import SummaryWriter\n",
    "import numpy as np\n",
    "import shutil\n",
    "import _pickle as cPickle\n",
    "from sklearn import preprocessing\n",
    "from mxnet.gluon.parameter import Parameter, ParameterDict\n",
    "from common.util import download_file\n",
    "import subprocess\n",
    "import time\n",
    "\n",
    "from IPython.core.debugger import Tracer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpickle(file):\n",
    "    fo = open(file, 'rb')\n",
    "    dict = cPickle.load(fo, encoding='bytes')\n",
    "    fo.close()\n",
    "    return dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Options:\n",
    "    def __init__(self):\n",
    "        self.seed_val = 0\n",
    "        self.batch_size = 4\n",
    "        self.data_dir = '/tanData/datasets/imagenet/data'\n",
    "        self.log_dir = '/tanData/logs'\n",
    "        self.model_dir ='/tanData/models'\n",
    "        self.exp_name = 'imagenet_alllabels_resnext_lddrm_mm_pathnorm_seed_%i'%(self.seed_val)\n",
    "        self.gpus = 2\n",
    "        self.first_gpu = 1\n",
    "        \n",
    "opt = Options()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = opt.batch_size\n",
    "batch_size *= max(1, opt.gpus)\n",
    "ctx = [mx.gpu(i+opt.first_gpu) for i in range(opt.gpus)] if opt.gpus > 0 else [mx.cpu()]\n",
    "# ctx = [mx.gpu(1), mx.gpu(4)]\n",
    "kv = mx.kvstore.create('device')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gpu_device(ctx=mx.gpu(0)):\n",
    "    try:\n",
    "        _ = mx.nd.array([1, 2, 3], ctx=ctx)\n",
    "    except mx.MXNetError:\n",
    "        return None\n",
    "    return ctx\n",
    "\n",
    "for i, gpu in enumerate(ctx):\n",
    "    assert gpu_device(gpu), 'GPU device %i is not available!'%(i + self.first_gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = os.path.join(opt.log_dir, opt.exp_name)\n",
    "if not os.path.exists(log_dir):\n",
    "    os.makedirs(log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = mx.io.ImageRecordIter(\n",
    "    path_imgrec=os.path.join(opt.data_dir, 'train_unsup_480.rec'),\n",
    "    label_width=1,\n",
    "    data_name='data',\n",
    "    label_name='softmax_label',\n",
    "    data_shape=(3, 224, 224),\n",
    "    batch_size=batch_size,\n",
    "    mean_r=123.68,\n",
    "    mean_g=116.779,\n",
    "    mean_b=103.939,\n",
    "    pad=0,\n",
    "    fill_value=0,\n",
    "    shuffle=True,\n",
    "    rand_crop=True,\n",
    "    rand_mirror=True,\n",
    "    max_random_scale=1.0,\n",
    "    min_random_scale=0.533,\n",
    "    max_aspect_ratio=4.0/3.0,\n",
    "    min_aspect_ratio=3.0/4.0,\n",
    "    max_rotate_angle=10,\n",
    "    brightness=0.4,\n",
    "    contrast=0.4,\n",
    "    saturation=0.4,\n",
    "    random_h=9,\n",
    "    num_parts=kv.num_workers,\n",
    "    part_index=kv.rank,\n",
    "    preprocess_threads=32)\n",
    "\n",
    "valid_data = mx.io.ImageRecordIter(\n",
    "    path_imgrec=os.path.join(opt.data_dir,'val_256.rec'),\n",
    "    label_width=1,\n",
    "    data_name='data',\n",
    "    label_name='softmax_label',\n",
    "    batch_size=batch_size,\n",
    "    data_shape=(3, 224, 224),\n",
    "    rand_crop=False,\n",
    "    rand_mirror=False,\n",
    "    fill_value=0,\n",
    "    mean_r=123.68,\n",
    "    mean_g=116.779,\n",
    "    mean_b=103.939,\n",
    "    num_parts=kv.num_workers,\n",
    "    part_index=kv.rank,\n",
    "    preprocess_threads=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "L2_loss = gluon.loss.L2Loss()\n",
    "L1_loss = gluon.loss.L1Loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Normal(mx.init.Initializer):\n",
    "    \"\"\"Initializes weights with random values sampled from a normal distribution\n",
    "    with a mean and standard deviation of `sigma`.\n",
    "    \"\"\"\n",
    "    def __init__(self, mean=0, sigma=0.01):\n",
    "        super(Normal, self).__init__(sigma=sigma)\n",
    "        self.sigma = sigma\n",
    "        self.mean = mean\n",
    "\n",
    "    def _init_weight(self, _, arr):\n",
    "        mx.random.normal(self.mean, self.sigma, out=arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from resnet import ResNet164_v2\n",
    "from mxnet.gluon.model_zoo import vision as models\n",
    "from resnext_ld_opt import ResNext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_top1 = mx.metric.Accuracy()\n",
    "acc_top5 = mx.metric.TopKAccuracy(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "writer = SummaryWriter(os.path.join(opt.log_dir, opt.exp_name))\n",
    "\n",
    "# NEED TO BE FIXED\n",
    "def test(ctx, val_data):\n",
    "    acc_top1_val = mx.metric.Accuracy()\n",
    "    acc_top5_val = mx.metric.TopKAccuracy(5)\n",
    "    for i, batch in enumerate(val_data):\n",
    "        data = gluon.utils.split_and_load(batch.data[0], ctx_list=ctx, batch_axis=0)\n",
    "        label = gluon.utils.split_and_load(batch.label[0], ctx_list=ctx, batch_axis=0)\n",
    "        outputs = [net(X) for X in data]\n",
    "        acc_top1_val.update(label, outputs)\n",
    "        acc_top5_val.update(label, outputs)\n",
    "\n",
    "    _, top1 = acc_top1_val.get()\n",
    "    _, top5 = acc_top5_val.get()\n",
    "    return (1 - top1, 1 - top5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_rampup(current, rampup_length):\n",
    "    \"\"\"Linear rampup\"\"\"\n",
    "    assert current >= 0 and rampup_length >= 0\n",
    "    if current >= rampup_length:\n",
    "        return 1.0\n",
    "    else:\n",
    "        return current / rampup_length\n",
    "\n",
    "def cosine_rampdown(current, rampdown_length):\n",
    "    \"\"\"Cosine rampdown from https://arxiv.org/abs/1608.03983\"\"\"\n",
    "    assert 0 <= current <= rampdown_length\n",
    "    return float(.5 * (np.cos(np.pi * current / rampdown_length) + 1))\n",
    "\n",
    "def adjust_learning_rate(optimizer, epoch, step_in_epoch, total_steps_in_epoch):\n",
    "    lr = opt.lr\n",
    "    epoch = epoch + step_in_epoch / total_steps_in_epoch\n",
    "\n",
    "    # LR warm-up to handle large minibatch sizes from https://arxiv.org/abs/1706.02677\n",
    "    lr = linear_rampup(epoch, opt.lr_rampup) * (opt.lr - opt.initial_lr) + opt.initial_lr\n",
    "\n",
    "    # Cosine LR rampdown from https://arxiv.org/abs/1608.03983 (but one cycle only)\n",
    "    if opt.lr_rampdown_epochs:\n",
    "        lr *= cosine_rampdown(epoch, opt.lr_rampdown_epochs)\n",
    "\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, train_data, valid_data, num_epochs, lr, wd, ctx, lr_decay):\n",
    "    trainer = gluon.Trainer(\n",
    "        net.collect_params(), 'adam', {'learning_rate': 0.0001, 'wd': wd})\n",
    "    \n",
    "    prev_time = datetime.datetime.now()\n",
    "    best_top1_err = np.inf\n",
    "    best_top5_err = np.inf\n",
    "    log_interval = 1000\n",
    "    \n",
    "#     # Learning rate decay factor\n",
    "#     lr_decay = 0.1\n",
    "#     # Epochs where learning rate decays\n",
    "#     lr_decay_epoch = [30, 60, 90, np.inf]\n",
    "\n",
    "    # Nesterov accelerated gradient descent\n",
    "    optimizer = 'nag'\n",
    "    # Set parameters\n",
    "    optimizer_params = {'learning_rate': 0.1, 'wd': 5e-5, 'momentum': 0.9}\n",
    "\n",
    "    # Define our trainer for net\n",
    "    trainer = gluon.Trainer(net.collect_params(), optimizer, optimizer_params)\n",
    "    \n",
    "    for epoch in range(num_epochs-100):\n",
    "        train_data.reset()\n",
    "        \n",
    "        tic = time.time()\n",
    "        btic = time.time()\n",
    "        acc_top1.reset()\n",
    "        acc_top5.reset()\n",
    "        train_loss = 0\n",
    "        num_batch = 0\n",
    "        \n",
    "#         if epoch == 20:\n",
    "#             sgd_lr = 0.15\n",
    "#             decay_val = np.exp(np.log(sgd_lr / 0.0001) / (num_epochs - 2))\n",
    "#             sgd_lr = sgd_lr * decay_val\n",
    "#             trainer = gluon.Trainer(net.collect_params(), 'SGD', {'learning_rate': sgd_lr, 'wd': wd})\n",
    "            \n",
    "#         if epoch >= 20:\n",
    "#             trainer.set_learning_rate(trainer.learning_rate / decay_val)\n",
    "        \n",
    "        for i, batch_sup in enumerate(train_data):\n",
    "            bs = batch_sup.data[0].shape[0]\n",
    "            \n",
    "            data_sup = gluon.utils.split_and_load(batch_sup.data[0], ctx_list=ctx, batch_axis=0)\n",
    "            label_sup = gluon.utils.split_and_load(batch_sup.label[0], ctx_list=ctx, batch_axis=0)\n",
    "            \n",
    "            Tracer()()\n",
    "            \n",
    "            loss = []\n",
    "            outputs_sup = []\n",
    "            \n",
    "            with autograd.record():\n",
    "                for xsup, ysup in zip(data_sup, label_sup):\n",
    "                    output_sup, xhat_sup, loss_mm_sup, rpn = net(xsup, ysup)\n",
    "                    loss_xentropy_sup = criterion(output_sup, ysup)\n",
    "                    loss_drm_sup = L2_loss(xhat_sup, xsup)\n",
    "                    loss_sup = loss_xentropy_sup + loss_drm_sup + loss_mm_sup + rpn\n",
    "                    Tracer()()\n",
    "                    \n",
    "                    loss.append(loss_sup)\n",
    "                    outputs_sup.append(output_sup)\n",
    "                    \n",
    "            for l in loss:\n",
    "                l.backward()\n",
    "                \n",
    "            trainer.step(bs)\n",
    "            \n",
    "            Tracer()()\n",
    "            \n",
    "            acc_top1.update(label_sup, outputs_sup)\n",
    "            acc_top5.update(label_sup, outputs_sup)\n",
    "            train_loss += sum([l.sum().asscalar() for l in loss])\n",
    "            num_batch += 1\n",
    "            if log_interval and not (i + 1) % log_interval:\n",
    "                _, top1 = acc_top1.get()\n",
    "                _, top5 = acc_top5.get()\n",
    "                err_top1, err_top5 = (1-top1, 1-top5)\n",
    "                print('Epoch[%d] Batch [%d]     Speed: %f samples/sec   top1-err=%f     top5-err=%f'%(\n",
    "                          epoch, i, batch_size*log_interval/(time.time()-btic), err_top1, err_top5))\n",
    "                btic = time.time()\n",
    "        \n",
    "        _, top1 = acc_top1.get()\n",
    "        _, top5 = acc_top5.get()\n",
    "        err_top1, err_top5 = (1-top1, 1-top5)\n",
    "        train_loss /= num_batch * batch_size\n",
    "        writer.add_scalars('acc', {'train_top1': 1.0 - err_top1}, epoch)\n",
    "        writer.add_scalars('acc', {'train_top5': 1.0 - err_top5}, epoch)\n",
    "        \n",
    "        err_top1_val, err_top5_val = test(ctx, valid_data)\n",
    "        \n",
    "        if err_top1_val < best_top1_err:\n",
    "            best_top1_err = err_top1_val\n",
    "            net.collect_params().save('%s/%s_best_top1.params'%(opt.model_dir, opt.exp_name))\n",
    "        \n",
    "        if err_top5_val < best_top5_err:\n",
    "            best_top5_err = err_top5_val\n",
    "            net.collect_params().save('%s/%s_best_top5.params'%(opt.model_dir, opt.exp_name))\n",
    "        \n",
    "        print('[Epoch %d] training: err-top1=%f err-top5=%f loss=%f'%(epoch, err_top1, err_top5, train_loss))\n",
    "        print('[Epoch %d] time cost: %f'%(epoch, time.time()-tic))\n",
    "        print('[Epoch %d] validation: err-top1=%f err-top5=%f'%(epoch, err_top1_val, err_top5_val))\n",
    "        \n",
    "        writer.add_scalars('acc', {'valid_top1': 1.0 - err_top1_val}, epoch)\n",
    "        writer.add_scalars('acc', {'valid_top5': 1.0 - err_top5_val}, epoch)\n",
    "        \n",
    "        if not epoch % 50:\n",
    "            net.collect_params().save('%s/%s_epoch_%i.params'%(opt.model_dir, opt.exp_name, epoch))\n",
    "    \n",
    "    return best_top1_err, best_top5_err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 500\n",
    "learning_rate = 0.15\n",
    "weight_decay = 5e-4\n",
    "lr_decay = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_train(num_exp, ctx):\n",
    "    valid_acc = 0\n",
    "    for i in range(num_exp):\n",
    "        ### CIFAR VGG_DRM    \n",
    "        model = ResNext('resnext50', cardinality=32, bottleneck_width=4, classes=1000)\n",
    "        for param in model.collect_params().values():\n",
    "            if param.name.find('conv') != -1 or param.name.find('dense') != -1:\n",
    "                if param.name.find('weight') != -1:\n",
    "                    param.initialize(init=mx.initializer.Xavier(), ctx=ctx)\n",
    "                else:\n",
    "                    param.initialize(init=mx.init.Zero(), ctx=ctx)\n",
    "            elif param.name.find('batchnorm') != -1 or param.name.find('instancenorm') != -1:\n",
    "                if param.name.find('gamma') != -1:\n",
    "                    param.initialize(init=Normal(mean=1, sigma=0.02), ctx=ctx)\n",
    "                else:\n",
    "                    param.initialize(init=mx.init.Zero(), ctx=ctx)\n",
    "            elif param.name.find('biasadder') != -1:\n",
    "                param.initialize(init=mx.init.Zero(), ctx=ctx)\n",
    "            else:\n",
    "                param.initialize(init=mx.initializer.Xavier(), ctx=ctx)\n",
    "                \n",
    "        # model.hybridize()\n",
    "                \n",
    "        best_top1_err, best_top5_err = train(model, train_data, valid_data, num_epochs, learning_rate, weight_decay, ctx, lr_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:48: DeprecationWarning: `Tracer` is deprecated since version 5.1, directly use `IPython.core.debugger.Pdb.set_trace()`\n",
      "/usr/local/lib/python3.5/dist-packages/IPython/core/debugger.py:168: DeprecationWarning: The `color_scheme` argument is deprecated since version 5.1\n",
      "  self.debugger = Pdb(colors)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m<ipython-input-14-c967782ad243>\u001b[0m(50)\u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     48 \u001b[0;31m            \u001b[0mTracer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     49 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 50 \u001b[0;31m            \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     51 \u001b[0;31m            \u001b[0moutputs_sup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     52 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "(8, 3, 224, 224)\n",
      "Exiting Debugger.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py:1736: DeprecationWarning: `BdbQuit_IPython_excepthook` is deprecated since version 5.1\n",
      "  stb = handler(self,etype,value,tb,tb_offset=tb_offset)\n"
     ]
    }
   ],
   "source": [
    "run_train(1, ctx=ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
