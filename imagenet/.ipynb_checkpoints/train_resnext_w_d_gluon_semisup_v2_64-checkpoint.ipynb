{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse,logging, os, math\n",
    "import mxnet as mx\n",
    "from mxnet import image\n",
    "from mxnet import nd, gluon, autograd, init\n",
    "from mxnet.gluon.data.vision import ImageFolderDataset\n",
    "from mxnet.gluon.data import DataLoader\n",
    "from mxnet.gluon import nn\n",
    "from tensorboardX import SummaryWriter\n",
    "import numpy as np\n",
    "import shutil\n",
    "import _pickle as cPickle\n",
    "from mxnet.gluon.parameter import Parameter, ParameterDict\n",
    "import subprocess\n",
    "import time\n",
    "from mxnet.gluon.loss import Loss, _apply_weighting, _reshape_like\n",
    "\n",
    "from IPython.core.debugger import Tracer\n",
    "\n",
    "from resnext_w_d_maxmin_v2_64 import resnext\n",
    "\n",
    "# import os\n",
    "# os.environ['MXNET_GPU_MEM_POOL_TYPE'] = 'Round'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "formatter = logging.Formatter('%(asctime)s - %(message)s')\n",
    "console = logging.StreamHandler()\n",
    "console.setFormatter(formatter)\n",
    "logger.addHandler(console)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Options:\n",
    "    def __init__(self):\n",
    "        self.gpus = '0,1,2,3,4,5,6,7' #the gpus will be used, e.g \"0,1,2,3\"\n",
    "        self.data_dir = '/tanData/datasets/imagenet/data' #the input data directory\n",
    "        self.log_dir = '/tanData/logs'\n",
    "        self.model_dir ='/tanData/models'\n",
    "        self.exp_name = 'semisup_exp1_128K'\n",
    "        self.data_type = 'imagenet' #the dataset type\n",
    "        self.depth = 50 #the depth of resnet\n",
    "        self.batch_size = 128 #the batch size\n",
    "        self.num_group = 64 #the number of convolution groups\n",
    "        self.drop_out = 0.0 #the probability of an element to be zeroed\n",
    "        self.alpha_max = 0.5\n",
    "        self.alpha_min = 0.5\n",
    "        self.alpha_drm = 0.5\n",
    "        self.alpha_rpn = 1.0\n",
    "        self.alpha_kl = 0.5\n",
    "        self.alpha_mm = 0.5\n",
    "        self.alpha_min = 0.5\n",
    "        self.alpha_max = 1.0 - self.alpha_min\n",
    "        \n",
    "        self.list_dir = './' #the directory which contain the training list file\n",
    "        self.lr = 0.1 #initialization learning rate\n",
    "        self.mom = 0.9 #momentum for sgd\n",
    "        self.bn_mom = 0.9 #momentum for batch normlization\n",
    "        self.wd = 0.0001 #weight decay for sgd\n",
    "        self.workspace = 512 #memory space size(MB) used in convolution, \n",
    "                            #if xpu memory is oom, then you can try smaller vale, such as --workspace 256 \n",
    "        self.num_classes = 1000 #the class number of your task\n",
    "        self.aug_level = 1 # level 1: use only random crop and random mirror, \n",
    "                           #level 2: add scale/aspect/hsv augmentation based on level 1, \n",
    "                           #level 3: add rotation/shear augmentation based on level 2 \n",
    "        self.num_examples = 1152000 # the number of training examples\n",
    "        self.kv_store = 'device' # the kvstore type'\n",
    "        self.model_load_epoch = 0 # load the model on an epoch using the model-load-prefix\n",
    "        self.frequent = 50 # frequency of logging\n",
    "        self.memonger = False # true means using memonger to save momory, https://github.com/dmlc/mxnet-memonger\n",
    "        self.retrain = False # true means continue training\n",
    "        \n",
    "args = Options()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-09-24 09:28:55,155 - <__main__.Options object at 0x7f9c0018d780>\n"
     ]
    }
   ],
   "source": [
    "hdlr = logging.FileHandler('./log/log-semi128Kresnext-{}-{}.log'.format(args.data_type, args.depth))\n",
    "hdlr.setFormatter(formatter)\n",
    "logger.addHandler(hdlr)\n",
    "logging.info(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "kv = mx.kvstore.create(args.kv_store)\n",
    "ctx = mx.cpu() if args.gpus is None else [mx.gpu(int(i)) for i in args.gpus.split(',')]\n",
    "batch_size = args.batch_size\n",
    "batch_size *= max(1, len(ctx))\n",
    "begin_epoch = args.model_load_epoch if args.model_load_epoch else 0\n",
    "if not os.path.exists(\"./model\"):\n",
    "    os.mkdir(\"./model\")\n",
    "model_prefix = \"semi128Kresnext_{}_{}_{}_{}\".format(args.data_type, args.depth, kv.rank, args.exp_name)\n",
    "# model_prefix = \"model/se-resnext-{}-{}-{}\".format(args.data_type, args.depth, kv.rank)\n",
    "arg_params = None\n",
    "aux_params = None\n",
    "if args.retrain:\n",
    "    _, arg_params, aux_params = mx.model.load_checkpoint(model_prefix, args.model_load_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_sup = mx.io.ImageRecordIter(\n",
    "    path_imgrec         = os.path.join(args.data_dir, \"train.rec\") if args.data_type == 'cifar10' else\n",
    "                          os.path.join(args.data_dir, \"train_sup_64.rec\") if args.aug_level == 1\n",
    "                          else os.path.join(args.data_dir, \"train_sup_64.rec\") ,\n",
    "    label_width         = 1,\n",
    "    data_name           = 'data',\n",
    "    label_name          = 'softmax_label',\n",
    "    data_shape          = (3, 32, 32) if args.data_type==\"cifar10\" else (3, 64, 64),\n",
    "    batch_size          = batch_size // 2,\n",
    "    pad                 = 4 if args.data_type == \"cifar10\" else 0,\n",
    "    fill_value          = 127,  # only used when pad is valid\n",
    "    rand_crop           = True,\n",
    "    max_random_scale    = 1.0,  # 480 with imagnet, 32 with cifar10\n",
    "    min_random_scale    = 1.0 if args.data_type == \"cifar10\" else 1.0 if args.aug_level == 1 else 0.533,  # 256.0/480.0=0.533, 256.0/384.0=0.667 256.0/256=1.0\n",
    "    max_aspect_ratio    = 0 if args.data_type == \"cifar10\" else 0 if args.aug_level == 1 else 0.25, # 0.25\n",
    "    random_h            = 0 if args.data_type == \"cifar10\" else 0 if args.aug_level == 1 else 36,  # 0.4*90\n",
    "    random_s            = 0 if args.data_type == \"cifar10\" else 0 if args.aug_level == 1 else 50,  # 0.4*127\n",
    "    random_l            = 0 if args.data_type == \"cifar10\" else 0 if args.aug_level == 1 else 50,  # 0.4*127\n",
    "    max_rotate_angle    = 0 if args.aug_level <= 2 else 10,\n",
    "    max_shear_ratio     = 0 if args.aug_level <= 2 else 0.0, #0.1 args.aug_level = 3\n",
    "    rand_mirror         = True,\n",
    "    shuffle             = True,\n",
    "    num_parts           = kv.num_workers,\n",
    "    part_index          = kv.rank,\n",
    "    mean_r=123.68,\n",
    "    mean_g=116.779,\n",
    "    mean_b=103.939,\n",
    "    std_r=58.395,\n",
    "    std_g=57.12,\n",
    "    std_b=57.375)\n",
    "\n",
    "train_data_unsup = mx.io.ImageRecordIter(\n",
    "    path_imgrec         = os.path.join(args.data_dir, \"train.rec\") if args.data_type == 'cifar10' else\n",
    "                          os.path.join(args.data_dir, \"train_unsup_64.rec\") if args.aug_level == 1\n",
    "                          else os.path.join(args.data_dir, \"train_unsup_64.rec\") ,\n",
    "    label_width         = 1,\n",
    "    data_name           = 'data',\n",
    "    label_name          = 'softmax_label',\n",
    "    data_shape          = (3, 32, 32) if args.data_type==\"cifar10\" else (3, 64, 64),\n",
    "    batch_size          = batch_size // 2,\n",
    "    pad                 = 4 if args.data_type == \"cifar10\" else 0,\n",
    "    fill_value          = 127,  # only used when pad is valid\n",
    "    rand_crop           = True,\n",
    "    max_random_scale    = 1.0,  # 480 with imagnet, 32 with cifar10\n",
    "    min_random_scale    = 1.0 if args.data_type == \"cifar10\" else 1.0 if args.aug_level == 1 else 0.533,  # 256.0/480.0=0.533, 256.0/384.0=0.667 256.0/256=1.0\n",
    "    max_aspect_ratio    = 0 if args.data_type == \"cifar10\" else 0 if args.aug_level == 1 else 0.25, # 0.25\n",
    "    random_h            = 0 if args.data_type == \"cifar10\" else 0 if args.aug_level == 1 else 36,  # 0.4*90\n",
    "    random_s            = 0 if args.data_type == \"cifar10\" else 0 if args.aug_level == 1 else 50,  # 0.4*127\n",
    "    random_l            = 0 if args.data_type == \"cifar10\" else 0 if args.aug_level == 1 else 50,  # 0.4*127\n",
    "    max_rotate_angle    = 0 if args.aug_level <= 2 else 10,\n",
    "    max_shear_ratio     = 0 if args.aug_level <= 2 else 0.0, #0.1 args.aug_level = 3\n",
    "    rand_mirror         = True,\n",
    "    shuffle             = True,\n",
    "    num_parts           = kv.num_workers,\n",
    "    part_index          = kv.rank,\n",
    "    mean_r=123.68,\n",
    "    mean_g=116.779,\n",
    "    mean_b=103.939,\n",
    "    std_r=58.395,\n",
    "    std_g=57.12,\n",
    "    std_b=57.375)\n",
    "\n",
    "val_data = mx.io.ImageRecordIter(\n",
    "    path_imgrec         = os.path.join(args.data_dir, \"val.rec\") if args.data_type == 'cifar10' else\n",
    "                          os.path.join(args.data_dir, \"val_64.rec\"),\n",
    "    label_width         = 1,\n",
    "    data_name           = 'data',\n",
    "    label_name          = 'softmax_label',\n",
    "    batch_size          = batch_size,\n",
    "    data_shape          = (3, 32, 32) if args.data_type==\"cifar10\" else (3, 64, 64),\n",
    "    rand_crop           = False,\n",
    "    rand_mirror         = False,\n",
    "    num_parts           = kv.num_workers,\n",
    "    part_index          = kv.rank,\n",
    "    mean_r=123.68,\n",
    "    mean_g=116.779,\n",
    "    mean_b=103.939,\n",
    "    std_r=58.395,\n",
    "    std_g=57.12,\n",
    "    std_b=57.375)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Normal(mx.init.Initializer):\n",
    "    \"\"\"Initializes weights with random values sampled from a normal distribution\n",
    "    with a mean and standard deviation of `sigma`.\n",
    "    \"\"\"\n",
    "    def __init__(self, mean=0, sigma=0.01):\n",
    "        super(Normal, self).__init__(sigma=sigma)\n",
    "        self.sigma = sigma\n",
    "        self.mean = mean\n",
    "\n",
    "    def _init_weight(self, _, arr):\n",
    "        mx.random.normal(self.mean, self.sigma, out=arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_factor_scheduler(begin_epoch, epoch_size, step=[30, 60, 90, 95, 110, 120], factor=0.1):\n",
    "    step_ = [epoch_size * (x-begin_epoch) for x in step if x-begin_epoch > 0]\n",
    "    return mx.lr_scheduler.MultiFactorScheduler(step=step_, factor=factor) if len(step_) else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "L2_loss = gluon.loss.L2Loss()\n",
    "\n",
    "acc_top1 = mx.metric.Accuracy()\n",
    "acc_top5 = mx.metric.TopKAccuracy(5)\n",
    "import datetime\n",
    "writer = SummaryWriter(os.path.join(args.log_dir, args.exp_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(net, val_data, ctx):\n",
    "    val_data.reset()\n",
    "    \n",
    "    acc_top1_val = mx.metric.Accuracy()\n",
    "    acc_top5_val = mx.metric.TopKAccuracy(5)\n",
    "    acc_top1_val_max = mx.metric.Accuracy()\n",
    "    acc_top5_val_max = mx.metric.TopKAccuracy(5)\n",
    "    acc_top1_val_min = mx.metric.Accuracy()\n",
    "    acc_top5_val_min = mx.metric.TopKAccuracy(5)\n",
    "    acc_top1_val.reset()\n",
    "    acc_top5_val.reset()\n",
    "    acc_top1_val_max.reset()\n",
    "    acc_top5_val_max.reset()\n",
    "    acc_top1_val_min.reset()\n",
    "    acc_top5_val_min.reset()\n",
    "    for i, batch in enumerate(val_data):\n",
    "        data = gluon.utils.split_and_load(batch.data[0], ctx_list=ctx, batch_axis=0)\n",
    "        label = gluon.utils.split_and_load(batch.label[0], ctx_list=ctx, batch_axis=0)\n",
    "        \n",
    "        outputs = []\n",
    "        outputsmax = []\n",
    "        outputsmin = []\n",
    "        for x, y in zip(data, label):\n",
    "            cond = nd.zeros_like(y)\n",
    "            zmax, zmin, _, _, _, _ = net(x, y, cond)\n",
    "            z = args.alpha_max * zmax + args.alpha_min * zmin\n",
    "            outputs.append(z)\n",
    "            outputsmax.append(zmax)\n",
    "            outputsmin.append(zmin)\n",
    "            \n",
    "            \n",
    "        acc_top1_val.update(label, outputs)\n",
    "        acc_top5_val.update(label, outputs)\n",
    "        acc_top1_val_max.update(label, outputsmax)\n",
    "        acc_top5_val_max.update(label, outputsmax)\n",
    "        acc_top1_val_min.update(label, outputsmin)\n",
    "        acc_top5_val_min.update(label, outputsmin)\n",
    "        \n",
    "        del data, label, z, zmax, zmin, x, y, cond\n",
    "\n",
    "    _, top1 = acc_top1_val.get()\n",
    "    _, top5 = acc_top5_val.get()\n",
    "    _, top1max = acc_top1_val_max.get()\n",
    "    _, top5max = acc_top5_val_max.get()\n",
    "    _, top1min = acc_top1_val_min.get()\n",
    "    _, top5min = acc_top5_val_min.get()\n",
    "    return (top1, top5, top1max, top5max, top1min, top5min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, train_data_sup, train_data_unsup, val_data, num_epochs, ctx):\n",
    "    epoch_size = max(int(args.num_examples / (batch_size//2) / kv.num_workers), 1)\n",
    "    # lr_sch = multi_factor_scheduler(begin_epoch, epoch_size, step=[15, 30], factor=0.1)\n",
    "    # trainer = gluon.Trainer(net.collect_params(), 'nag', {'learning_rate':args.lr, 'momentum':args.mom, 'wd':args.wd, 'lr_scheduler': lr_sch, 'multi_precision': True})\n",
    "    \n",
    "    trainer = gluon.Trainer(\n",
    "        net.collect_params(), 'adam', {'learning_rate': 0.001, 'wd': args.wd})\n",
    "    prev_time = datetime.datetime.now()\n",
    "    best_top1_val = 0.; best_top1_valmax = 0.; best_top1_valmin = 0.\n",
    "    best_top5_val = 0.; best_top5_valmax = 0.; best_top5_valmin = 0.\n",
    "    log_interval = 5\n",
    "    \n",
    "    for epoch in range(begin_epoch, num_epochs):\n",
    "        train_data_sup.reset()\n",
    "        train_data_unsup.reset()\n",
    "        \n",
    "        if epoch == 10 or epoch == 20:\n",
    "            trainer.set_learning_rate(0.1 * trainer.learning_rate)\n",
    "        \n",
    "        tic = time.time()\n",
    "        btic = time.time()\n",
    "        acc_top1.reset()\n",
    "        acc_top5.reset()\n",
    "        train_loss = 0\n",
    "        num_batch = 0\n",
    "        \n",
    "        for i, (batch_sup, batch_unsup) in enumerate(zip(train_data_sup, train_data_unsup)):\n",
    "            bs = batch_unsup.data[0].shape[0]\n",
    "            \n",
    "            data_sup = gluon.utils.split_and_load(batch_sup.data[0], ctx_list=ctx, batch_axis=0)\n",
    "            label_sup = gluon.utils.split_and_load(batch_sup.label[0], ctx_list=ctx, batch_axis=0)\n",
    "            data_unsup = gluon.utils.split_and_load(batch_unsup.data[0], ctx_list=ctx, batch_axis=0)\n",
    "            label_unsup = gluon.utils.split_and_load(batch_unsup.label[0], ctx_list=ctx, batch_axis=0)\n",
    "            \n",
    "            loss = []\n",
    "            outputs = []\n",
    "            \n",
    "            with autograd.record():\n",
    "                for x_sup, x_unsup, y_sup, y_unsup in zip(data_sup, data_unsup, label_sup, label_unsup):\n",
    "                    cond_sup = nd.ones_like(y_sup)\n",
    "                    zmax_sup, zmin_sup, xhatmax_sup, xhatmin_sup, loss_mmmax_sup, loss_mmmin_sup = net(x_sup, y_sup, cond_sup)\n",
    "                    loss_xent_sup = args.alpha_max * criterion(zmax_sup, y_sup) + args.alpha_min * criterion(zmin_sup, y_sup)\n",
    "                    loss_drm_sup = args.alpha_max * L2_loss(xhatmax_sup, x_sup) + args.alpha_min * L2_loss(xhatmin_sup, x_sup)\n",
    "                    softmax_scores_sup = args.alpha_max * nd.softmax(zmax_sup) + args.alpha_min * nd.softmax(zmin_sup)\n",
    "                    loss_kl_sup = -nd.sum(nd.log(1000.0*softmax_scores_sup + 1e-8) * softmax_scores_sup, axis=1)\n",
    "                    loss_mm_sup = args.alpha_max * loss_mmmax_sup + args.alpha_min * loss_mmmin_sup\n",
    "                    loss_total_sup = loss_xent_sup + args.alpha_drm * loss_drm_sup + args.alpha_kl * loss_kl_sup + args.alpha_mm * loss_mm_sup\n",
    "                    \n",
    "                    cond_unsup = nd.zeros_like(y_unsup)\n",
    "                    zmax_unsup, zmin_unsup, xhatmax_unsup, xhatmin_unsup, loss_mmmax_unsup, loss_mmmin_unsup = net(x_unsup, y_unsup, cond_unsup)\n",
    "                    loss_drm_unsup = args.alpha_max * L2_loss(xhatmax_unsup, x_unsup) + args.alpha_min * L2_loss(xhatmin_unsup, x_unsup)\n",
    "                    softmax_scores_unsup = args.alpha_max * nd.softmax(zmax_unsup) + args.alpha_min * nd.softmax(zmin_unsup)\n",
    "                    loss_kl_unsup = -nd.sum(nd.log(1000.0*softmax_scores_unsup + 1e-8) * softmax_scores_unsup, axis=1)\n",
    "                    loss_mm_unsup = args.alpha_max * loss_mmmax_unsup + args.alpha_min * loss_mmmin_unsup\n",
    "                    loss_total_unsup = args.alpha_drm * loss_drm_unsup + args.alpha_kl * loss_kl_unsup + args.alpha_mm * loss_mm_unsup\n",
    "                    \n",
    "                    z_sup = args.alpha_max * zmax_sup + args.alpha_min * zmin_sup\n",
    "\n",
    "                    loss.append(loss_total_sup + loss_total_unsup)\n",
    "                    outputs.append(z_sup)\n",
    "                    \n",
    "#                     logging.info('Loss sup=%f'%loss_total_sup.sum().asscalar())\n",
    "#                     logging.info('Loss unsup=%f'%loss_total_unsup.sum().asscalar())\n",
    "#                     logging.info('Loss xent sup=%f'%loss_xent_sup.sum().asscalar())\n",
    "#                     logging.info('loss_drm_sup=%f'%loss_drm_sup.sum().asscalar())\n",
    "#                     logging.info('loss_drm_unsup=%f'%loss_drm_unsup.sum().asscalar())\n",
    "#                     logging.info('loss_kl_sup=%f'%loss_kl_sup.sum().asscalar())\n",
    "#                     logging.info('loss_kl_unsup=%f'%loss_kl_unsup.sum().asscalar())\n",
    "#                     logging.info('loss_mm_sup=%f'%loss_mm_sup.sum().asscalar())\n",
    "#                     logging.info('loss_mm_unsup=%f'%loss_mm_unsup.sum().asscalar())\n",
    "#                     logging.info('rpn_sup=%f'%rpn_sup.sum().asscalar())\n",
    "#                     logging.info('rpn_unsup=%f'%rpn_unsup.sum().asscalar())\n",
    "                    \n",
    "#                     logging.info('xhatmax_sup: mean=%f max=%f min=%f'%(xhatmax_sup.mean().asscalar(), xhatmax_sup.max().asscalar(), xhatmax_sup.min().asscalar()))\n",
    "#                     logging.info('xhatmax_unsup: mean=%f max=%f min=%f'%(xhatmax_unsup.mean().asscalar(), xhatmax_unsup.max().asscalar(), xhatmax_unsup.min().asscalar()))\n",
    "#                     logging.info('xhatmin_sup: mean=%f max=%f min=%f'%(xhatmin_sup.mean().asscalar(), xhatmin_sup.max().asscalar(), xhatmin_sup.min().asscalar()))\n",
    "#                     logging.info('xhatmin_unsup: mean=%f max=%f min=%f'%(xhatmin_unsup.mean().asscalar(), xhatmin_unsup.max().asscalar(), xhatmin_unsup.min().asscalar()))\n",
    "                    \n",
    "#                     logging.info('xsup: mean=%f max=%f min=%f'%(x_sup.mean(axis=3).mean(axis=2).mean(axis=1).mean().asscalar(), x_sup.max().asscalar(), x_sup.min().asscalar()))\n",
    "#                     logging.info('x_unsup: mean=%f max=%f min=%f'%(x_unsup.mean(axis=3).mean(axis=2).mean(axis=1).mean().asscalar(), x_unsup.max().asscalar(), x_unsup.min().asscalar()))\n",
    "                                  \n",
    "            for l in loss:\n",
    "                l.backward()\n",
    "                # logging.info('Loss =%f'%l.sum().asscalar())\n",
    "            \n",
    "#             for param in net.collect_params().values():\n",
    "#                 if param.grad_req != 'null':\n",
    "#                     logging.info(param.name)\n",
    "#                     for grad in param.list_grad():\n",
    "#                         logging.info(np.any(np.isnan(grad.asnumpy())))\n",
    "            \n",
    "            trainer.step(bs)\n",
    "            \n",
    "            acc_top1.update(label_sup, outputs)\n",
    "            acc_top5.update(label_sup, outputs)\n",
    "            train_loss += sum([l.sum().asscalar() for l in loss])\n",
    "            num_batch += 1\n",
    "            if log_interval and not i % log_interval:\n",
    "                _, top1 = acc_top1.get()\n",
    "                _, top5 = acc_top5.get()\n",
    "                logging.info('Epoch[%d] Batch [%d] Lr: %f Speed: %f samples/sec   loss=%f   top1-acc=%f     top5-acc=%f'%(\n",
    "                          epoch, i, trainer.learning_rate, batch_size*log_interval/(time.time()-btic), train_loss/(num_batch * batch_size), top1, top5))\n",
    "                btic = time.time()\n",
    "            \n",
    "            del data_sup, label_sup, data_unsup, label_unsup, loss, outputs, \n",
    "            del x_sup, x_unsup, y_sup, y_unsup, cond_sup, cond_unsup\n",
    "            del zmax_sup, zmin_sup, xhatmax_sup, xhatmin_sup, loss_mmmax_sup, loss_mmmin_sup\n",
    "            del loss_xent_sup, loss_drm_sup, softmax_scores_sup, loss_kl_sup, loss_mm_sup, loss_total_sup, z_sup\n",
    "            del zmax_unsup, zmin_unsup, xhatmax_unsup, xhatmin_unsup, loss_mmmax_unsup, loss_mmmin_unsup\n",
    "            del loss_drm_unsup, softmax_scores_unsup, loss_kl_unsup, loss_mm_unsup, loss_total_unsup\n",
    "        \n",
    "        _, top1 = acc_top1.get()\n",
    "        _, top5 = acc_top5.get()\n",
    "        train_loss /= num_batch * batch_size\n",
    "        writer.add_scalars('acc', {'train_top1': top1}, epoch)\n",
    "        writer.add_scalars('acc', {'train_top5': top5}, epoch)\n",
    "        \n",
    "        top1_val, top5_val, top1_valmax, top5_valmax, top1_valmin, top5_valmin = test(net=net, val_data=val_data, ctx=ctx)\n",
    "        \n",
    "        if top1_val > best_top1_val:\n",
    "            best_top1_val = top1_val\n",
    "            net.collect_params().save('%s/%s_best_top1.params'%(args.model_dir, model_prefix))\n",
    "        \n",
    "        if top1_valmax > best_top1_valmax:\n",
    "            best_top1_valmax = top1_valmax\n",
    "            net.collect_params().save('%s/%s_best_top1_max.params'%(args.model_dir, model_prefix))\n",
    "            \n",
    "        if top1_valmin > best_top1_valmin:\n",
    "            best_top1_valmin = top1_valmin\n",
    "            net.collect_params().save('%s/%s_best_top1_min.params'%(args.model_dir, model_prefix))\n",
    "        \n",
    "        if top5_val > best_top5_val:\n",
    "            best_top5_val = top5_val\n",
    "            net.collect_params().save('%s/%s_best_top5.params'%(args.model_dir, model_prefix))\n",
    "        \n",
    "        if top5_valmax > best_top5_valmax:\n",
    "            best_top5_valmax = top5_valmax\n",
    "            net.collect_params().save('%s/%s_best_top5_max.params'%(args.model_dir, model_prefix))\n",
    "        \n",
    "        if top5_valmin > best_top5_valmin:\n",
    "            best_top5_valmin = top5_valmin\n",
    "            net.collect_params().save('%s/%s_best_top5_min.params'%(args.model_dir, model_prefix))\n",
    "        \n",
    "        logging.info('[Epoch %d] training: acc-top1=%f acc-top5=%f loss=%f lr=%f'%(epoch, top1, top5, train_loss, trainer.learning_rate))\n",
    "        logging.info('[Epoch %d] time cost: %f'%(epoch, time.time()-tic))\n",
    "        logging.info('[Epoch %d] validation: acc-top1=%f acc-top5=%f best-acc-top1=%f best-acc-top5=%f'%(epoch, top1_val, top5_val, best_top1_val, best_top5_val))\n",
    "        logging.info('[Epoch %d] validation: acc-top1-max=%f acc-top5-max=%f best-acc-top1-max=%f best-acc-top5-max=%f'%(epoch, top1_valmax, top5_valmax, best_top1_valmax, best_top5_valmax))\n",
    "        logging.info('[Epoch %d] validation: acc-top1-min=%f acc-top5-min=%f best-acc-top1-min=%f best-acc-top5-min=%f'%(epoch, top1_valmin, top5_valmin, best_top1_valmin, best_top5_valmin))\n",
    "        \n",
    "        writer.add_scalars('acc', {'valid_top1': top1_val}, epoch)\n",
    "        writer.add_scalars('acc', {'valid_top5': top5_val}, epoch)\n",
    "        writer.add_scalars('acc', {'valid_top1_max': top1_valmax}, epoch)\n",
    "        writer.add_scalars('acc', {'valid_top5_max': top5_valmax}, epoch)\n",
    "        writer.add_scalars('acc', {'valid_top1_min': top1_valmin}, epoch)\n",
    "        writer.add_scalars('acc', {'valid_top5_min': top5_valmin}, epoch)\n",
    "        \n",
    "        net.collect_params().save('%s/%s_current.params'%(args.model_dir, model_prefix))\n",
    "        if not epoch % 10:\n",
    "            net.collect_params().save('%s/%s_epoch_%i.params'%(args.model_dir, model_prefix, epoch))\n",
    "    \n",
    "    return best_top1_val, best_top5_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio_list = [0.25, 0.125, 0.0625, 0.03125]   # 1/4, 1/8, 1/16, 1/32\n",
    "if args.depth == 18:\n",
    "    units = [2, 2, 2, 2]\n",
    "elif args.depth == 34:\n",
    "    units = [3, 4, 6, 3]\n",
    "elif args.depth == 50:\n",
    "    units = [3, 4, 6, 3]\n",
    "elif args.depth == 101:\n",
    "    units = [3, 4, 23, 3]\n",
    "elif args.depth == 152:\n",
    "    units = [3, 8, 36, 3]\n",
    "elif args.depth == 200:\n",
    "    units = [3, 24, 36, 3]\n",
    "elif args.depth == 269:\n",
    "    units = [3, 30, 48, 8]\n",
    "else:\n",
    "    raise ValueError(\"no experiments done on detph {}, you can do it youself\".format(args.depth))\n",
    "\n",
    "num_epochs = 200 if args.data_type == \"cifar10\" else 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_train(ctx): \n",
    "    model = resnext(units=units, num_stage=4, filter_list=[64, 256, 512, 1024, 2048] if args.depth >=50 else [64, 64, 128, 256, 512], ratio_list=ratio_list, num_class=args.num_classes, num_group=args.num_group, data_type=\"imagenet\", drop_out=args.drop_out, bn_mom=args.bn_mom)\n",
    "    # model.collect_params().load('/tanData/models/seresnext_imagenet_50_0_exp1_current.params', ctx=ctx)\n",
    "    for param in model.collect_params().values():\n",
    "        if param.name.find('conv') != -1 or param.name.find('dense') != -1:\n",
    "            if param.name.find('weight') != -1:\n",
    "                param.initialize(init=mx.init.Xavier(rnd_type='gaussian', factor_type=\"in\", magnitude=2), ctx=ctx)\n",
    "            else:\n",
    "                param.initialize(init=mx.init.Zero(), ctx=ctx)\n",
    "        elif param.name.find('batchnorm') != -1:\n",
    "            if param.name.find('gamma') != -1:\n",
    "                param.initialize(init=Normal(mean=1, sigma=0.02), ctx=ctx)\n",
    "            else:\n",
    "                param.initialize(init=mx.init.Zero(), ctx=ctx)\n",
    "        elif param.name.find('insnorm') != -1:\n",
    "            if param.name.find('gamma') != -1:\n",
    "                param.initialize(init=Normal(mean=1, sigma=0.02), ctx=ctx)\n",
    "            else:\n",
    "                param.initialize(init=mx.init.Zero(), ctx=ctx)\n",
    "        elif param.name.find('biasadder') != -1:\n",
    "            param.initialize(init=mx.init.Zero(), ctx=ctx)\n",
    "        else:\n",
    "            param.initialize(init=mx.init.Xavier(rnd_type='gaussian', factor_type=\"in\", magnitude=2), ctx=ctx)\n",
    "                  \n",
    "    model.hybridize()\n",
    "        \n",
    "    best_top1_val, best_top5_val = train(net=model, train_data_sup=train_data_sup, train_data_unsup=train_data_unsup, val_data=val_data, num_epochs=num_epochs, ctx=ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-09-24 09:31:01,350 - Epoch[0] Batch [0] Lr: 0.001000 Speed: 98.639672 samples/sec   loss=26.788785   top1-acc=0.002930     top5-acc=0.006836\n",
      "2018-09-24 09:33:06,762 - Epoch[0] Batch [5] Lr: 0.001000 Speed: 81.660618 samples/sec   loss=14.689771   top1-acc=0.018555     top5-acc=0.050130\n",
      "2018-09-24 09:35:50,954 - Epoch[0] Batch [10] Lr: 0.001000 Speed: 62.368759 samples/sec   loss=11.767613   top1-acc=0.010476     top5-acc=0.043590\n"
     ]
    }
   ],
   "source": [
    "run_train(ctx=ctx)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_python3)",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
